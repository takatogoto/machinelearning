{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kmeans import KMeans\n",
    "\n",
    "from data_loader import toy_dataset, load_digits\n",
    "from utils import Figure\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "class GMM():\n",
    "    '''\n",
    "        Fits a Gausian Mixture model to the data.\n",
    "\n",
    "        attrs:\n",
    "            n_cluster : Number of mixtures (Int)\n",
    "            e : error tolerance (Float) \n",
    "            max_iter : maximum number of updates (Int)\n",
    "            init : initialization of means and variance\n",
    "                Can be 'random' or 'kmeans' \n",
    "            means : means of Gaussian mixtures (n_cluster X D numpy array)\n",
    "            variances : variance of Gaussian mixtures (n_cluster X D X D numpy array) \n",
    "            pi_k : mixture probabilities of different component ((n_cluster,) size numpy array)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_cluster, init='k_means', max_iter=100, e=0.0001):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.e = e\n",
    "        self.max_iter = max_iter\n",
    "        self.init = init\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.pi_k = None\n",
    "\n",
    "    def fit(self, x):\n",
    "        '''\n",
    "            Fits a GMM to x.\n",
    "\n",
    "            x: is a NXD size numpy array\n",
    "            updates:\n",
    "                self.means\n",
    "                self.variances\n",
    "                self.pi_k\n",
    "        '''\n",
    "        assert len(x.shape) == 2, 'x can only be 2 dimensional'\n",
    "\n",
    "        np.random.seed(42)\n",
    "        N, D = x.shape\n",
    "\n",
    "        if (self.init == 'k_means'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means using k-means clustering\n",
    "            # - compute variance and pi_k (see P4.pdf)\n",
    "\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            raise Exception(\n",
    "                'Implement initialization of variances, means, pi_k using k-means')\n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        elif (self.init == 'random'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means randomly\n",
    "            # - initialize variance to be identity and pi_k to be uniform\n",
    "\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            raise Exception(\n",
    "                'Implement initialization of variances, means, pi_k randomly')\n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        else:\n",
    "            raise Exception('Invalid initialization provided')\n",
    "\n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - Use EM to learn the means, variances, and pi_k and assign them to self\n",
    "        # - Update until convergence or until you have made self.max_iter updates.\n",
    "        # - Return the number of E/M-Steps executed (Int) \n",
    "        # Hint: Try to separate E & M step for clarity\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        raise Exception('Implement fit function (filename: gmm.py)')\n",
    "        \n",
    "        \n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "\n",
    "    def sample(self, N):\n",
    "        '''\n",
    "        sample from the GMM model\n",
    "\n",
    "        N is a positive integer\n",
    "        return : NXD array of samples\n",
    "\n",
    "        '''\n",
    "        assert type(N) == int and N > 0, 'N should be a positive integer'\n",
    "        np.random.seed(42)\n",
    "        if (self.means is None):\n",
    "            raise Exception('Train GMM before sampling')\n",
    "\n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - generate samples from the GMM\n",
    "        # - return the samples\n",
    "\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        raise Exception('Implement sample function in gmm.py')\n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "        return samples        \n",
    "\n",
    "    def compute_log_likelihood(self, x, means=None, variances=None, pi_k=None):\n",
    "        '''\n",
    "            Return log-likelihood for the data\n",
    "\n",
    "            x is a NXD matrix\n",
    "            return : a float number which is the log-likelihood of data\n",
    "        '''\n",
    "        assert len(x.shape) == 2,  'x can only be 2 dimensional'\n",
    "        if means is None:\n",
    "            means = self.means\n",
    "        if variances is None:\n",
    "            variances = self.variances\n",
    "        if pi_k is None:\n",
    "            pi_k = self.pi_k    \n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - calculate log-likelihood using means, variances and pi_k attr in self\n",
    "        # - return the log-likelihood (Float)\n",
    "        # Note: you can call this function in fit function (if required)\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        raise Exception('Implement compute_log_likelihood function in gmm.py')\n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "        return log_likelihood\n",
    "\n",
    "    class Gaussian_pdf():\n",
    "        def __init__(self,mean,variance):\n",
    "            self.mean = mean\n",
    "            self.variance = variance\n",
    "            self.c = None\n",
    "            self.inv = None\n",
    "            '''\n",
    "                Input: \n",
    "                    Means: A 1 X D numpy array of the Gaussian mean\n",
    "                    Variance: A D X D numpy array of the Gaussian covariance matrix\n",
    "                Output: \n",
    "                    None: \n",
    "            '''\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - Set self.inv equal to the inverse the variance matrix (after ensuring it is full rank - see P4.pdf)\n",
    "            # - Set self.c equal to ((2pi)^D) * det(variance) (after ensuring the variance matrix is full rank)\n",
    "            # Note you can call this class in compute_log_likelihood and fit\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            raise Exception('Impliment Guassian_pdf __init__')\n",
    "            \n",
    "            \n",
    "            \n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        def getLikelihood(self,x):\n",
    "            '''\n",
    "                Input: \n",
    "                    x: a 1 X D numpy array representing a sample\n",
    "                Output: \n",
    "                    p: a numpy float, the likelihood sample x was generated by this Gaussian\n",
    "                Hint: \n",
    "                    p = e^(-0.5(x-mean)*(inv(variance))*(x-mean)') / sqrt(c)\n",
    "                    where ' is transpose and * is matrix multiplication\n",
    "            '''\n",
    "            #TODO\n",
    "            # - Comment/remove the exception\n",
    "            # - Calculate the likelihood of sample x generated by this Gaussian\n",
    "            # Note: use the described implementation of a Gaussian to ensure compatibility with the solutions\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            raise Exception('Impliment Guassian_pdf getLikelihood')\n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = toy_dataset(4, 100)\n",
    "init = ['k_means', 'random']\n",
    "\n",
    "n_cluster = 4\n",
    "max_iter=1000\n",
    "e=1e-6\n",
    "## gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=, e=1e-6)\n",
    "\n",
    "# after fit\n",
    "np.random.seed(42)\n",
    "N, D = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #if (self.init == 'k_means'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means using k-means clustering\n",
    "            # - compute variance and pi_k (see P4.pdf)\n",
    "            kmean = KMeans(n_cluster, max_iter, e) # self.n_cluster self.max_iter \n",
    "            means, _, _ = kmean.fit(x) # self.means\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10915641,  4.06440898],\n",
       "       [-0.07648207, -3.90558464],\n",
       "       [-4.06823887, -0.10771231],\n",
       "       [ 3.85187516,  0.07434196]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #elif (self.init == 'random'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means randomly\n",
    "            # - initialize variance to be identity and pi_k to be uniform\n",
    "            means = np.random.rand(n_cluster, D) # self.means self.n_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38552709e-01, 7.78765841e-04],\n",
       "       [9.92211559e-01, 6.17481510e-01],\n",
       "       [6.11653160e-01, 7.06630522e-03],\n",
       "       [2.30624250e-02, 5.24774660e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Implement initialization of variances, means, pi_k using k-means",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d2c3160e393c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mn_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mgmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_cluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e61255d1f0dd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# DONOT MODIFY CODE ABOVE THIS LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             raise Exception(\n\u001b[0;32m---> 55\u001b[0;31m                 'Implement initialization of variances, means, pi_k using k-means')\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;31m# DONOT MODIFY CODE BELOW THIS LINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Implement initialization of variances, means, pi_k using k-means"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_loader import toy_dataset, load_digits\n",
    "from utils import Figure\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def compute_elipse_params(variance):\n",
    "    '''\n",
    "        Compute elipse params for plotting from variance\n",
    "    '''\n",
    "\n",
    "    # http://www.cs.cornell.edu/cv/OtherPdf/Ellipse.pdf Slide 17\n",
    "    # https://stackoverflow.com/a/41821484\n",
    "\n",
    "    variance_inv = np.linalg.inv(variance)\n",
    "    a = variance_inv[0, 0]\n",
    "    c = variance_inv[1, 1]\n",
    "    b = variance_inv[0, 1] + variance_inv[1, 0]\n",
    "\n",
    "    M = (variance_inv + variance_inv.T) / 2\n",
    "    eig, _ = np.linalg.eig(M)\n",
    "    if (np.abs(eig[0] - a) < np.abs(eig[0] - c)):\n",
    "        lambda1, lambda2 = eig\n",
    "    else:\n",
    "        lambda2, lambda1 = eig\n",
    "\n",
    "    angle = np.arctan(b / (a - c)) / 2\n",
    "    return np.sqrt(1 / lambda1), np.sqrt(1 / lambda2), angle\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GMM on 2D toy dataset\n",
    "# The dataset is generated from N gaussian distributions equally spaced on N radius circle.\n",
    "# Here, N=4\n",
    "# You should be able to visualize the learnt gaussian distribution in plots folder\n",
    "# Complete implementation of fit function for GMM class in gmm.py\n",
    "################################################################################\n",
    "x, y = toy_dataset(4, 100)\n",
    "init = ['k_means', 'random']\n",
    "\n",
    "for i in init:\n",
    "    n_cluster = 4\n",
    "    gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=i, e=1e-6)\n",
    "    iterations = gmm.fit(x)\n",
    "    ll = gmm.compute_log_likelihood(x)\n",
    "\n",
    "    assert gmm.means.shape == (\n",
    "        n_cluster, 2), 'means should be numpy array with {}X2 shape'.format(n_cluster)\n",
    "\n",
    "    assert gmm.variances.shape == (\n",
    "        n_cluster, 2, 2), 'variances should be numpy array with {}X2X2 shape'.format(n_cluster)\n",
    "\n",
    "    assert gmm.pi_k.shape == (\n",
    "        n_cluster,), 'pi_k should be numpy vector of size'.format(n_cluster)\n",
    "\n",
    "    assert iterations > 0 and type(\n",
    "        iterations) == int, 'Number of updates should be positive integer'\n",
    "\n",
    "    assert type(ll) == float, 'log-likelihood should be float'\n",
    "\n",
    "    print('GMM for toy dataset with {} init converged in {} iteration. Final log-likelihood of data: {}'.format(\n",
    "        i, iterations, ll))\n",
    "\n",
    "    np.savez('results/gmm_toy_{}.npz'.format(i), iterations=iterations,\n",
    "             variances=gmm.variances, pi_k=gmm.pi_k, means=gmm.means, log_likelihood=ll, x=x, y=y)\n",
    "\n",
    "    # plot\n",
    "    fig = Figure()\n",
    "    fig.ax.scatter(x[:, 0], x[:, 1], c=y)\n",
    "    # fig.ax.scatter(gmm.means[:, 0], gmm.means[:, 1], c='red')\n",
    "    for component in range(n_cluster):\n",
    "        a, b, angle = compute_elipse_params(gmm.variances[component])\n",
    "        e = Ellipse(xy=gmm.means[component], width=a * 5, height=b * 5,\n",
    "                    angle=angle, alpha=gmm.pi_k[component])\n",
    "        fig.ax.add_artist(e)\n",
    "    fig.savefig('plots/gmm_toy_dataset_{}.png'.format(i))\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GMM on digits dataset\n",
    "# We fit a gaussian distribution on digits dataset and show generate samples from the distribution\n",
    "# Complete implementation of sample function for GMM class in gmm.py\n",
    "################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_digits()\n",
    "\n",
    "for i in init:\n",
    "    n_cluster = 30\n",
    "    gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=i, e=1e-10)\n",
    "    iterations = gmm.fit(x_train)\n",
    "    ll = gmm.compute_log_likelihood(x_train)\n",
    "    print('GMM for digits dataset with {} init converged in {} iterations. Final log-likelihood of data: {}'.format(i, iterations, ll))\n",
    "\n",
    "    # plot cluster means\n",
    "    means = gmm.means\n",
    "    from matplotlib import pyplot as plt\n",
    "    l = int(np.ceil(np.sqrt(n_cluster)))\n",
    "\n",
    "    im = np.zeros((10 * l, 10 * l))\n",
    "    for m in range(l):\n",
    "        for n in range(l):\n",
    "            if (m * l + n < n_cluster):\n",
    "                im[10 * m:10 * m + 8, 10 * n:10 * n +\n",
    "                    8] = means[m * l + n].reshape([8, 8])\n",
    "    im = (im > 0) * im\n",
    "    plt.imsave('plots/means_{}.png'.format(i), im, cmap='Greys')\n",
    "\n",
    "    # plot samples\n",
    "    N = 100\n",
    "    l = int(np.ceil(np.sqrt(N)))\n",
    "    samples = gmm.sample(N)\n",
    "\n",
    "    assert samples.shape == (\n",
    "        N, x_train.shape[1]), 'Samples should be numpy array with dimensions {}X{}'.format(N, x_train.shape[1])\n",
    "\n",
    "    im = np.zeros((10 * l, 10 * l))\n",
    "    for m in range(l):\n",
    "        for n in range(l):\n",
    "            if (m * l + n < N):\n",
    "                im[10 * m: 10 * m + 8, 10 * n: 10 * n +\n",
    "                    8] = samples[m * l + n].reshape([8, 8])\n",
    "    im = (im > 0) * im\n",
    "    plt.imsave('plots/samples_{}.png'.format(i), im, cmap='Greys')\n",
    "\n",
    "    np.savez('results/gmm_digits_{}.npz'.format(i), iterations=np.array(\n",
    "        [iterations]), variances=gmm.variances, pi_k=gmm.pi_k, means=gmm.means, samples=samples, log_likelihood=ll, x=x_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

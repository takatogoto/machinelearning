{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kmeans import KMeans\n",
    "\n",
    "from data_loader import toy_dataset, load_digits\n",
    "from utils import Figure\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "class GMM():\n",
    "    '''\n",
    "        Fits a Gausian Mixture model to the data.\n",
    "\n",
    "        attrs:\n",
    "            n_cluster : Number of mixtures (Int)\n",
    "            e : error tolerance (Float) \n",
    "            max_iter : maximum number of updates (Int)\n",
    "            init : initialization of means and variance\n",
    "                Can be 'random' or 'kmeans' \n",
    "            means : means of Gaussian mixtures (n_cluster X D numpy array)\n",
    "            variances : variance of Gaussian mixtures (n_cluster X D X D numpy array) \n",
    "            pi_k : mixture probabilities of different component ((n_cluster,) size numpy array)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_cluster, init='k_means', max_iter=100, e=0.0001):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.e = e\n",
    "        self.max_iter = max_iter\n",
    "        self.init = init\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.pi_k = None\n",
    "\n",
    "    def fit(self, x):\n",
    "        '''\n",
    "            Fits a GMM to x.\n",
    "\n",
    "            x: is a NXD size numpy array\n",
    "            updates:\n",
    "                self.means\n",
    "                self.variances\n",
    "                self.pi_k\n",
    "        '''\n",
    "        assert len(x.shape) == 2, 'x can only be 2 dimensional'\n",
    "\n",
    "        np.random.seed(42)\n",
    "        N, D = x.shape\n",
    "\n",
    "        if (self.init == 'k_means'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means using k-means clustering\n",
    "            # - compute variance and pi_k (see P4.pdf)\n",
    "\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            #raise Exception(\n",
    "            #    'Implement initialization of variances, means, pi_k using k-means')\n",
    "            \n",
    "            kmean = KMeans(self.n_cluster, self.max_iter, self.e) # self.n_cluster self.max_iter \n",
    "            self.means, ymu, _ = kmean.fit(x) # self.means\n",
    "            self.pi_k = np.array([np.sum(ymu == k) for k in range(self.n_cluster)]) / N #self.pi_k self.n_cluster\n",
    "            \n",
    "            # gamma_ik = {0, 1} at this initialize\n",
    "            self.variances = np.zeros((n_cluster, D, D)) #self.variances self.n_cluster self.means\n",
    "            for k in range(self.n_cluster):\n",
    "                xt = x[ymu == k,:] - self.means[k,:] \n",
    "                self.variances[k, :, :] = np.dot(np.transpose(xt),xt) / np.sum(ymu==k) #self.variances \n",
    "            \n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        elif (self.init == 'random'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means randomly\n",
    "            # - initialize variance to be identity and pi_k to be uniform\n",
    "\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            #raise Exception(\n",
    "            #    'Implement initialization of variances, means, pi_k randomly')\n",
    "            \n",
    "            self.means = np.random.rand(self.n_cluster, D) # self.means self.n_cluster\n",
    "            self.pi_k = np.random.rand(self.n_cluster,) #self.pi_k self.n_cluster\n",
    "            self.variances = np.random.rand(self.n_cluster, D, D) #self.variances self.n_cluster\n",
    "            \n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        else:\n",
    "            raise Exception('Invalid initialization provided')\n",
    "\n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - Use EM to learn the means, variances, and pi_k and assign them to self\n",
    "        # - Update until convergence or until you have made self.max_iter updates.\n",
    "        # - Return the number of E/M-Steps executed (Int) \n",
    "        # Hint: Try to separate E & M step for clarity\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        #raise Exception('Implement fit function (filename: gmm.py)')\n",
    "        \n",
    "        #4\n",
    "        l = self.compute_log_likelihood(x, self.means, self.variances, self.pi_k)\n",
    "        gamma = np.zeros((N, self.n_cluster))\n",
    "        for itr in range(self.max_iter):\n",
    "            print(\"Iteration,\", itr)\n",
    "            # 6 E step\n",
    "            for n in range(N):\n",
    "                sumnorm = .0\n",
    "                normk = np.zeros(self.n_cluster)\n",
    "                for k in range(self.n_cluster):\n",
    "                    normk[k]= self.pi_k[k] * self.Gaussian_pdf(\n",
    "                        self.means[k,:], self.variances[k,:]).getLikelihood(x[n,:])\n",
    "                    sumnorm += normk[k]\n",
    "                gamma[n, :] = normk / sumnorm\n",
    "            \n",
    "            # 7 M step\n",
    "            # eq.(5)\n",
    "            Nk = np.sum(gamma, axis=0)\n",
    "            \n",
    "            # eq.(6)\n",
    "            means2 = np.zeros(self.means.shape)\n",
    "            for k in range(self.n_cluster):\n",
    "                means2[k, :] = np.sum(np.multiply(gamma[:, k].reshape(gamma.shape[0],1), x), axis=0)/Nk[k]\n",
    "            \n",
    "            \n",
    "            # eq.(7)\n",
    "            variances2 = np.zeros(self.variances.shape)\n",
    "            for k in range(self.n_cluster):\n",
    "                sumvark = .0\n",
    "                for n in range(N):\n",
    "                    xmu = x[n, :] - self.means[k, :]\n",
    "                    sumvark += gamma[n, k]*(np.dot(np.transpose(xmu), xmu))\n",
    "                variances2[k, :, :] = sumvark / Nk[k]\n",
    "            \n",
    "            # eq.(8)\n",
    "            self.pi_k = Nk / N\n",
    "            self.means = means2\n",
    "            self.variances = variances2\n",
    "            \n",
    "            l1 = self.compute_log_likelihood(x, self.means, self.variances, self.pi_k)\n",
    "            \n",
    "            # stop condition\n",
    "            if abs(l-l1) < self.e:\n",
    "                break\n",
    "            l = l1\n",
    "        \n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "\n",
    "    def sample(self, N):\n",
    "        '''\n",
    "        sample from the GMM model\n",
    "\n",
    "        N is a positive integer\n",
    "        return : NXD array of samples\n",
    "\n",
    "        '''\n",
    "        assert type(N) == int and N > 0, 'N should be a positive integer'\n",
    "        np.random.seed(42)\n",
    "        if (self.means is None):\n",
    "            raise Exception('Train GMM before sampling')\n",
    "\n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - generate samples from the GMM\n",
    "        # - return the samples\n",
    "\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        raise Exception('Implement sample function in gmm.py')\n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "        return samples        \n",
    "\n",
    "    def compute_log_likelihood(self, x, means=None, variances=None, pi_k=None):\n",
    "        '''\n",
    "            Return log-likelihood for the data\n",
    "\n",
    "            x is a NXD matrix\n",
    "            return : a float number which is the log-likelihood of data\n",
    "        '''\n",
    "        assert len(x.shape) == 2,  'x can only be 2 dimensional'\n",
    "        if means is None:\n",
    "            means = self.means\n",
    "        if variances is None:\n",
    "            variances = self.variances\n",
    "        if pi_k is None:\n",
    "            pi_k = self.pi_k    \n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - calculate log-likelihood using means, variances and pi_k attr in self\n",
    "        # - return the log-likelihood (Float)\n",
    "        # Note: you can call this function in fit function (if required)\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        #raise Exception('Implement compute_log_likelihood function in gmm.py')\n",
    "        N, D = x.shape\n",
    "        K = self.pi_k.shape[0]\n",
    "        log_likelihood = .0\n",
    "        #for n in range(3):\n",
    "        for n in range(N):\n",
    "            lnk = .0\n",
    "            for k in range(K):\n",
    "                lnk += self.pi_k[k] * self.Gaussian_pdf(\n",
    "                    self.means[k,:], self.variances[k,:]).getLikelihood(x[n,:])\n",
    "            log_likelihood += np.log(lnk)\n",
    "        \n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "        return log_likelihood\n",
    "\n",
    "    class Gaussian_pdf():\n",
    "        def __init__(self,mean,variance):\n",
    "            self.mean = mean\n",
    "            self.variance = variance\n",
    "            self.c = None\n",
    "            self.inv = None\n",
    "            '''\n",
    "                Input: \n",
    "                    Means: A 1 X D numpy array of the Gaussian mean\n",
    "                    Variance: A D X D numpy array of the Gaussian covariance matrix\n",
    "                Output: \n",
    "                    None: \n",
    "            '''\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - Set self.inv equal to the inverse the variance matrix (after ensuring it is full rank - see P4.pdf)\n",
    "            # - Set self.c equal to ((2pi)^D) * det(variance) (after ensuring the variance matrix is full rank)\n",
    "            # Note you can call this class in compute_log_likelihood and fit\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            #raise Exception('Impliment Guassian_pdf __init__')\n",
    "            D = self.variance.shape[0] # self.variance\n",
    "            while np.linalg.matrix_rank(self.variance) != len(self.variance): # self.variance\n",
    "                self.variance = self.variance + 1e-3 * np.identity(len(self.variance)) # self.variance\n",
    "            self.inv = np.linalg.inv(self.variance) # self.variance self.inv\n",
    "            self.c = ((2*np.pi)**D) * np.linalg.det(self.variance) # self.c self.variance            \n",
    "            \n",
    "            \n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n",
    "        def getLikelihood(self,x):\n",
    "            '''\n",
    "                Input: \n",
    "                    x: a 1 X D numpy array representing a sample\n",
    "                Output: \n",
    "                    p: a numpy float, the likelihood sample x was generated by this Gaussian\n",
    "                Hint: \n",
    "                    p = e^(-0.5(x-mean)*(inv(variance))*(x-mean)') / sqrt(c)\n",
    "                    where ' is transpose and * is matrix multiplication\n",
    "            '''\n",
    "            #TODO\n",
    "            # - Comment/remove the exception\n",
    "            # - Calculate the likelihood of sample x generated by this Gaussian\n",
    "            # Note: use the described implementation of a Gaussian to ensure compatibility with the solutions\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            #raise Exception('Impliment Guassian_pdf getLikelihood')\n",
    "            p = np.exp(-0.5 * np.dot(np.dot((x - self.mean), self.inv),\n",
    "                                     np.transpose(x - self.mean))) / np.sqrt(self.c) # self.mean self.inv self.c\n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = toy_dataset(4, 100)\n",
    "init = ['k_means', 'random']\n",
    "\n",
    "n_cluster = 4\n",
    "max_iter=1000\n",
    "e=1e-6\n",
    "## gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=, e=1e-6)\n",
    "\n",
    "# after fit\n",
    "np.random.seed(42)\n",
    "N, D = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 self.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #if (self.init == 'k_means'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means using k-means clustering\n",
    "            # - compute variance and pi_k (see P4.pdf)\n",
    "            kmean = KMeans(n_cluster, max_iter, e) # self.n_cluster self.max_iter \n",
    "            means, ymu, _ = kmean.fit(x) # self.means\n",
    "            \n",
    "            pi_k = np.array([np.sum(ymu == k) for k in range(n_cluster)]) / N #self.pi_k self.n_cluster\n",
    "            \n",
    "            # gamma_ik = {0, 1} at this initialize\n",
    "            variances = np.zeros((n_cluster, D, D)) #self.variances self.n_cluster\n",
    "            for k in range(n_cluster):\n",
    "                xt = x[ymu == k,:] - means[k,:]\n",
    "                variances[k, :, :] = np.dot(np.transpose(xt),xt) / np.sum(ymu==k) #self.variances \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10915641,  4.06440898],\n",
       "       [-0.07648207, -3.90558464],\n",
       "       [-4.06823887, -0.10771231],\n",
       "       [ 3.85187516,  0.07434196]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ymu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ymu == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.245, 0.25 , 0.25 , 0.255])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self.pi_k\n",
    "np.array([np.sum(ymu == k) for k in set(ymu)]) /N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.245, 0.25 , 0.25 , 0.255])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.sum(ymu == k) for k in range(n_cluster)]) /N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89698826, -0.00362015],\n",
       "       [-0.00362015,  0.88002076]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = x[ymu == 0,:] - means[0,:]\n",
    "np.dot(np.transpose(xt),xt) / np.sum(ymu==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.89698826 -0.00362015]\n",
      "  [-0.00362015  0.88002076]]\n",
      "\n",
      " [[ 0.99401738 -0.12779475]\n",
      "  [-0.12779475  1.06004803]]\n",
      "\n",
      " [[ 0.94838648  0.07112622]\n",
      "  [ 0.07112622  1.00335619]]\n",
      "\n",
      " [[ 0.76487677 -0.03873293]\n",
      "  [-0.03873293  1.04969266]]]\n"
     ]
    }
   ],
   "source": [
    "variances = np.zeros((n_cluster, D, D))\n",
    "for k in range(n_cluster):\n",
    "    xt = x[ymu == k,:] - means[k,:]\n",
    "    variances[k, :, :] = np.dot(np.transpose(xt),xt) / np.sum(ymu==k)\n",
    "print(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.array([[1,0],[0,1],[1,1]])\n",
    "np.dot(aa.T, aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #elif (self.init == 'random'):\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - initialize means randomly\n",
    "            # - initialize variance to be identity and pi_k to be uniform\n",
    "            means = np.random.rand(n_cluster, D) # self.means self.n_cluster\n",
    "            pi_k = np.random.rand(n_cluster,) #self.pi_k self.n_cluster\n",
    "            variances = np.random.rand(n_cluster, D, D) #self.variances self.n_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.38552709e-01, 7.78765841e-04],\n",
       "       [9.92211559e-01, 6.17481510e-01],\n",
       "       [6.11653160e-01, 7.06630522e-03],\n",
       "       [2.30624250e-02, 5.24774660e-01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17336465, 0.39106061, 0.18223609, 0.75536141])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "kk = pi_k.shape[0]\n",
    "print(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.02306243, 0.52477466],\n",
       "        [0.39986097, 0.04666566]],\n",
       "\n",
       "       [[0.97375552, 0.23277134],\n",
       "        [0.09060643, 0.61838601]],\n",
       "\n",
       "       [[0.38246199, 0.98323089],\n",
       "        [0.46676289, 0.85994041]],\n",
       "\n",
       "       [[0.68030754, 0.45049925],\n",
       "        [0.01326496, 0.94220176]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Compute the log-likelihood l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"#def compute_log_likelihood(self, x, means=None, variances=None, pi_k=None):\n",
    "        '''\n",
    "            Return log-likelihood for the data\n",
    "\n",
    "            x is a NXD matrix\n",
    "            return : a float number which is the log-likelihood of data\n",
    "        '''\n",
    "        assert len(x.shape) == 2,  'x can only be 2 dimensional'\n",
    "        if means is None:\n",
    "            means = self.means\n",
    "        if variances is None:\n",
    "            variances = self.variances\n",
    "        if pi_k is None:\n",
    "            pi_k = self.pi_k \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        # - comment/remove the exception\n",
    "        # - calculate log-likelihood using means, variances and pi_k attr in self\n",
    "        # - return the log-likelihood (Float)\n",
    "        # Note: you can call this function in fit function (if required)\n",
    "        # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "        #raise Exception('Implement compute_log_likelihood function in gmm.py')\n",
    "        # DONOT MODIFY CODE BELOW THIS LINE\n",
    "        \n",
    "        #return log_likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \"\"\"\n",
    "        class Gaussian_pdf():\n",
    "        def __init__(self,mean,variance):\n",
    "            self.mean = mean\n",
    "            self.variance = variance\n",
    "            self.c = None\n",
    "            self.inv = None\n",
    "            '''\n",
    "                Input: \n",
    "                    Means: A 1 X D numpy array of the Gaussian mean\n",
    "                    Variance: A D X D numpy array of the Gaussian covariance matrix\n",
    "                Output: \n",
    "                    None: \n",
    "            '''\n",
    "            # TODO\n",
    "            # - comment/remove the exception\n",
    "            # - Set self.inv equal to the inverse the variance matrix (after ensuring it is full rank - see P4.pdf)\n",
    "            # - Set self.c equal to ((2pi)^D) * det(variance) (after ensuring the variance matrix is full rank)\n",
    "            # Note you can call this class in compute_log_likelihood and fit\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            raise Exception('Impliment Guassian_pdf __init__')\n",
    "                \n",
    "        \"\"\"\n",
    "        mean = means[0,:]\n",
    "        variance = variances[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "            D = variance.shape[0]\n",
    "            while np.linalg.matrix_rank(variance) != len(variance): # self.variance\n",
    "                variance = variance + 1e-3 * np.identity(len(variance)) # self.variance\n",
    "            inv = np.linalg.inv(variance) # self.variance self.inv\n",
    "            c = ((2*np.pi)**D) * np.linalg.det(variance) # self.c self.variance\n",
    "        \n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1148603  0.00458621]\n",
      " [0.00458621 1.1363557 ]]\n"
     ]
    }
   ],
   "source": [
    "print(inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.49671415 -0.1382643 ]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8672945049698254e-10\n"
     ]
    }
   ],
   "source": [
    "            #def getLikelihood(self,x):\n",
    "            '''\n",
    "                Input: \n",
    "                    x: a 1 X D numpy array representing a sample\n",
    "                Output: \n",
    "                    p: a numpy float, the likelihood sample x was generated by this Gaussian\n",
    "                Hint: \n",
    "                    p = e^(-0.5(x-mean)*(inv(variance))*(x-mean)') / sqrt(c)\n",
    "                    where ' is transpose and * is matrix multiplication\n",
    "            '''\n",
    "            #TODO\n",
    "            # - Comment/remove the exception\n",
    "            # - Calculate the likelihood of sample x generated by this Gaussian\n",
    "            # Note: use the described implementation of a Gaussian to ensure compatibility with the solutions\n",
    "            # DONOT MODIFY CODE ABOVE THIS LINE\n",
    "            #raise Exception('Impliment Guassian_pdf getLikelihood')\n",
    "            p = np.exp(-0.5 * np.dot(\n",
    "                np.dot(\n",
    "                    (x - mean), inv), np.transpose(\n",
    "                    x - mean))) / np.sqrt(c)# self.mean self.inv self.c\n",
    "            print(p)\n",
    "            # DONOT MODIFY CODE BELOW THIS LINE\n",
    "            #return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([1,2]),np.array([[1],[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = toy_dataset(4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.001 0.   ]\n",
      " [0.    0.001]]\n",
      "[[ 385.08839354 -614.91160646]\n",
      " [-384.31975404  615.68024596]]\n",
      "[[0.61491161 0.61491161]\n",
      " [0.38431975 0.38431975]]\n",
      "[[ 1.00000000e+00 -5.68434189e-14]\n",
      " [ 2.84217094e-14  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "ain = np.array([[0.8, 0.8],[.5,.5]])\n",
    "print(1e-3 * np.identity(len(ain)))\n",
    "while np.linalg.matrix_rank(ain) != len(ain):\n",
    "    ain = ain + 1e-3 * np.identity(len(ain))\n",
    "ainv = np.linalg.inv(ain)\n",
    "print(ainv)\n",
    "print(np.dot(np.array([[0.8, 0.8],[.5,.5]]), ainv))\n",
    "print(np.dot(ain, ainv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.25179981e+16,  3.60287970e+16],\n",
       "       [ 2.25179981e+16, -3.60287970e+16]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(np.array([[0.8, 0.8],[.5,.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.2204460492503083e-17"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(np.array([[0.8, 0.8],[.5,.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(np.array([[0.8, 0.8],[.5,.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49975012, 0.49975012],\n",
       "       [0.49975012, 0.49975012]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([[1,1],[1,1]]), ainv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = np.array([[1,1],[2,3]])\n",
    "bbinv = np.linalg.inv(bb)\n",
    "np.dot(bb,bbinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: EM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4)\n"
     ]
    }
   ],
   "source": [
    "gmik = np.random.rand(N,n_cluster).reshape(N,n_cluster)\n",
    "print(gmik.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203.26596957 202.98398924 203.7552045  204.03667623]\n"
     ]
    }
   ],
   "source": [
    "Nk = np.sum(gmik, axis=0)\n",
    "print(Nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "[1 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmminik = np.array([[1,1,0],[0,1,0],[0,0,1]])\n",
    "print(gmminik)\n",
    "print(gmminik[0,:])\n",
    "nkmi = np.sum(gmminik, axis=0)\n",
    "np.ravel(gmminik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1]\n",
      " [-1 -1]\n",
      " [ 1  0]]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "xmini = np.array([[1,1],[-1,-1],[1,0]])\n",
    "print(xmini)\n",
    "print(xmini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(cluster X N, )\n",
    "np.tile(np.ravel(gmminik),(xmini.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [-2, -2],\n",
       "       [ 3,  0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(np.array([[1],[2],[3]]), xmini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, -1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.multiply(np.array([[1],[2],[3]]), xmini), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, -2,  3],\n",
       "       [ 1, -2,  0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(np.array([1,2,3]), xmini.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmminik[:,0].reshape(gmminik.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:199: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:110: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:28: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims, initial)\n",
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py:1794: RuntimeWarning: invalid value encountered in greater\n",
      "  return count_nonzero(S > tol, axis=-1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_loader import toy_dataset, load_digits\n",
    "from utils import Figure\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "def compute_elipse_params(variance):\n",
    "    '''\n",
    "        Compute elipse params for plotting from variance\n",
    "    '''\n",
    "\n",
    "    # http://www.cs.cornell.edu/cv/OtherPdf/Ellipse.pdf Slide 17\n",
    "    # https://stackoverflow.com/a/41821484\n",
    "\n",
    "    variance_inv = np.linalg.inv(variance)\n",
    "    a = variance_inv[0, 0]\n",
    "    c = variance_inv[1, 1]\n",
    "    b = variance_inv[0, 1] + variance_inv[1, 0]\n",
    "\n",
    "    M = (variance_inv + variance_inv.T) / 2\n",
    "    eig, _ = np.linalg.eig(M)\n",
    "    if (np.abs(eig[0] - a) < np.abs(eig[0] - c)):\n",
    "        lambda1, lambda2 = eig\n",
    "    else:\n",
    "        lambda2, lambda1 = eig\n",
    "\n",
    "    angle = np.arctan(b / (a - c)) / 2\n",
    "    return np.sqrt(1 / lambda1), np.sqrt(1 / lambda2), angle\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GMM on 2D toy dataset\n",
    "# The dataset is generated from N gaussian distributions equally spaced on N radius circle.\n",
    "# Here, N=4\n",
    "# You should be able to visualize the learnt gaussian distribution in plots folder\n",
    "# Complete implementation of fit function for GMM class in gmm.py\n",
    "################################################################################\n",
    "x, y = toy_dataset(4, 100)\n",
    "init = ['k_means', 'random']\n",
    "\n",
    "for i in init:\n",
    "    n_cluster = 4\n",
    "    gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=i, e=1e-6)\n",
    "    iterations = gmm.fit(x)\n",
    "    ll = gmm.compute_log_likelihood(x)\n",
    "\n",
    "    assert gmm.means.shape == (\n",
    "        n_cluster, 2), 'means should be numpy array with {}X2 shape'.format(n_cluster)\n",
    "\n",
    "    assert gmm.variances.shape == (\n",
    "        n_cluster, 2, 2), 'variances should be numpy array with {}X2X2 shape'.format(n_cluster)\n",
    "\n",
    "    assert gmm.pi_k.shape == (\n",
    "        n_cluster,), 'pi_k should be numpy vector of size'.format(n_cluster)\n",
    "\n",
    "    assert iterations > 0 and type(\n",
    "        iterations) == int, 'Number of updates should be positive integer'\n",
    "\n",
    "    assert type(ll) == float, 'log-likelihood should be float'\n",
    "\n",
    "    print('GMM for toy dataset with {} init converged in {} iteration. Final log-likelihood of data: {}'.format(\n",
    "        i, iterations, ll))\n",
    "\n",
    "    np.savez('results/gmm_toy_{}.npz'.format(i), iterations=iterations,\n",
    "             variances=gmm.variances, pi_k=gmm.pi_k, means=gmm.means, log_likelihood=ll, x=x, y=y)\n",
    "\n",
    "    # plot\n",
    "    fig = Figure()\n",
    "    fig.ax.scatter(x[:, 0], x[:, 1], c=y)\n",
    "    # fig.ax.scatter(gmm.means[:, 0], gmm.means[:, 1], c='red')\n",
    "    for component in range(n_cluster):\n",
    "        a, b, angle = compute_elipse_params(gmm.variances[component])\n",
    "        e = Ellipse(xy=gmm.means[component], width=a * 5, height=b * 5,\n",
    "                    angle=angle, alpha=gmm.pi_k[component])\n",
    "        fig.ax.add_artist(e)\n",
    "    fig.savefig('plots/gmm_toy_dataset_{}.png'.format(i))\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# GMM on digits dataset\n",
    "# We fit a gaussian distribution on digits dataset and show generate samples from the distribution\n",
    "# Complete implementation of sample function for GMM class in gmm.py\n",
    "################################################################################\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_digits()\n",
    "\n",
    "for i in init:\n",
    "    n_cluster = 30\n",
    "    gmm = GMM(n_cluster=n_cluster, max_iter=1000, init=i, e=1e-10)\n",
    "    iterations = gmm.fit(x_train)\n",
    "    ll = gmm.compute_log_likelihood(x_train)\n",
    "    print('GMM for digits dataset with {} init converged in {} iterations. Final log-likelihood of data: {}'.format(i, iterations, ll))\n",
    "\n",
    "    # plot cluster means\n",
    "    means = gmm.means\n",
    "    from matplotlib import pyplot as plt\n",
    "    l = int(np.ceil(np.sqrt(n_cluster)))\n",
    "\n",
    "    im = np.zeros((10 * l, 10 * l))\n",
    "    for m in range(l):\n",
    "        for n in range(l):\n",
    "            if (m * l + n < n_cluster):\n",
    "                im[10 * m:10 * m + 8, 10 * n:10 * n +\n",
    "                    8] = means[m * l + n].reshape([8, 8])\n",
    "    im = (im > 0) * im\n",
    "    plt.imsave('plots/means_{}.png'.format(i), im, cmap='Greys')\n",
    "\n",
    "    # plot samples\n",
    "    N = 100\n",
    "    l = int(np.ceil(np.sqrt(N)))\n",
    "    samples = gmm.sample(N)\n",
    "\n",
    "    assert samples.shape == (\n",
    "        N, x_train.shape[1]), 'Samples should be numpy array with dimensions {}X{}'.format(N, x_train.shape[1])\n",
    "\n",
    "    im = np.zeros((10 * l, 10 * l))\n",
    "    for m in range(l):\n",
    "        for n in range(l):\n",
    "            if (m * l + n < N):\n",
    "                im[10 * m: 10 * m + 8, 10 * n: 10 * n +\n",
    "                    8] = samples[m * l + n].reshape([8, 8])\n",
    "    im = (im > 0) * im\n",
    "    plt.imsave('plots/samples_{}.png'.format(i), im, cmap='Greys')\n",
    "\n",
    "    np.savez('results/gmm_digits_{}.npz'.format(i), iterations=np.array(\n",
    "        [iterations]), variances=gmm.variances, pi_k=gmm.pi_k, means=gmm.means, samples=samples, log_likelihood=ll, x=x_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

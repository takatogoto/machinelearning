{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff_output: 5.622747561890727e-08\n",
      "linear.forward should be correct\n",
      "##########################\n",
      "max_diff_grad_X: 6.071981381304149e-08\n",
      "max_diff_grad_W: 2.1809429970576704e-10\n",
      "max_diff_grad_b: -1.7190150351432404e-10\n",
      "linear.backward should be correct\n",
      "##########################\n",
      "max_diff_output: 6.883943155783582e-09\n",
      "relu.forward should be correct\n",
      "##########################\n",
      "max_diff_grad_X: 1.8961226093888063e-09\n",
      "relu.backward should be correct\n",
      "##########################\n",
      "max_diff_grad_X: 2.7442624962631315e-09\n",
      "dropout.backward should be correct\n",
      "##########################\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Do not change the input and output format.\n",
    "If our script cannot run your code or the format is improper, your code will not be graded.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import dnn_misc\n",
    "\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# example data\n",
    "X = np.random.normal(0, 1, (5, 3))\n",
    "\n",
    "\n",
    "# example modules\n",
    "check_linear = dnn_misc.linear_layer(input_D = 3, output_D = 2)\n",
    "check_relu = dnn_misc.relu()\n",
    "check_dropout = dnn_misc.dropout(r = 0.5)\n",
    "\n",
    "\n",
    "# check_linear.forward\n",
    "hat_X = check_linear.forward(X)\n",
    "ground_hat_X = np.array([[ 0.42525407, -0.2120611 ],\n",
    " [ 0.15174804, -0.36218431],\n",
    " [ 0.20957104, -0.57861084],\n",
    " [ 0.03460477, -0.35992763],\n",
    " [-0.07256568,  0.1385197 ]])\n",
    "\n",
    "if (hat_X.shape[0] != 5) or (hat_X.shape[1] != 2):\n",
    "    print('Wrong output dimension of linear.forward')\n",
    "else:\n",
    "    max_relative_diff = np.amax(np.abs(ground_hat_X - hat_X) / (ground_hat_X + 1e-8))\n",
    "    print('max_diff_output: ' + str(max_relative_diff))\n",
    "    if max_relative_diff >= 1e-7:\n",
    "        print('linear.forward might be wrong')\n",
    "    else:\n",
    "        print('linear.forward should be correct')\n",
    "print('##########################')\n",
    "\n",
    "\n",
    "# check_linear.backward\n",
    "grad_hat_X = np.random.normal(0, 1, (5, 2))\n",
    "grad_X = check_linear.backward(X, grad_hat_X)\n",
    "\n",
    "ground_grad_X = np.array([[-0.32766959,  0.13123228, -0.0470483 ],\n",
    " [ 0.22780188, -0.04838436,  0.04225799],\n",
    " [ 0.03115675, -0.32648556, -0.06550193],\n",
    " [-0.01895741, -0.21411292, -0.05212837],\n",
    " [-0.26923074, -0.78986304, -0.23870499]])\n",
    "\n",
    "ground_grad_W = np.array([[-0.27579345, -2.08570514],\n",
    " [ 4.52754775, -0.40995374],\n",
    " [-1.2049515,   1.77662551]])\n",
    "\n",
    "ground_grad_b = np.array([[-4.55094716, -2.51399667]])\n",
    "\n",
    "if (grad_X.shape[0] != 5) or (grad_X.shape[1] != 3):\n",
    "    print('Wrong output dimension of linear.backward')\n",
    "else:\n",
    "    max_relative_diff_X = np.amax(np.abs(ground_grad_X - grad_X) / (ground_grad_X + 1e-8))\n",
    "    print('max_diff_grad_X: ' + str(max_relative_diff_X))\n",
    "    max_relative_diff_W = np.amax(np.abs(ground_grad_W - check_linear.gradient['W']) / (ground_grad_W + 1e-8))\n",
    "    print('max_diff_grad_W: ' + str(max_relative_diff_W))\n",
    "    max_relative_diff_b = np.amax(np.abs(ground_grad_b - check_linear.gradient['b']) / (ground_grad_b + 1e-8))\n",
    "    print('max_diff_grad_b: ' + str(max_relative_diff_b))\n",
    "\n",
    "    if (max_relative_diff_X >= 1e-7) or (max_relative_diff_W >= 1e-7) or (max_relative_diff_b >= 1e-7):\n",
    "        print('linear.backward might be wrong')\n",
    "    else:\n",
    "        print('linear.backward should be correct')\n",
    "print('##########################')\n",
    "\n",
    "\n",
    "# check_relu.forward\n",
    "hat_X = check_relu.forward(X)\n",
    "ground_hat_X = np.array([[ 0.,          0.99734545,  0.2829785 ],\n",
    " [ 0.,          0.,          1.65143654],\n",
    " [ 0.,          0.,          1.26593626],\n",
    " [ 0.,          0.,          0.        ],\n",
    " [ 1.49138963,  0.,          0.        ]])\n",
    "\n",
    "if (hat_X.shape[0] != 5) or (hat_X.shape[1] != 3):\n",
    "    print('Wrong output dimension of relu.forward')\n",
    "else:\n",
    "    max_relative_diff = np.amax(np.abs(ground_hat_X - hat_X) / (ground_hat_X + 1e-8))\n",
    "    print('max_diff_output: ' + str(max_relative_diff))\n",
    "    if max_relative_diff >= 1e-7:\n",
    "        print('relu.forward might be wrong')\n",
    "    else:\n",
    "        print('relu.forward should be correct')\n",
    "print('##########################')\n",
    "\n",
    "# check_relu.backward\n",
    "grad_hat_X = np.random.normal(0, 1, (5, 3))\n",
    "grad_X = check_relu.backward(X, grad_hat_X)\n",
    "ground_grad_X = np.array([[-0.,          0.92746243, -0.17363568],\n",
    " [ 0.,          0.,         -0.87953634],\n",
    " [ 0.,         -0.,         -1.72766949],\n",
    " [-0.,          0.,          0.        ],\n",
    " [-0.01183049,  0.,          0.        ]])\n",
    "\n",
    "if (grad_X.shape[0] != 5) or (grad_X.shape[1] != 3):\n",
    "    print('Wrong output dimension of relu.backward')\n",
    "else:\n",
    "    max_relative_diff_X = np.amax(np.abs(ground_grad_X - grad_X) / (ground_grad_X + 1e-8))\n",
    "    print('max_diff_grad_X: ' + str(max_relative_diff_X))\n",
    "\n",
    "    if (max_relative_diff_X >= 1e-7):\n",
    "        print('relu.backward might be wrong')\n",
    "    else:\n",
    "        print('relu.backward should be correct')\n",
    "print('##########################')\n",
    "\n",
    "# check_dropout.forward\n",
    "hat_X = check_dropout.forward(X, is_train = True)\n",
    "\n",
    "\n",
    "# check_dropout.backward\n",
    "grad_hat_X = np.random.normal(0, 1, (5, 3))\n",
    "grad_X = check_dropout.backward(X, grad_hat_X)\n",
    "ground_grad_X = np.array([[ 0.,         -0.39530184, -1.45606984],\n",
    " [-1.22062684, -0.,          0.        ],\n",
    " [ 0.,          1.7354356,   2.53503582],\n",
    " [ 4.21567995, -0.4721789,  -0.46416366],\n",
    " [-2.15627882,  2.32636907,  1.04498015]])\n",
    "\n",
    "if (grad_X.shape[0] != 5) or (grad_X.shape[1] != 3):\n",
    "    print('Wrong output dimension of dropout.backward')\n",
    "else:\n",
    "    max_relative_diff_X = np.amax(np.abs(ground_grad_X - grad_X) / (grad_X + 1e-8))\n",
    "    print('max_diff_grad_X: ' + str(max_relative_diff_X))\n",
    "\n",
    "    if (max_relative_diff_X >= 1e-7):\n",
    "        print('dropout.backward might be wrong')\n",
    "    else:\n",
    "        print('dropout.backward should be correct')\n",
    "print('##########################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.55094716, -2.51399667])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_linear.gradient['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ground_grad_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(np.abs(ground_grad_X - grad_X) / (ground_grad_X + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(np.abs(ground_grad_W - check_linear.gradient['W']) / (ground_grad_W + 1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear.gradient['W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_grad_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_hat_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04343513,  0.22059301],\n",
       "       [ 0.21867861,  0.10040539],\n",
       "       [ 0.03861864,  0.07373686]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_linear.params['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1490732 , -0.09358339]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_linear.params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0856306 ,  0.99734545,  0.2829785 ],\n",
       "       [-1.50629471, -0.57860025,  1.65143654],\n",
       "       [-2.42667924, -0.42891263,  1.26593626],\n",
       "       [-0.8667404 , -0.67888615, -0.09470897],\n",
       "       [ 1.49138963, -0.638902  , -0.44398196]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42525407, -0.2120611 ],\n",
       "       [ 0.15174804, -0.36218431],\n",
       "       [ 0.20957104, -0.57861084],\n",
       "       [ 0.03460477, -0.35992763],\n",
       "       [-0.07256568,  0.1385197 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, check_linear.params['W']) + check_linear.params['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

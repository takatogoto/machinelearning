{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logstic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y = {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    \\n    import argparse\\n    import sys\\n\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--type\", )\\n    parser.add_argument(\"--output\")\\n    args = parser.parse_args()\\n\\n    if args.output:\\n            sys.stdout = open(args.output, \\'w\\')\\n\\n    if not args.type or args.type == \\'binary\\':\\n        run_binary()\\n    \\n    if not args.type or args.type == \\'multiclass\\':\\n        run_multiclass()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Replace TODO with your code\n",
    "#######################################################################\n",
    "def softmax(x):\n",
    "    # avoid overflow\n",
    "\n",
    "    if len(x.shape) ==1:\n",
    "        c = np.max(x,0)\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,0)\n",
    "        y = exp_x / sum_exp_x\n",
    "    else:\n",
    "        c = np.transpose(np.tile(np.max(x,1),(x.shape[1],1)))\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,1)\n",
    "        y = np.transpose(np.divide(exp_x.T,sum_exp_x))\n",
    "\n",
    "    return y \n",
    "\n",
    "def underlog(x):\n",
    "    x = x - np.max(x)\n",
    "    y = x - np.log(np.sum(np.exp(x)))\n",
    "    \n",
    "    return y\n",
    "\n",
    "def binary_train(X, y, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: binary training labels, a N dimensional numpy array where \n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of logistic regression\n",
    "    - b: scalar, which is the bias of logistic regression\n",
    "\n",
    "    Find the optimal parameters w and b for inputs X and y.\n",
    "    Use the *average* of the gradients for all training examples\n",
    "    multiplied by the step_size to update parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "\n",
    "\n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "    for i in range(max_iterations):\n",
    "        z = (np.sum(X * w.T,1) +b)\n",
    "        sgm = sigmoid(z) - y \n",
    "        b = b - step_size * np.sum(sgm,0)\n",
    "        w = w - step_size * np.sum(X * np.tile(sgm, (D,1)).T, 0)\n",
    "        \n",
    "    assert w.shape == (D,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of binary predictions: {0, 1}\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    y = np.sum(X * w, 1) + b\n",
    "    preds = np.round(sigmoid(y))\n",
    "    \n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n",
    "\n",
    "\n",
    "def multinomial_train(X, y, C, \n",
    "                     w0=None, \n",
    "                     b0=None, \n",
    "                     step_size=0.5, \n",
    "                     max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array where\n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - C: number of classes in the data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: C-by-D weight matrix of multinomial logistic regression, where \n",
    "    C is the number of classes and D is the dimensionality of features.\n",
    "    - b: bias vector of length C, where C is the number of classes\n",
    "\n",
    "    Implement multinomial logistic regression for multiclass \n",
    "    classification. Again use the *average* of the gradients for all training \n",
    "    examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    You may find it useful to use a special (one-hot) representation of the labels, \n",
    "    where each label y_i is represented as a row of zeros with a single 1 in\n",
    "    the column, that corresponds to the class y_i.\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    \n",
    "    # class label to one-hot\n",
    "    yc = np.identity(C)[y].astype(int)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        wxb = np.dot(X,w.T) + np.tile(b,(N,1))\n",
    "        smyx = np.multiply(np.tile((softmax(wxb) - yc),(D,1))\n",
    "                           ,np.tile(X.ravel(order='F'),(C,1)).T).reshape(D, N, C)      \n",
    "        w = w - step_size * np.sum(smyx,1).T\n",
    "        b = b - step_size * np.sum((softmax(wxb) - yc),0)\n",
    "\n",
    "        \n",
    "    assert w.shape == (C, D)\n",
    "    assert b.shape == (C,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def multinomial_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained multinomial classifier\n",
    "    - b: bias terms of the trained multinomial classifier\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of multiclass predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes\n",
    "\n",
    "    Make predictions for multinomial classifier.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"   \n",
    "    # deterministic prediction == no need to sigmoid\n",
    "    yc = np.dot(X, w.T) +  np.tile(b,(N,1))\n",
    "    # one-hot to class label\n",
    "    preds = np.argmax(yc, axis = 1)\n",
    "\n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def OVR_train(X, y, C, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array, \n",
    "    indicating the labels of each training point\n",
    "    - C: number of classes in the data\n",
    "    - w0: initial value of weight matrix\n",
    "    - b0: initial value of bias term\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: a C-by-D weight matrix of OVR logistic regression\n",
    "    - b: bias vector of length C\n",
    "\n",
    "    Implement multiclass classification using one-versus-rest with binary logistic \n",
    "    regression as the black-box. Recall that the one-versus-rest classifier is \n",
    "    trained by training C different classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    for c in range(C):\n",
    "        yc = (y==c).astype(int)\n",
    "        w[c,:], b[c] = binary_train(X, yc, w0=w[c,:], b0=b[c], step_size=0.5, max_iterations=1000)\n",
    "    \n",
    "    \n",
    "    assert w.shape == (C, D), 'wrong shape of weights matrix'\n",
    "    assert b.shape == (C,), 'wrong shape of bias terms vector'\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def OVR_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained OVR model\n",
    "    - b: bias terms of the trained OVR model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: vector of class label predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes.\n",
    "\n",
    "    Make predictions using OVR strategy and probability predictions from binary\n",
    "    classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    yc = np.dot(X, w.T) +  np.tile(b,(N,1))\n",
    "    preds = np.argmax(yc, axis = 1)\n",
    "    \n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# DO NOT MODIFY THE CODE BELOW \n",
    "#######################################################################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)\n",
    "\n",
    "def run_binary():\n",
    "    from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "    print('Performing binary classification on synthetic data')\n",
    "    X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "        \n",
    "    w, b = binary_train(X_train, y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Performing binary classification on binarized MNIST')\n",
    "    X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "    binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "    binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "    w, b = binary_train(X_train, binarized_y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(binarized_y_train, train_preds),\n",
    "             accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "def run_multiclass():\n",
    "    from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                            toy_data_multiclass_5_classes, \\\n",
    "                            data_loader_mnist \n",
    "    \n",
    "    datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                        'Synthetic data', 3), \n",
    "                (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "                (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "    for data, name, num_classes in datasets:\n",
    "        print('%s: %d class classification' % (name, num_classes))\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        print('One-versus-rest:')\n",
    "        w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "        preds = OVR_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "        print('Multinomial:')\n",
    "        w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "        preds = multinomial_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "\n",
    "\"\"\"if __name__ == '__main__':\n",
    "    \n",
    "    import argparse\n",
    "    import sys\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--type\", )\n",
    "    parser.add_argument(\"--output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.output:\n",
    "            sys.stdout = open(args.output, 'w')\n",
    "\n",
    "    if not args.type or args.type == 'binary':\n",
    "        run_binary()\n",
    "    \n",
    "    if not args.type or args.type == 'multiclass':\n",
    "        run_multiclass()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import toy_data_binary, \\\n",
    "                   data_loader_mnist \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confrim binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing binary classification on synthetic data\n",
      "train acc: 0.994286, test acc: 1.000000\n",
      "SK-learn\n",
      "train acc: 0.994286, test acc: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Performing binary classification on synthetic data')\n",
    "X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "    \n",
    "w, b = binary_train(X_train, y_train)\n",
    "\n",
    "train_preds = binary_predict(X_train, w, b)\n",
    "preds = binary_predict(X_test, w, b)\n",
    "print('train acc: %f, test acc: %f' % \n",
    "        (accuracy_score(y_train, train_preds),\n",
    "         accuracy_score(y_test, preds)))\n",
    "\n",
    "# sklearn\n",
    "lr = LogisticRegression(C=1e15, max_iter=1000, solver=\"saga\")\n",
    "lr.fit(X_train, y_train)\n",
    "print('SK-learn')\n",
    "print('train acc: %f, test acc: %f' % \n",
    "      (lr.score(X_train, y_train),lr.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing binary classification on binarized MNIST\n",
      "train acc: 0.695000, test acc: 0.695000\n",
      "SK-learn\n",
      "train acc: 0.899600, test acc: 0.812000\n"
     ]
    }
   ],
   "source": [
    "print('Performing binary classification on binarized MNIST')\n",
    "X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "w, b = binary_train(X_train, binarized_y_train, max_iterations=1000)\n",
    "    \n",
    "train_preds = binary_predict(X_train, w, b)\n",
    "preds = binary_predict(X_test, w, b)\n",
    "print('train acc: %f, test acc: %f' % \n",
    "        (accuracy_score(binarized_y_train, train_preds),\n",
    "         accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "lrmn = LogisticRegression(C=1e15, max_iter=1000, solver=\"saga\")\n",
    "lrmn.fit(X_train, binarized_y_train)\n",
    "print('SK-learn')\n",
    "print('train acc: %f, test acc: %f' % \n",
    "      (lrmn.score(X_train, binarized_y_train),lrmn.score(X_test, binarized_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confrim multi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data: 3 class classification\n",
      "One-versus-rest:\n",
      "train acc: 0.871429, test acc: 0.846667\n",
      "Multinomial:\n",
      "train acc: 0.897143, test acc: 0.846667\n",
      "SK-learn:\n",
      "train acc: 0.908571, test acc: 0.840000\n",
      "Synthetic data: 5 class classification\n",
      "One-versus-rest:\n",
      "train acc: 0.697143, test acc: 0.666667\n",
      "Multinomial:\n",
      "train acc: 0.825714, test acc: 0.786667\n",
      "SK-learn:\n",
      "train acc: 0.865714, test acc: 0.846667\n",
      "MNIST: 10 class classification\n",
      "One-versus-rest:\n",
      "train acc: 0.895000, test acc: 0.829000\n",
      "Multinomial:\n",
      "train acc: 0.937600, test acc: 0.856000\n",
      "SK-learn:\n",
      "train acc: 1.000000, test acc: 0.793000\n"
     ]
    }
   ],
   "source": [
    "from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                        toy_data_multiclass_5_classes, \\\n",
    "                        data_loader_mnist \n",
    "\n",
    "datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                    'Synthetic data', 3), \n",
    "            (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "            (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "for data, name, num_classes in datasets:\n",
    "    print('%s: %d class classification' % (name, num_classes))\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    print('One-versus-rest:')\n",
    "    w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "    train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "    preds = OVR_predict(X_test, w=w, b=b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "        (accuracy_score(y_train, train_preds),\n",
    "         accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Multinomial:')\n",
    "    w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "    train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "    preds = multinomial_predict(X_test, w=w, b=b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "        (accuracy_score(y_train, train_preds),\n",
    "         accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('SK-learn:')\n",
    "    lrmt = LogisticRegression(C=1e15, max_iter=1000, solver=\"saga\")\n",
    "    lrmt.fit(X_train, y_train)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "      (lrmt.score(X_train, y_train),lrmt.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logstic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \\'__main__\\':\\n    \\n    import argparse\\n    import sys\\n\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--type\", )\\n    parser.add_argument(\"--output\")\\n    args = parser.parse_args()\\n\\n    if args.output:\\n            sys.stdout = open(args.output, \\'w\\')\\n\\n    if not args.type or args.type == \\'binary\\':\\n        run_binary()\\n\\n    if not args.type or args.type == \\'multiclass\\':\\n        run_multiclass()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Replace TODO with your code\n",
    "#######################################################################\n",
    "\n",
    "def tobinary(y):\n",
    "    return (2*y - 1).astype(int)\n",
    "\n",
    "def binary_train(X, y, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: binary training labels, a N dimensional numpy array where \n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of logistic regression\n",
    "    - b: scalar, which is the bias of logistic regression\n",
    "\n",
    "    Find the optimal parameters w and b for inputs X and y.\n",
    "    Use the *average* of the gradients for all training examples\n",
    "    multiplied by the step_size to update parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "\n",
    "\n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of binary predictions: {0, 1}\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"      \n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n",
    "\n",
    "\n",
    "def multinomial_train(X, y, C, \n",
    "                     w0=None, \n",
    "                     b0=None, \n",
    "                     step_size=0.5, \n",
    "                     max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array where\n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - C: number of classes in the data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: C-by-D weight matrix of multinomial logistic regression, where \n",
    "    C is the number of classes and D is the dimensionality of features.\n",
    "    - b: bias vector of length C, where C is the number of classes\n",
    "\n",
    "    Implement multinomial logistic regression for multiclass \n",
    "    classification. Again use the *average* of the gradients for all training \n",
    "    examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    You may find it useful to use a special (one-hot) representation of the labels, \n",
    "    where each label y_i is represented as a row of zeros with a single 1 in\n",
    "    the column, that corresponds to the class y_i.\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "\n",
    "    assert w.shape == (C, D)\n",
    "    assert b.shape == (C,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def multinomial_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained multinomial classifier\n",
    "    - b: bias terms of the trained multinomial classifier\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of multiclass predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes\n",
    "\n",
    "    Make predictions for multinomial classifier.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"   \n",
    "\n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def OVR_train(X, y, C, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array, \n",
    "    indicating the labels of each training point\n",
    "    - C: number of classes in the data\n",
    "    - w0: initial value of weight matrix\n",
    "    - b0: initial value of bias term\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: a C-by-D weight matrix of OVR logistic regression\n",
    "    - b: bias vector of length C\n",
    "\n",
    "    Implement multiclass classification using one-versus-rest with binary logistic \n",
    "    regression as the black-box. Recall that the one-versus-rest classifier is \n",
    "    trained by training C different classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    assert w.shape == (C, D), 'wrong shape of weights matrix'\n",
    "    assert b.shape == (C,), 'wrong shape of bias terms vector'\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def OVR_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained OVR model\n",
    "    - b: bias terms of the trained OVR model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: vector of class label predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes.\n",
    "\n",
    "    Make predictions using OVR strategy and probability predictions from binary\n",
    "    classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "\n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# DO NOT MODIFY THE CODE BELOW \n",
    "#######################################################################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)\n",
    "\n",
    "def run_binary():\n",
    "    from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "    print('Performing binary classification on synthetic data')\n",
    "    X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "        \n",
    "    w, b = binary_train(X_train, y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Performing binary classification on binarized MNIST')\n",
    "    X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "    binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "    binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "    w, b = binary_train(X_train, binarized_y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(binarized_y_train, train_preds),\n",
    "             accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "def run_multiclass():\n",
    "    from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                            toy_data_multiclass_5_classes, \\\n",
    "                            data_loader_mnist \n",
    "    \n",
    "    datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                        'Synthetic data', 3), \n",
    "                (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "                (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "    for data, name, num_classes in datasets:\n",
    "        print('%s: %d class classification' % (name, num_classes))\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        print('One-versus-rest:')\n",
    "        w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "        preds = OVR_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "        print('Multinomial:')\n",
    "        w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "        preds = multinomial_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    import argparse\n",
    "    import sys\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--type\", )\n",
    "    parser.add_argument(\"--output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.output:\n",
    "            sys.stdout = open(args.output, 'w')\n",
    "\n",
    "    if not args.type or args.type == 'binary':\n",
    "        run_binary()\n",
    "\n",
    "    if not args.type or args.type == 'multiclass':\n",
    "        run_multiclass()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)\n",
    "\n",
    "def run_binary():\n",
    "    from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "    print('Performing binary classification on synthetic data')\n",
    "    X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "        \n",
    "    w, b = binary_train(X_train, y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Performing binary classification on binarized MNIST')\n",
    "    X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "    binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "    binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "    w, b = binary_train(X_train, binarized_y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(binarized_y_train, train_preds),\n",
    "             accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "def run_multiclass():\n",
    "    from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                            toy_data_multiclass_5_classes, \\\n",
    "                            data_loader_mnist \n",
    "    \n",
    "    datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                        'Synthetic data', 3), \n",
    "                (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "                (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "    for data, name, num_classes in datasets:\n",
    "        print('%s: %d class classification' % (name, num_classes))\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        print('One-versus-rest:')\n",
    "        w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "        preds = OVR_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "        print('Multinomial:')\n",
    "        w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "        preds = multinomial_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing binary classification on synthetic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "print('Performing binary classification on synthetic data')\n",
    "X_train, X_test, y_train, y_test = toy_data_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= False\n",
    "1 if a else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = -1 <= 0\n",
    "0 if y else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def y0ne(a):\n",
    "    return -1 if a == 0 else 1\n",
    "y0ne(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1])\n",
    "y1 = 2*y - np.ones(len(y)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y1 +1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1])\n",
    "def tobinary(y):\n",
    "    return (2*y - 1).astype(int)\n",
    "y1 = tobinary(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09135723, -0.55892185],\n",
       "       [-0.9425491 , -1.12970685],\n",
       "       [ 1.0654138 , -0.24751864],\n",
       "       [ 0.54117719,  1.14927333],\n",
       "       [ 0.91515201,  1.04416088],\n",
       "       [-0.94611747, -0.27272357],\n",
       "       [ 1.05845318, -0.04946371],\n",
       "       [ 0.88265028,  0.09612078],\n",
       "       [ 0.75749448,  1.20650897],\n",
       "       [ 1.054367  ,  0.65020118],\n",
       "       [-0.48477958, -0.92323325],\n",
       "       [-0.93222036,  0.05921843],\n",
       "       [-1.19845466,  2.5733598 ],\n",
       "       [ 0.80463752, -0.56407863],\n",
       "       [ 0.88558004, -0.53086877],\n",
       "       [ 0.88289999, -1.51574411],\n",
       "       [-0.91447135,  0.01392929],\n",
       "       [ 0.1243599 , -0.09671311],\n",
       "       [ 0.78492314,  0.4933179 ],\n",
       "       [ 1.09711512, -0.06575026],\n",
       "       [ 1.23054129,  0.24380071],\n",
       "       [ 1.03449554,  0.38240975],\n",
       "       [-0.89035836, -0.90431663],\n",
       "       [ 1.30998108,  2.52693243],\n",
       "       [-0.80990157,  1.10330188],\n",
       "       [-1.28907266,  0.05572491],\n",
       "       [ 1.12140739,  1.42050425],\n",
       "       [-1.06521415, -0.2403254 ],\n",
       "       [ 0.89253975,  0.58392819],\n",
       "       [-1.34106055,  0.22388402],\n",
       "       [-0.37432319,  0.25602973],\n",
       "       [-0.78056001,  0.47146836],\n",
       "       [ 0.84887127,  0.64548418],\n",
       "       [-0.91199809,  0.72576662],\n",
       "       [-1.44208466,  1.1593298 ],\n",
       "       [ 1.12480193, -2.04173487],\n",
       "       [ 1.0505379 , -0.03955515],\n",
       "       [-0.84969495, -2.08192941],\n",
       "       [ 1.08161545, -1.58390282],\n",
       "       [ 1.23107861, -0.62696706],\n",
       "       [ 1.01048861,  0.70835645],\n",
       "       [-1.25959098, -0.34268759],\n",
       "       [ 0.67414784, -0.65262398],\n",
       "       [ 1.00144856, -1.57022472],\n",
       "       [-1.32286669,  1.20121392],\n",
       "       [-1.59100644,  1.66547444],\n",
       "       [-1.13860542, -0.6115178 ],\n",
       "       [ 1.19164958, -0.81693567],\n",
       "       [-0.7600945 , -2.65096981],\n",
       "       [-0.98029294, -0.76325916],\n",
       "       [ 0.90094923, -0.08973569],\n",
       "       [ 1.30479721, -0.5176113 ],\n",
       "       [ 1.00955769, -1.0265153 ],\n",
       "       [-0.61853067, -1.50314295],\n",
       "       [-1.09023883, -0.42098448],\n",
       "       [ 1.010952  , -1.00162001],\n",
       "       [-1.06846061, -0.92216532],\n",
       "       [-1.07723599,  0.72167206],\n",
       "       [-1.5599636 ,  0.27996863],\n",
       "       [-0.79163004, -0.37482081],\n",
       "       [-1.09562311, -0.5100164 ],\n",
       "       [ 0.97851382, -0.07016571],\n",
       "       [-1.24491261,  0.39445214],\n",
       "       [ 0.82812358, -0.52286003],\n",
       "       [ 0.88113182, -0.48943944],\n",
       "       [ 0.79867059,  0.87736229],\n",
       "       [-0.9455862 ,  1.01437007],\n",
       "       [ 1.01660847, -0.47874862],\n",
       "       [ 1.07049495, -0.73093004],\n",
       "       [-0.66860279,  0.39913611],\n",
       "       [ 1.2484801 , -0.70012081],\n",
       "       [ 1.00988095,  0.177701  ],\n",
       "       [ 0.72366907,  0.45918008],\n",
       "       [ 1.08640195, -0.30917212],\n",
       "       [ 1.09372897,  0.61058575],\n",
       "       [-0.94905217,  1.49604431],\n",
       "       [ 1.23882605,  0.20838281],\n",
       "       [ 1.08328019, -1.66096093],\n",
       "       [-0.76014051,  0.77086519],\n",
       "       [ 0.97984524,  2.16325472],\n",
       "       [-0.99850772,  0.75698862],\n",
       "       [-1.10001591,  0.4134349 ],\n",
       "       [-0.68220667,  0.34758171],\n",
       "       [-1.09200644,  0.11422765],\n",
       "       [ 1.43583593,  0.78580016],\n",
       "       [-1.14164582,  0.30780177],\n",
       "       [-0.82149177, -0.41187697],\n",
       "       [-0.7328541 ,  0.83033582],\n",
       "       [ 1.04837782,  0.59065483],\n",
       "       [-0.53913124,  0.04852163],\n",
       "       [ 0.78551469, -0.44618343],\n",
       "       [ 1.30040524, -0.94939889],\n",
       "       [-1.2395034 ,  0.10643023],\n",
       "       [ 1.12992943,  1.0536418 ],\n",
       "       [-1.51451591, -0.15567724],\n",
       "       [ 1.00297323,  0.8896308 ],\n",
       "       [-1.06832924, -0.86399077],\n",
       "       [ 0.99730813, -0.83235557],\n",
       "       [-0.83634374, -1.4084613 ],\n",
       "       [-0.8875511 , -1.25111358],\n",
       "       [ 1.08834829, -1.34445051],\n",
       "       [ 1.01373042,  2.29889812],\n",
       "       [-0.79972561,  0.17086544],\n",
       "       [ 0.99826639, -1.00414077],\n",
       "       [ 1.14615228,  2.63238206],\n",
       "       [ 0.89137688,  0.22378795],\n",
       "       [-1.00393896, -0.65183611],\n",
       "       [-1.14643001, -0.84984437],\n",
       "       [-0.89558531, -0.70434369],\n",
       "       [ 0.79501409, -0.63773998],\n",
       "       [-0.92483791,  0.65436566],\n",
       "       [-1.14693156,  1.52955032],\n",
       "       [-1.26499274, -1.66940528],\n",
       "       [ 1.16393825, -0.42018682],\n",
       "       [-0.92415724, -2.69688664],\n",
       "       [ 1.04588789, -0.51728845],\n",
       "       [-0.80839502, -0.54685894],\n",
       "       [-0.97322104,  0.33849641],\n",
       "       [-0.90829179,  0.75138712],\n",
       "       [-1.35028788,  0.18676676],\n",
       "       [ 0.9647473 , -2.42387933],\n",
       "       [-0.88208403, -1.2446547 ],\n",
       "       [-1.06410783,  0.63278187],\n",
       "       [-0.55552737, -0.48712538],\n",
       "       [-0.69911879, -0.48760622],\n",
       "       [ 0.93358509,  0.14671369],\n",
       "       [ 1.30194497,  0.83392215],\n",
       "       [-1.4706461 , -0.18398334],\n",
       "       [-1.01013009,  0.48100923],\n",
       "       [ 1.02128923, -0.35929209],\n",
       "       [ 0.97735964, -0.57074629],\n",
       "       [ 1.10073379,  0.24938368],\n",
       "       [-0.90044009, -0.77781669],\n",
       "       [ 0.96918579,  1.44697788],\n",
       "       [ 0.91833547,  0.50727403],\n",
       "       [-1.23808157, -0.37144087],\n",
       "       [ 1.11206509, -0.28178461],\n",
       "       [ 0.93770879,  0.70775194],\n",
       "       [ 0.71133321, -2.19880596],\n",
       "       [ 0.91848912, -0.68105166],\n",
       "       [-0.9709183 , -1.69246463],\n",
       "       [-0.86171921, -0.14436041],\n",
       "       [-0.81500057,  0.98269098],\n",
       "       [-0.85021086, -0.47765745],\n",
       "       [ 0.89293355,  1.5033983 ],\n",
       "       [-0.57580863,  1.27155509],\n",
       "       [ 0.84079948,  1.57745328],\n",
       "       [ 1.16465352,  0.62180996],\n",
       "       [ 0.99537184, -0.09917586],\n",
       "       [ 0.98157739,  1.40934744],\n",
       "       [ 1.1808636 ,  0.27902153],\n",
       "       [ 0.98409922, -0.6763923 ],\n",
       "       [-1.02125399,  0.44426331],\n",
       "       [ 0.91185277,  0.20768769],\n",
       "       [-0.94945983,  0.32613302],\n",
       "       [ 1.12348275, -0.0164229 ],\n",
       "       [ 0.88360927, -1.12272202],\n",
       "       [ 0.89404678,  0.18660912],\n",
       "       [ 0.92020106,  0.67481949],\n",
       "       [ 1.04350028,  0.02975614],\n",
       "       [ 1.21266857, -1.02123282],\n",
       "       [ 1.00665041,  0.04643655],\n",
       "       [ 1.06963441,  1.16929559],\n",
       "       [-0.92689755, -0.26888869],\n",
       "       [ 1.0328482 ,  2.45530014],\n",
       "       [ 1.03172177,  0.65854427],\n",
       "       [ 0.88776361,  0.45675322],\n",
       "       [-0.88181237, -1.35168461],\n",
       "       [-1.13030846, -1.08106333],\n",
       "       [ 1.04568295,  0.20292302],\n",
       "       [ 0.92162728,  0.64272276],\n",
       "       [ 0.89057536, -0.79829724],\n",
       "       [-0.45550266,  0.19808476],\n",
       "       [-1.08726066,  1.14375404],\n",
       "       [-1.13700881,  0.27045683],\n",
       "       [-0.7500733 ,  0.61593561],\n",
       "       [ 1.07135112,  1.11729583],\n",
       "       [-1.2199662 , -2.07339023],\n",
       "       [-1.4314837 ,  0.57707213],\n",
       "       [-1.17528708,  0.63859246],\n",
       "       [-1.45253706, -0.1580079 ],\n",
       "       [-0.69246422,  0.60600995],\n",
       "       [ 0.782836  , -0.09529553],\n",
       "       [-1.13772599,  0.01843393],\n",
       "       [ 0.96106461,  0.84064355],\n",
       "       [-0.92381507,  0.37730049],\n",
       "       [-1.2683197 ,  0.82317058],\n",
       "       [ 0.81311226,  1.38215899],\n",
       "       [ 0.85808534,  0.47141556],\n",
       "       [-0.71731599,  0.33366211],\n",
       "       [-0.91358189, -0.44429326],\n",
       "       [-1.21007536, -0.20304539],\n",
       "       [ 0.96978514, -0.55547712],\n",
       "       [-0.90405909, -0.85608383],\n",
       "       [ 1.08693936, -0.04015795],\n",
       "       [ 1.21957581, -0.44643361],\n",
       "       [ 1.11515767, -1.48556037],\n",
       "       [-0.72158939, -0.0376347 ],\n",
       "       [-1.35628428, -0.38455554],\n",
       "       [ 1.0138267 , -0.17694723],\n",
       "       [ 1.10578435, -0.43449623],\n",
       "       [-1.54493237,  0.74326409],\n",
       "       [ 1.08912812, -0.51604473],\n",
       "       [ 0.98189242, -0.96697614],\n",
       "       [-1.35920865,  1.66902153],\n",
       "       [ 1.13803464,  1.55050049],\n",
       "       [ 1.06282557, -0.57563783],\n",
       "       [-1.31252336, -1.22212781],\n",
       "       [ 0.97053892,  0.27157884],\n",
       "       [-0.91925208, -1.11057585],\n",
       "       [-0.8617894 , -1.65485667],\n",
       "       [-1.34766344, -0.57366201],\n",
       "       [-1.25014813,  0.48937456],\n",
       "       [-1.23128488, -0.62481858],\n",
       "       [-0.97459666, -1.47858625],\n",
       "       [ 0.91625916, -0.8946073 ],\n",
       "       [-1.38535477,  0.69620636],\n",
       "       [ 1.20949587, -0.68198425],\n",
       "       [ 1.03467664,  0.16645221],\n",
       "       [ 0.89982471,  0.70030988],\n",
       "       [-1.19754918,  0.33760266],\n",
       "       [-0.28114101,  0.12922118],\n",
       "       [ 0.82477904,  1.08078073],\n",
       "       [ 1.01073064, -0.03468489],\n",
       "       [-0.89889447, -1.10652591],\n",
       "       [ 0.98738447, -1.07008477],\n",
       "       [-1.40910848, -0.51121568],\n",
       "       [-1.29557905,  0.09933231],\n",
       "       [-0.69149528, -0.60398519],\n",
       "       [-0.76287695, -2.03812454],\n",
       "       [ 1.08816852,  0.74625357],\n",
       "       [-0.9761063 , -1.71016839],\n",
       "       [ 1.21560828,  1.79768653],\n",
       "       [-1.41305399,  0.86960592],\n",
       "       [ 0.93021246, -1.27674858],\n",
       "       [ 0.85499116, -0.70317643],\n",
       "       [ 1.21696932,  1.79587767],\n",
       "       [-1.42710413, -0.97876372],\n",
       "       [-0.74734278, -0.72574381],\n",
       "       [-0.9390464 ,  1.00629281],\n",
       "       [-1.11443068, -0.26987494],\n",
       "       [ 0.85006133,  0.3520554 ],\n",
       "       [-0.36074952, -0.12578692],\n",
       "       [-0.78448999,  0.52980418],\n",
       "       [ 0.7510388 ,  1.88115707],\n",
       "       [-0.84166227, -0.05023811],\n",
       "       [ 0.9151545 ,  1.27845186],\n",
       "       [-1.135916  , -0.90756366],\n",
       "       [-1.34909434, -0.40807537],\n",
       "       [ 1.03977626,  0.42961822],\n",
       "       [ 1.09769444,  0.28586539],\n",
       "       [-0.77030157, -0.54342477],\n",
       "       [-1.22553705, -0.36361221],\n",
       "       [ 0.86106096, -0.72713718],\n",
       "       [-0.90542112,  1.16778206],\n",
       "       [ 0.86878687,  2.56008454],\n",
       "       [ 0.97918306, -0.99835404],\n",
       "       [ 0.86473694, -0.04771136],\n",
       "       [-0.95513771, -0.18490214],\n",
       "       [-1.27368969,  1.75479418],\n",
       "       [-1.190664  , -1.4066611 ],\n",
       "       [-0.97166942,  0.47897983],\n",
       "       [ 1.05686495, -0.38770156],\n",
       "       [ 0.92070267, -0.46227529],\n",
       "       [ 1.16695754, -0.16711808],\n",
       "       [ 1.08424841, -0.36283856],\n",
       "       [-1.22858032, -1.2899609 ],\n",
       "       [-0.20616084, -1.12905177],\n",
       "       [ 1.00346958, -1.37931923],\n",
       "       [ 0.93332141,  0.28916864],\n",
       "       [ 1.02817822, -0.07443343],\n",
       "       [ 1.04401102, -0.63738713],\n",
       "       [-0.77603608, -0.83095012],\n",
       "       [-1.36354953, -0.30954644],\n",
       "       [ 0.75887496, -1.2803044 ],\n",
       "       [ 0.9730563 ,  0.57258278],\n",
       "       [ 0.85317095, -0.57117899],\n",
       "       [-0.4464504 ,  1.09150685],\n",
       "       [ 1.13520671,  1.4437646 ],\n",
       "       [ 1.18479915,  0.26705027],\n",
       "       [-1.08512629, -1.66152006],\n",
       "       [ 0.92490406,  0.12810441],\n",
       "       [ 0.87605045,  1.39935544],\n",
       "       [-0.54767569,  0.07331797],\n",
       "       [-0.82873486, -0.98960482],\n",
       "       [-0.60420239, -1.00808631],\n",
       "       [ 1.01850719, -0.56246678],\n",
       "       [-0.75995303, -0.86041337],\n",
       "       [ 1.00074228,  0.19109907],\n",
       "       [ 0.97137829, -0.43973106],\n",
       "       [-1.02172589, -0.05558467],\n",
       "       [ 0.8007642 , -0.53099696],\n",
       "       [ 0.82151354, -0.97587325],\n",
       "       [ 0.98460428, -0.61278869],\n",
       "       [ 1.15553913,  0.19655478],\n",
       "       [ 0.98378613,  0.36867331],\n",
       "       [ 0.80425556,  0.92463368],\n",
       "       [-0.8945323 ,  0.32692737],\n",
       "       [-1.09398866, -1.29507877],\n",
       "       [-0.98124087, -0.53975968],\n",
       "       [-1.36098487, -0.23093453],\n",
       "       [ 0.88417545, -0.44550252],\n",
       "       [ 1.01606913,  0.38019785],\n",
       "       [-0.86189294,  0.71095997],\n",
       "       [ 1.02595327,  1.18839327],\n",
       "       [-1.46911904, -0.24574306],\n",
       "       [ 1.13579113,  1.79455786],\n",
       "       [-1.06911184, -0.05694562],\n",
       "       [ 0.94320459,  0.33445679],\n",
       "       [-1.01045527,  0.82940558],\n",
       "       [ 0.87298771, -0.76779757],\n",
       "       [-0.56869956,  0.18186626],\n",
       "       [ 1.07744525, -0.18687164],\n",
       "       [-1.00773766, -1.12548905],\n",
       "       [-0.71885363, -0.25497722],\n",
       "       [-0.44954591, -1.77872025],\n",
       "       [ 0.84689973,  0.02688584],\n",
       "       [ 1.15848323,  0.42545756],\n",
       "       [-1.04706873, -0.7737892 ],\n",
       "       [-1.19313211, -1.84087423],\n",
       "       [-0.77183713, -0.05429487],\n",
       "       [-0.81740869, -0.52452027],\n",
       "       [ 0.89829947, -0.62314053],\n",
       "       [ 0.81534372, -1.52552517],\n",
       "       [-0.89578224,  2.27069286],\n",
       "       [-0.45944135, -1.2110162 ],\n",
       "       [-0.92938707, -0.57677133],\n",
       "       [-1.13983957,  0.8711247 ],\n",
       "       [ 0.96186368,  0.44001445],\n",
       "       [-0.7585994 ,  1.75227044],\n",
       "       [ 1.13453878,  0.82048218],\n",
       "       [ 0.92725874, -0.24123606],\n",
       "       [ 1.19349009, -1.88954073],\n",
       "       [-1.5033905 ,  0.50091719],\n",
       "       [-0.85919791,  0.71299843],\n",
       "       [ 1.14982698, -0.55519953],\n",
       "       [-1.07161271,  1.24608519],\n",
       "       [ 1.23285605, -0.45230632],\n",
       "       [ 1.16693731,  1.07363175],\n",
       "       [ 1.10157285, -0.50205422],\n",
       "       [ 1.02880162, -1.15836469],\n",
       "       [-0.93682992, -1.7025836 ],\n",
       "       [-1.02101504, -1.87079192],\n",
       "       [ 0.98484915,  0.56976728],\n",
       "       [ 0.95450571, -0.21398884],\n",
       "       [ 0.96475748,  1.19504663],\n",
       "       [-1.28444572, -0.03275327],\n",
       "       [ 0.83250194, -1.4480139 ],\n",
       "       [-1.13524126, -0.23894805],\n",
       "       [-0.96762896,  0.04808495]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.09135723e+00,  5.74508969e-02,  2.06541380e+00,  1.54117719e+00,\n",
       "        1.91515201e+00,  5.38825266e-02,  2.05845318e+00,  1.88265028e+00,\n",
       "        1.75749448e+00,  2.05436700e+00,  5.15220425e-01,  6.77796376e-02,\n",
       "       -1.98454663e-01,  1.80463752e+00,  1.88558004e+00,  1.88289999e+00,\n",
       "        8.55286536e-02,  1.12435990e+00,  1.78492314e+00,  2.09711512e+00,\n",
       "        2.23054129e+00,  2.03449554e+00,  1.09641637e-01,  2.30998108e+00,\n",
       "        1.90098429e-01, -2.89072656e-01,  2.12140739e+00, -6.52141464e-02,\n",
       "        1.89253975e+00, -3.41060551e-01,  6.25676814e-01,  2.19439988e-01,\n",
       "        1.84887127e+00,  8.80019126e-02, -4.42084656e-01,  2.12480193e+00,\n",
       "        2.05053790e+00,  1.50305051e-01,  2.08161545e+00,  2.23107861e+00,\n",
       "        2.01048861e+00, -2.59590983e-01,  1.67414784e+00,  2.00144856e+00,\n",
       "       -3.22866695e-01, -5.91006439e-01, -1.38605419e-01,  2.19164958e+00,\n",
       "        2.39905502e-01,  1.97070587e-02,  1.90094923e+00,  2.30479721e+00,\n",
       "        2.00955769e+00,  3.81469329e-01, -9.02388337e-02,  2.01095200e+00,\n",
       "       -6.84606147e-02, -7.72359900e-02, -5.59963605e-01,  2.08369965e-01,\n",
       "       -9.56231130e-02,  1.97851382e+00, -2.44912612e-01,  1.82812358e+00,\n",
       "        1.88113182e+00,  1.79867059e+00,  5.44137985e-02,  2.01660847e+00,\n",
       "        2.07049495e+00,  3.31397211e-01,  2.24848010e+00,  2.00988095e+00,\n",
       "        1.72366907e+00,  2.08640195e+00,  2.09372897e+00,  5.09478257e-02,\n",
       "        2.23882605e+00,  2.08328019e+00,  2.39859492e-01,  1.97984524e+00,\n",
       "        1.49228297e-03, -1.00015914e-01,  3.17793334e-01, -9.20064369e-02,\n",
       "        2.43583593e+00, -1.41645820e-01,  1.78508233e-01,  2.67145903e-01,\n",
       "        2.04837782e+00,  4.60868755e-01,  1.78551469e+00,  2.30040524e+00,\n",
       "       -2.39503400e-01,  2.12992943e+00, -5.14515912e-01,  2.00297323e+00,\n",
       "       -6.83292377e-02,  1.99730813e+00,  1.63656262e-01,  1.12448898e-01,\n",
       "        2.08834829e+00,  2.01373042e+00,  2.00274388e-01,  1.99826639e+00,\n",
       "        2.14615228e+00,  1.89137688e+00, -3.93895560e-03, -1.46430007e-01,\n",
       "        1.04414689e-01,  1.79501409e+00,  7.51620850e-02, -1.46931557e-01,\n",
       "       -2.64992736e-01,  2.16393825e+00,  7.58427608e-02,  2.04588789e+00,\n",
       "        1.91604983e-01,  2.67789587e-02,  9.17082077e-02, -3.50287881e-01,\n",
       "        1.96474730e+00,  1.17915973e-01, -6.41078317e-02,  4.44472630e-01,\n",
       "        3.00881212e-01,  1.93358509e+00,  2.30194497e+00, -4.70646102e-01,\n",
       "       -1.01300914e-02,  2.02128923e+00,  1.97735964e+00,  2.10073379e+00,\n",
       "        9.95599101e-02,  1.96918579e+00,  1.91833547e+00, -2.38081572e-01,\n",
       "        2.11206509e+00,  1.93770879e+00,  1.71133321e+00,  1.91848912e+00,\n",
       "        2.90817046e-02,  1.38280785e-01,  1.84999430e-01,  1.49789136e-01,\n",
       "        1.89293355e+00,  4.24191365e-01,  1.84079948e+00,  2.16465352e+00,\n",
       "        1.99537184e+00,  1.98157739e+00,  2.18086360e+00,  1.98409922e+00,\n",
       "       -2.12539880e-02,  1.91185277e+00,  5.05401659e-02,  2.12348275e+00,\n",
       "        1.88360927e+00,  1.89404678e+00,  1.92020106e+00,  2.04350028e+00,\n",
       "        2.21266857e+00,  2.00665041e+00,  2.06963441e+00,  7.31024514e-02,\n",
       "        2.03284820e+00,  2.03172177e+00,  1.88776361e+00,  1.18187628e-01,\n",
       "       -1.30308460e-01,  2.04568295e+00,  1.92162728e+00,  1.89057536e+00,\n",
       "        5.44497339e-01, -8.72606554e-02, -1.37008815e-01,  2.49926699e-01,\n",
       "        2.07135112e+00, -2.19966204e-01, -4.31483700e-01, -1.75287081e-01,\n",
       "       -4.52537060e-01,  3.07535777e-01,  1.78283600e+00, -1.37725992e-01,\n",
       "        1.96106461e+00,  7.61849303e-02, -2.68319696e-01,  1.81311226e+00,\n",
       "        1.85808534e+00,  2.82684010e-01,  8.64181079e-02, -2.10075362e-01,\n",
       "        1.96978514e+00,  9.59409100e-02,  2.08693936e+00,  2.21957581e+00,\n",
       "        2.11515767e+00,  2.78410611e-01, -3.56284275e-01,  2.01382670e+00,\n",
       "        2.10578435e+00, -5.44932371e-01,  2.08912812e+00,  1.98189242e+00,\n",
       "       -3.59208651e-01,  2.13803464e+00,  2.06282557e+00, -3.12523358e-01,\n",
       "        1.97053892e+00,  8.07479163e-02,  1.38210602e-01, -3.47663440e-01,\n",
       "       -2.50148132e-01, -2.31284880e-01,  2.54033361e-02,  1.91625916e+00,\n",
       "       -3.85354773e-01,  2.20949587e+00,  2.03467664e+00,  1.89982471e+00,\n",
       "       -1.97549182e-01,  7.18858988e-01,  1.82477904e+00,  2.01073064e+00,\n",
       "        1.01105534e-01,  1.98738447e+00, -4.09108480e-01, -2.95579046e-01,\n",
       "        3.08504718e-01,  2.37123053e-01,  2.08816852e+00,  2.38936971e-02,\n",
       "        2.21560828e+00, -4.13053988e-01,  1.93021246e+00,  1.85499116e+00,\n",
       "        2.21696932e+00, -4.27104128e-01,  2.52657219e-01,  6.09535992e-02,\n",
       "       -1.14430680e-01,  1.85006133e+00,  6.39250484e-01,  2.15510012e-01,\n",
       "        1.75103880e+00,  1.58337730e-01,  1.91515450e+00, -1.35916002e-01,\n",
       "       -3.49094341e-01,  2.03977626e+00,  2.09769444e+00,  2.29698428e-01,\n",
       "       -2.25537047e-01,  1.86106096e+00,  9.45788775e-02,  1.86878687e+00,\n",
       "        1.97918306e+00,  1.86473694e+00,  4.48622868e-02, -2.73689685e-01,\n",
       "       -1.90663999e-01,  2.83305764e-02,  2.05686495e+00,  1.92070267e+00,\n",
       "        2.16695754e+00,  2.08424841e+00, -2.28580319e-01,  7.93839161e-01,\n",
       "        2.00346958e+00,  1.93332141e+00,  2.02817822e+00,  2.04401102e+00,\n",
       "        2.23963917e-01, -3.63549532e-01,  1.75887496e+00,  1.97305630e+00,\n",
       "        1.85317095e+00,  5.53549599e-01,  2.13520671e+00,  2.18479915e+00,\n",
       "       -8.51262946e-02,  1.92490406e+00,  1.87605045e+00,  4.52324305e-01,\n",
       "        1.71265136e-01,  3.95797607e-01,  2.01850719e+00,  2.40046969e-01,\n",
       "        2.00074228e+00,  1.97137829e+00, -2.17258853e-02,  1.80076420e+00,\n",
       "        1.82151354e+00,  1.98460428e+00,  2.15553913e+00,  1.98378613e+00,\n",
       "        1.80425556e+00,  1.05467700e-01, -9.39886560e-02,  1.87591261e-02,\n",
       "       -3.60984871e-01,  1.88417545e+00,  2.01606913e+00,  1.38107057e-01,\n",
       "        2.02595327e+00, -4.69119038e-01,  2.13579113e+00, -6.91118356e-02,\n",
       "        1.94320459e+00, -1.04552736e-02,  1.87298771e+00,  4.31300444e-01,\n",
       "        2.07744525e+00, -7.73766313e-03,  2.81146374e-01,  5.50454087e-01,\n",
       "        1.84689973e+00,  2.15848323e+00, -4.70687322e-02, -1.93132111e-01,\n",
       "        2.28162873e-01,  1.82591307e-01,  1.89829947e+00,  1.81534372e+00,\n",
       "        1.04217763e-01,  5.40558646e-01,  7.06129343e-02, -1.39839566e-01,\n",
       "        1.96186368e+00,  2.41400596e-01,  2.13453878e+00,  1.92725874e+00,\n",
       "        2.19349009e+00, -5.03390503e-01,  1.40802088e-01,  2.14982698e+00,\n",
       "       -7.16127139e-02,  2.23285605e+00,  2.16693731e+00,  2.10157285e+00,\n",
       "        2.02880162e+00,  6.31700805e-02, -2.10150366e-02,  1.98484915e+00,\n",
       "        1.95450571e+00,  1.96475748e+00, -2.84445721e-01,  1.83250194e+00,\n",
       "       -1.35241263e-01,  3.23710366e-02])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1, 0])\n",
    "b = 1\n",
    "np.sum(X_train * w.T,1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logstic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y = {0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \\'__main__\\':\\n    \\n    import argparse\\n    import sys\\n\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\"--type\", )\\n    parser.add_argument(\"--output\")\\n    args = parser.parse_args()\\n\\n    if args.output:\\n            sys.stdout = open(args.output, \\'w\\')\\n\\n    if not args.type or args.type == \\'binary\\':\\n        run_binary()\\n\\n    if not args.type or args.type == \\'multiclass\\':\\n        run_multiclass()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# Replace TODO with your code\n",
    "#######################################################################\n",
    "def softmax(x):\n",
    "    # avoid overflow\n",
    "\n",
    "    \n",
    "    if len(x.shape) ==1:\n",
    "        c = np.max(x,0)\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,0)\n",
    "        y = exp_x / sum_exp_x\n",
    "    else:\n",
    "        c = np.transpose(np.tile(np.max(x,1),(x.shape[1],1)))\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,1)\n",
    "        y = np.transpose(np.divide(exp_x.T,sum_exp_x))\n",
    "\n",
    "    return y \n",
    "\n",
    "def underlog(x):\n",
    "    x = x - np.max(x)\n",
    "    y = x - np.log(np.sum(np.exp(x)))\n",
    "    \n",
    "    return y\n",
    "\n",
    "def binary_train(X, y, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: binary training labels, a N dimensional numpy array where \n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of logistic regression\n",
    "    - b: scalar, which is the bias of logistic regression\n",
    "\n",
    "    Find the optimal parameters w and b for inputs X and y.\n",
    "    Use the *average* of the gradients for all training examples\n",
    "    multiplied by the step_size to update parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "\n",
    "\n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "    for i in range(max_iterations):\n",
    "        z = (np.sum(X * w.T,1) +b)\n",
    "        sgm = sigmoid(z) - y \n",
    "        b = b - step_size * np.sum(sgm,0)\n",
    "        w = w - step_size * np.sum(X * np.tile(sgm, (D,1)).T, 0)\n",
    "        \n",
    "    assert w.shape == (D,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of binary predictions: {0, 1}\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    y = np.sum(X * w, 1) + b\n",
    "    preds = np.round(sigmoid(y))\n",
    "    \n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n",
    "\n",
    "\n",
    "def multinomial_train(X, y, C, \n",
    "                     w0=None, \n",
    "                     b0=None, \n",
    "                     step_size=0.5, \n",
    "                     max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array where\n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - C: number of classes in the data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: C-by-D weight matrix of multinomial logistic regression, where \n",
    "    C is the number of classes and D is the dimensionality of features.\n",
    "    - b: bias vector of length C, where C is the number of classes\n",
    "\n",
    "    Implement multinomial logistic regression for multiclass \n",
    "    classification. Again use the *average* of the gradients for all training \n",
    "    examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    You may find it useful to use a special (one-hot) representation of the labels, \n",
    "    where each label y_i is represented as a row of zeros with a single 1 in\n",
    "    the column, that corresponds to the class y_i.\n",
    "    \"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "    for i in range(max_iterations):\n",
    "        z = (np.sum(X * w.T,1) +b)\n",
    "        sgm = sigmoid(z) - y \n",
    "        b = b - step_size * np.sum(sgm,0)\n",
    "        w = w - step_size * np.sum(X * np.tile(sgm, (D,1)).T, 0)\n",
    "        \n",
    "    assert w.shape == (C, D)\n",
    "    assert b.shape == (C,)\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def multinomial_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained multinomial classifier\n",
    "    - b: bias terms of the trained multinomial classifier\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N dimensional vector of multiclass predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes\n",
    "\n",
    "    Make predictions for multinomial classifier.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"   \n",
    "\n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def OVR_train(X, y, C, w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array, \n",
    "    indicating the labels of each training point\n",
    "    - C: number of classes in the data\n",
    "    - w0: initial value of weight matrix\n",
    "    - b0: initial value of bias term\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: a C-by-D weight matrix of OVR logistic regression\n",
    "    - b: bias vector of length C\n",
    "\n",
    "    Implement multiclass classification using one-versus-rest with binary logistic \n",
    "    regression as the black-box. Recall that the one-versus-rest classifier is \n",
    "    trained by training C different classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    for c in range(C):\n",
    "        yc = (y==c).astype(int)\n",
    "        w[c,:], b[c] = binary_train(X, yc, w0=w[c,:], b0=b[c], step_size=0.5, max_iterations=1000)\n",
    "    \n",
    "    \n",
    "    assert w.shape == (C, D), 'wrong shape of weights matrix'\n",
    "    assert b.shape == (C,), 'wrong shape of bias terms vector'\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def OVR_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: weights of the trained OVR model\n",
    "    - b: bias terms of the trained OVR model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: vector of class label predictions.\n",
    "    Outputted predictions should be from {0, C - 1}, where\n",
    "    C is the number of classes.\n",
    "\n",
    "    Make predictions using OVR strategy and probability predictions from binary\n",
    "    classifiers. \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    C = w.shape[0]\n",
    "    preds = np.zeros(N) \n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    yc = np.dot(X, w.T) +  np.tile(b,(N,1))\n",
    "    preds = np.argmax(yc, axis = 1)\n",
    "    \n",
    "    assert preds.shape == (N,)\n",
    "    return preds\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# DO NOT MODIFY THE CODE BELOW \n",
    "#######################################################################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)\n",
    "\n",
    "def run_binary():\n",
    "    from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "    print('Performing binary classification on synthetic data')\n",
    "    X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "        \n",
    "    w, b = binary_train(X_train, y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Performing binary classification on binarized MNIST')\n",
    "    X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "    binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "    binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "    w, b = binary_train(X_train, binarized_y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(binarized_y_train, train_preds),\n",
    "             accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "def run_multiclass():\n",
    "    from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                            toy_data_multiclass_5_classes, \\\n",
    "                            data_loader_mnist \n",
    "    \n",
    "    datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                        'Synthetic data', 3), \n",
    "                (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "                (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "    for data, name, num_classes in datasets:\n",
    "        print('%s: %d class classification' % (name, num_classes))\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        print('One-versus-rest:')\n",
    "        w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "        preds = OVR_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "        print('Multinomial:')\n",
    "        w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "        preds = multinomial_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    import argparse\n",
    "    import sys\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--type\", )\n",
    "    parser.add_argument(\"--output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.output:\n",
    "            sys.stdout = open(args.output, 'w')\n",
    "\n",
    "    if not args.type or args.type == 'binary':\n",
    "        run_binary()\n",
    "\n",
    "    if not args.type or args.type == 'multiclass':\n",
    "        run_multiclass()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)\n",
    "\n",
    "def run_binary():\n",
    "    from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "    print('Performing binary classification on synthetic data')\n",
    "    X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "        \n",
    "    w, b = binary_train(X_train, y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "    print('Performing binary classification on binarized MNIST')\n",
    "    X_train, X_test, y_train, y_test = data_loader_mnist()\n",
    "\n",
    "    binarized_y_train = [0 if yi < 5 else 1 for yi in y_train] \n",
    "    binarized_y_test = [0 if yi < 5 else 1 for yi in y_test] \n",
    "    \n",
    "    w, b = binary_train(X_train, binarized_y_train)\n",
    "    \n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(binarized_y_train, train_preds),\n",
    "             accuracy_score(binarized_y_test, preds)))\n",
    "\n",
    "def run_multiclass():\n",
    "    from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                            toy_data_multiclass_5_classes, \\\n",
    "                            data_loader_mnist \n",
    "    \n",
    "    datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                        'Synthetic data', 3), \n",
    "                (toy_data_multiclass_5_classes(), 'Synthetic data', 5), \n",
    "                (data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "    for data, name, num_classes in datasets:\n",
    "        print('%s: %d class classification' % (name, num_classes))\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        print('One-versus-rest:')\n",
    "        w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "        preds = OVR_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))\n",
    "    \n",
    "        print('Multinomial:')\n",
    "        w, b = multinomial_train(X_train, y_train, C=num_classes)\n",
    "        train_preds = multinomial_predict(X_train, w=w, b=b)\n",
    "        preds = multinomial_predict(X_test, w=w, b=b)\n",
    "        print('train acc: %f, test acc: %f' % \n",
    "            (accuracy_score(y_train, train_preds),\n",
    "             accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing binary classification on synthetic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from data_loader import toy_data_binary, \\\n",
    "                            data_loader_mnist \n",
    "\n",
    "print('Performing binary classification on synthetic data')\n",
    "X_train, X_test, y_train, y_test = toy_data_binary()\n",
    "N, D = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= False\n",
    "1 if a else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = -1 <= 0\n",
    "0 if y else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def y0ne(a):\n",
    "    return -1 if a == 0 else 1\n",
    "y0ne(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1])\n",
    "y1 = 2*y - np.ones(len(y)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y1 +1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1, 0, 1])\n",
    "def tobinary(y):\n",
    "    return (2*y - 1).astype(int)\n",
    "y1 = tobinary(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09135723, -0.55892185],\n",
       "       [-0.9425491 , -1.12970685],\n",
       "       [ 1.0654138 , -0.24751864],\n",
       "       [ 0.54117719,  1.14927333],\n",
       "       [ 0.91515201,  1.04416088],\n",
       "       [-0.94611747, -0.27272357],\n",
       "       [ 1.05845318, -0.04946371],\n",
       "       [ 0.88265028,  0.09612078],\n",
       "       [ 0.75749448,  1.20650897],\n",
       "       [ 1.054367  ,  0.65020118],\n",
       "       [-0.48477958, -0.92323325],\n",
       "       [-0.93222036,  0.05921843],\n",
       "       [-1.19845466,  2.5733598 ],\n",
       "       [ 0.80463752, -0.56407863],\n",
       "       [ 0.88558004, -0.53086877],\n",
       "       [ 0.88289999, -1.51574411],\n",
       "       [-0.91447135,  0.01392929],\n",
       "       [ 0.1243599 , -0.09671311],\n",
       "       [ 0.78492314,  0.4933179 ],\n",
       "       [ 1.09711512, -0.06575026],\n",
       "       [ 1.23054129,  0.24380071],\n",
       "       [ 1.03449554,  0.38240975],\n",
       "       [-0.89035836, -0.90431663],\n",
       "       [ 1.30998108,  2.52693243],\n",
       "       [-0.80990157,  1.10330188],\n",
       "       [-1.28907266,  0.05572491],\n",
       "       [ 1.12140739,  1.42050425],\n",
       "       [-1.06521415, -0.2403254 ],\n",
       "       [ 0.89253975,  0.58392819],\n",
       "       [-1.34106055,  0.22388402],\n",
       "       [-0.37432319,  0.25602973],\n",
       "       [-0.78056001,  0.47146836],\n",
       "       [ 0.84887127,  0.64548418],\n",
       "       [-0.91199809,  0.72576662],\n",
       "       [-1.44208466,  1.1593298 ],\n",
       "       [ 1.12480193, -2.04173487],\n",
       "       [ 1.0505379 , -0.03955515],\n",
       "       [-0.84969495, -2.08192941],\n",
       "       [ 1.08161545, -1.58390282],\n",
       "       [ 1.23107861, -0.62696706],\n",
       "       [ 1.01048861,  0.70835645],\n",
       "       [-1.25959098, -0.34268759],\n",
       "       [ 0.67414784, -0.65262398],\n",
       "       [ 1.00144856, -1.57022472],\n",
       "       [-1.32286669,  1.20121392],\n",
       "       [-1.59100644,  1.66547444],\n",
       "       [-1.13860542, -0.6115178 ],\n",
       "       [ 1.19164958, -0.81693567],\n",
       "       [-0.7600945 , -2.65096981],\n",
       "       [-0.98029294, -0.76325916],\n",
       "       [ 0.90094923, -0.08973569],\n",
       "       [ 1.30479721, -0.5176113 ],\n",
       "       [ 1.00955769, -1.0265153 ],\n",
       "       [-0.61853067, -1.50314295],\n",
       "       [-1.09023883, -0.42098448],\n",
       "       [ 1.010952  , -1.00162001],\n",
       "       [-1.06846061, -0.92216532],\n",
       "       [-1.07723599,  0.72167206],\n",
       "       [-1.5599636 ,  0.27996863],\n",
       "       [-0.79163004, -0.37482081],\n",
       "       [-1.09562311, -0.5100164 ],\n",
       "       [ 0.97851382, -0.07016571],\n",
       "       [-1.24491261,  0.39445214],\n",
       "       [ 0.82812358, -0.52286003],\n",
       "       [ 0.88113182, -0.48943944],\n",
       "       [ 0.79867059,  0.87736229],\n",
       "       [-0.9455862 ,  1.01437007],\n",
       "       [ 1.01660847, -0.47874862],\n",
       "       [ 1.07049495, -0.73093004],\n",
       "       [-0.66860279,  0.39913611],\n",
       "       [ 1.2484801 , -0.70012081],\n",
       "       [ 1.00988095,  0.177701  ],\n",
       "       [ 0.72366907,  0.45918008],\n",
       "       [ 1.08640195, -0.30917212],\n",
       "       [ 1.09372897,  0.61058575],\n",
       "       [-0.94905217,  1.49604431],\n",
       "       [ 1.23882605,  0.20838281],\n",
       "       [ 1.08328019, -1.66096093],\n",
       "       [-0.76014051,  0.77086519],\n",
       "       [ 0.97984524,  2.16325472],\n",
       "       [-0.99850772,  0.75698862],\n",
       "       [-1.10001591,  0.4134349 ],\n",
       "       [-0.68220667,  0.34758171],\n",
       "       [-1.09200644,  0.11422765],\n",
       "       [ 1.43583593,  0.78580016],\n",
       "       [-1.14164582,  0.30780177],\n",
       "       [-0.82149177, -0.41187697],\n",
       "       [-0.7328541 ,  0.83033582],\n",
       "       [ 1.04837782,  0.59065483],\n",
       "       [-0.53913124,  0.04852163],\n",
       "       [ 0.78551469, -0.44618343],\n",
       "       [ 1.30040524, -0.94939889],\n",
       "       [-1.2395034 ,  0.10643023],\n",
       "       [ 1.12992943,  1.0536418 ],\n",
       "       [-1.51451591, -0.15567724],\n",
       "       [ 1.00297323,  0.8896308 ],\n",
       "       [-1.06832924, -0.86399077],\n",
       "       [ 0.99730813, -0.83235557],\n",
       "       [-0.83634374, -1.4084613 ],\n",
       "       [-0.8875511 , -1.25111358],\n",
       "       [ 1.08834829, -1.34445051],\n",
       "       [ 1.01373042,  2.29889812],\n",
       "       [-0.79972561,  0.17086544],\n",
       "       [ 0.99826639, -1.00414077],\n",
       "       [ 1.14615228,  2.63238206],\n",
       "       [ 0.89137688,  0.22378795],\n",
       "       [-1.00393896, -0.65183611],\n",
       "       [-1.14643001, -0.84984437],\n",
       "       [-0.89558531, -0.70434369],\n",
       "       [ 0.79501409, -0.63773998],\n",
       "       [-0.92483791,  0.65436566],\n",
       "       [-1.14693156,  1.52955032],\n",
       "       [-1.26499274, -1.66940528],\n",
       "       [ 1.16393825, -0.42018682],\n",
       "       [-0.92415724, -2.69688664],\n",
       "       [ 1.04588789, -0.51728845],\n",
       "       [-0.80839502, -0.54685894],\n",
       "       [-0.97322104,  0.33849641],\n",
       "       [-0.90829179,  0.75138712],\n",
       "       [-1.35028788,  0.18676676],\n",
       "       [ 0.9647473 , -2.42387933],\n",
       "       [-0.88208403, -1.2446547 ],\n",
       "       [-1.06410783,  0.63278187],\n",
       "       [-0.55552737, -0.48712538],\n",
       "       [-0.69911879, -0.48760622],\n",
       "       [ 0.93358509,  0.14671369],\n",
       "       [ 1.30194497,  0.83392215],\n",
       "       [-1.4706461 , -0.18398334],\n",
       "       [-1.01013009,  0.48100923],\n",
       "       [ 1.02128923, -0.35929209],\n",
       "       [ 0.97735964, -0.57074629],\n",
       "       [ 1.10073379,  0.24938368],\n",
       "       [-0.90044009, -0.77781669],\n",
       "       [ 0.96918579,  1.44697788],\n",
       "       [ 0.91833547,  0.50727403],\n",
       "       [-1.23808157, -0.37144087],\n",
       "       [ 1.11206509, -0.28178461],\n",
       "       [ 0.93770879,  0.70775194],\n",
       "       [ 0.71133321, -2.19880596],\n",
       "       [ 0.91848912, -0.68105166],\n",
       "       [-0.9709183 , -1.69246463],\n",
       "       [-0.86171921, -0.14436041],\n",
       "       [-0.81500057,  0.98269098],\n",
       "       [-0.85021086, -0.47765745],\n",
       "       [ 0.89293355,  1.5033983 ],\n",
       "       [-0.57580863,  1.27155509],\n",
       "       [ 0.84079948,  1.57745328],\n",
       "       [ 1.16465352,  0.62180996],\n",
       "       [ 0.99537184, -0.09917586],\n",
       "       [ 0.98157739,  1.40934744],\n",
       "       [ 1.1808636 ,  0.27902153],\n",
       "       [ 0.98409922, -0.6763923 ],\n",
       "       [-1.02125399,  0.44426331],\n",
       "       [ 0.91185277,  0.20768769],\n",
       "       [-0.94945983,  0.32613302],\n",
       "       [ 1.12348275, -0.0164229 ],\n",
       "       [ 0.88360927, -1.12272202],\n",
       "       [ 0.89404678,  0.18660912],\n",
       "       [ 0.92020106,  0.67481949],\n",
       "       [ 1.04350028,  0.02975614],\n",
       "       [ 1.21266857, -1.02123282],\n",
       "       [ 1.00665041,  0.04643655],\n",
       "       [ 1.06963441,  1.16929559],\n",
       "       [-0.92689755, -0.26888869],\n",
       "       [ 1.0328482 ,  2.45530014],\n",
       "       [ 1.03172177,  0.65854427],\n",
       "       [ 0.88776361,  0.45675322],\n",
       "       [-0.88181237, -1.35168461],\n",
       "       [-1.13030846, -1.08106333],\n",
       "       [ 1.04568295,  0.20292302],\n",
       "       [ 0.92162728,  0.64272276],\n",
       "       [ 0.89057536, -0.79829724],\n",
       "       [-0.45550266,  0.19808476],\n",
       "       [-1.08726066,  1.14375404],\n",
       "       [-1.13700881,  0.27045683],\n",
       "       [-0.7500733 ,  0.61593561],\n",
       "       [ 1.07135112,  1.11729583],\n",
       "       [-1.2199662 , -2.07339023],\n",
       "       [-1.4314837 ,  0.57707213],\n",
       "       [-1.17528708,  0.63859246],\n",
       "       [-1.45253706, -0.1580079 ],\n",
       "       [-0.69246422,  0.60600995],\n",
       "       [ 0.782836  , -0.09529553],\n",
       "       [-1.13772599,  0.01843393],\n",
       "       [ 0.96106461,  0.84064355],\n",
       "       [-0.92381507,  0.37730049],\n",
       "       [-1.2683197 ,  0.82317058],\n",
       "       [ 0.81311226,  1.38215899],\n",
       "       [ 0.85808534,  0.47141556],\n",
       "       [-0.71731599,  0.33366211],\n",
       "       [-0.91358189, -0.44429326],\n",
       "       [-1.21007536, -0.20304539],\n",
       "       [ 0.96978514, -0.55547712],\n",
       "       [-0.90405909, -0.85608383],\n",
       "       [ 1.08693936, -0.04015795],\n",
       "       [ 1.21957581, -0.44643361],\n",
       "       [ 1.11515767, -1.48556037],\n",
       "       [-0.72158939, -0.0376347 ],\n",
       "       [-1.35628428, -0.38455554],\n",
       "       [ 1.0138267 , -0.17694723],\n",
       "       [ 1.10578435, -0.43449623],\n",
       "       [-1.54493237,  0.74326409],\n",
       "       [ 1.08912812, -0.51604473],\n",
       "       [ 0.98189242, -0.96697614],\n",
       "       [-1.35920865,  1.66902153],\n",
       "       [ 1.13803464,  1.55050049],\n",
       "       [ 1.06282557, -0.57563783],\n",
       "       [-1.31252336, -1.22212781],\n",
       "       [ 0.97053892,  0.27157884],\n",
       "       [-0.91925208, -1.11057585],\n",
       "       [-0.8617894 , -1.65485667],\n",
       "       [-1.34766344, -0.57366201],\n",
       "       [-1.25014813,  0.48937456],\n",
       "       [-1.23128488, -0.62481858],\n",
       "       [-0.97459666, -1.47858625],\n",
       "       [ 0.91625916, -0.8946073 ],\n",
       "       [-1.38535477,  0.69620636],\n",
       "       [ 1.20949587, -0.68198425],\n",
       "       [ 1.03467664,  0.16645221],\n",
       "       [ 0.89982471,  0.70030988],\n",
       "       [-1.19754918,  0.33760266],\n",
       "       [-0.28114101,  0.12922118],\n",
       "       [ 0.82477904,  1.08078073],\n",
       "       [ 1.01073064, -0.03468489],\n",
       "       [-0.89889447, -1.10652591],\n",
       "       [ 0.98738447, -1.07008477],\n",
       "       [-1.40910848, -0.51121568],\n",
       "       [-1.29557905,  0.09933231],\n",
       "       [-0.69149528, -0.60398519],\n",
       "       [-0.76287695, -2.03812454],\n",
       "       [ 1.08816852,  0.74625357],\n",
       "       [-0.9761063 , -1.71016839],\n",
       "       [ 1.21560828,  1.79768653],\n",
       "       [-1.41305399,  0.86960592],\n",
       "       [ 0.93021246, -1.27674858],\n",
       "       [ 0.85499116, -0.70317643],\n",
       "       [ 1.21696932,  1.79587767],\n",
       "       [-1.42710413, -0.97876372],\n",
       "       [-0.74734278, -0.72574381],\n",
       "       [-0.9390464 ,  1.00629281],\n",
       "       [-1.11443068, -0.26987494],\n",
       "       [ 0.85006133,  0.3520554 ],\n",
       "       [-0.36074952, -0.12578692],\n",
       "       [-0.78448999,  0.52980418],\n",
       "       [ 0.7510388 ,  1.88115707],\n",
       "       [-0.84166227, -0.05023811],\n",
       "       [ 0.9151545 ,  1.27845186],\n",
       "       [-1.135916  , -0.90756366],\n",
       "       [-1.34909434, -0.40807537],\n",
       "       [ 1.03977626,  0.42961822],\n",
       "       [ 1.09769444,  0.28586539],\n",
       "       [-0.77030157, -0.54342477],\n",
       "       [-1.22553705, -0.36361221],\n",
       "       [ 0.86106096, -0.72713718],\n",
       "       [-0.90542112,  1.16778206],\n",
       "       [ 0.86878687,  2.56008454],\n",
       "       [ 0.97918306, -0.99835404],\n",
       "       [ 0.86473694, -0.04771136],\n",
       "       [-0.95513771, -0.18490214],\n",
       "       [-1.27368969,  1.75479418],\n",
       "       [-1.190664  , -1.4066611 ],\n",
       "       [-0.97166942,  0.47897983],\n",
       "       [ 1.05686495, -0.38770156],\n",
       "       [ 0.92070267, -0.46227529],\n",
       "       [ 1.16695754, -0.16711808],\n",
       "       [ 1.08424841, -0.36283856],\n",
       "       [-1.22858032, -1.2899609 ],\n",
       "       [-0.20616084, -1.12905177],\n",
       "       [ 1.00346958, -1.37931923],\n",
       "       [ 0.93332141,  0.28916864],\n",
       "       [ 1.02817822, -0.07443343],\n",
       "       [ 1.04401102, -0.63738713],\n",
       "       [-0.77603608, -0.83095012],\n",
       "       [-1.36354953, -0.30954644],\n",
       "       [ 0.75887496, -1.2803044 ],\n",
       "       [ 0.9730563 ,  0.57258278],\n",
       "       [ 0.85317095, -0.57117899],\n",
       "       [-0.4464504 ,  1.09150685],\n",
       "       [ 1.13520671,  1.4437646 ],\n",
       "       [ 1.18479915,  0.26705027],\n",
       "       [-1.08512629, -1.66152006],\n",
       "       [ 0.92490406,  0.12810441],\n",
       "       [ 0.87605045,  1.39935544],\n",
       "       [-0.54767569,  0.07331797],\n",
       "       [-0.82873486, -0.98960482],\n",
       "       [-0.60420239, -1.00808631],\n",
       "       [ 1.01850719, -0.56246678],\n",
       "       [-0.75995303, -0.86041337],\n",
       "       [ 1.00074228,  0.19109907],\n",
       "       [ 0.97137829, -0.43973106],\n",
       "       [-1.02172589, -0.05558467],\n",
       "       [ 0.8007642 , -0.53099696],\n",
       "       [ 0.82151354, -0.97587325],\n",
       "       [ 0.98460428, -0.61278869],\n",
       "       [ 1.15553913,  0.19655478],\n",
       "       [ 0.98378613,  0.36867331],\n",
       "       [ 0.80425556,  0.92463368],\n",
       "       [-0.8945323 ,  0.32692737],\n",
       "       [-1.09398866, -1.29507877],\n",
       "       [-0.98124087, -0.53975968],\n",
       "       [-1.36098487, -0.23093453],\n",
       "       [ 0.88417545, -0.44550252],\n",
       "       [ 1.01606913,  0.38019785],\n",
       "       [-0.86189294,  0.71095997],\n",
       "       [ 1.02595327,  1.18839327],\n",
       "       [-1.46911904, -0.24574306],\n",
       "       [ 1.13579113,  1.79455786],\n",
       "       [-1.06911184, -0.05694562],\n",
       "       [ 0.94320459,  0.33445679],\n",
       "       [-1.01045527,  0.82940558],\n",
       "       [ 0.87298771, -0.76779757],\n",
       "       [-0.56869956,  0.18186626],\n",
       "       [ 1.07744525, -0.18687164],\n",
       "       [-1.00773766, -1.12548905],\n",
       "       [-0.71885363, -0.25497722],\n",
       "       [-0.44954591, -1.77872025],\n",
       "       [ 0.84689973,  0.02688584],\n",
       "       [ 1.15848323,  0.42545756],\n",
       "       [-1.04706873, -0.7737892 ],\n",
       "       [-1.19313211, -1.84087423],\n",
       "       [-0.77183713, -0.05429487],\n",
       "       [-0.81740869, -0.52452027],\n",
       "       [ 0.89829947, -0.62314053],\n",
       "       [ 0.81534372, -1.52552517],\n",
       "       [-0.89578224,  2.27069286],\n",
       "       [-0.45944135, -1.2110162 ],\n",
       "       [-0.92938707, -0.57677133],\n",
       "       [-1.13983957,  0.8711247 ],\n",
       "       [ 0.96186368,  0.44001445],\n",
       "       [-0.7585994 ,  1.75227044],\n",
       "       [ 1.13453878,  0.82048218],\n",
       "       [ 0.92725874, -0.24123606],\n",
       "       [ 1.19349009, -1.88954073],\n",
       "       [-1.5033905 ,  0.50091719],\n",
       "       [-0.85919791,  0.71299843],\n",
       "       [ 1.14982698, -0.55519953],\n",
       "       [-1.07161271,  1.24608519],\n",
       "       [ 1.23285605, -0.45230632],\n",
       "       [ 1.16693731,  1.07363175],\n",
       "       [ 1.10157285, -0.50205422],\n",
       "       [ 1.02880162, -1.15836469],\n",
       "       [-0.93682992, -1.7025836 ],\n",
       "       [-1.02101504, -1.87079192],\n",
       "       [ 0.98484915,  0.56976728],\n",
       "       [ 0.95450571, -0.21398884],\n",
       "       [ 0.96475748,  1.19504663],\n",
       "       [-1.28444572, -0.03275327],\n",
       "       [ 0.83250194, -1.4480139 ],\n",
       "       [-1.13524126, -0.23894805],\n",
       "       [-0.96762896,  0.04808495]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.09135723e+00,  5.74508969e-02,  2.06541380e+00,  1.54117719e+00,\n",
       "        1.91515201e+00,  5.38825266e-02,  2.05845318e+00,  1.88265028e+00,\n",
       "        1.75749448e+00,  2.05436700e+00,  5.15220425e-01,  6.77796376e-02,\n",
       "       -1.98454663e-01,  1.80463752e+00,  1.88558004e+00,  1.88289999e+00,\n",
       "        8.55286536e-02,  1.12435990e+00,  1.78492314e+00,  2.09711512e+00,\n",
       "        2.23054129e+00,  2.03449554e+00,  1.09641637e-01,  2.30998108e+00,\n",
       "        1.90098429e-01, -2.89072656e-01,  2.12140739e+00, -6.52141464e-02,\n",
       "        1.89253975e+00, -3.41060551e-01,  6.25676814e-01,  2.19439988e-01,\n",
       "        1.84887127e+00,  8.80019126e-02, -4.42084656e-01,  2.12480193e+00,\n",
       "        2.05053790e+00,  1.50305051e-01,  2.08161545e+00,  2.23107861e+00,\n",
       "        2.01048861e+00, -2.59590983e-01,  1.67414784e+00,  2.00144856e+00,\n",
       "       -3.22866695e-01, -5.91006439e-01, -1.38605419e-01,  2.19164958e+00,\n",
       "        2.39905502e-01,  1.97070587e-02,  1.90094923e+00,  2.30479721e+00,\n",
       "        2.00955769e+00,  3.81469329e-01, -9.02388337e-02,  2.01095200e+00,\n",
       "       -6.84606147e-02, -7.72359900e-02, -5.59963605e-01,  2.08369965e-01,\n",
       "       -9.56231130e-02,  1.97851382e+00, -2.44912612e-01,  1.82812358e+00,\n",
       "        1.88113182e+00,  1.79867059e+00,  5.44137985e-02,  2.01660847e+00,\n",
       "        2.07049495e+00,  3.31397211e-01,  2.24848010e+00,  2.00988095e+00,\n",
       "        1.72366907e+00,  2.08640195e+00,  2.09372897e+00,  5.09478257e-02,\n",
       "        2.23882605e+00,  2.08328019e+00,  2.39859492e-01,  1.97984524e+00,\n",
       "        1.49228297e-03, -1.00015914e-01,  3.17793334e-01, -9.20064369e-02,\n",
       "        2.43583593e+00, -1.41645820e-01,  1.78508233e-01,  2.67145903e-01,\n",
       "        2.04837782e+00,  4.60868755e-01,  1.78551469e+00,  2.30040524e+00,\n",
       "       -2.39503400e-01,  2.12992943e+00, -5.14515912e-01,  2.00297323e+00,\n",
       "       -6.83292377e-02,  1.99730813e+00,  1.63656262e-01,  1.12448898e-01,\n",
       "        2.08834829e+00,  2.01373042e+00,  2.00274388e-01,  1.99826639e+00,\n",
       "        2.14615228e+00,  1.89137688e+00, -3.93895560e-03, -1.46430007e-01,\n",
       "        1.04414689e-01,  1.79501409e+00,  7.51620850e-02, -1.46931557e-01,\n",
       "       -2.64992736e-01,  2.16393825e+00,  7.58427608e-02,  2.04588789e+00,\n",
       "        1.91604983e-01,  2.67789587e-02,  9.17082077e-02, -3.50287881e-01,\n",
       "        1.96474730e+00,  1.17915973e-01, -6.41078317e-02,  4.44472630e-01,\n",
       "        3.00881212e-01,  1.93358509e+00,  2.30194497e+00, -4.70646102e-01,\n",
       "       -1.01300914e-02,  2.02128923e+00,  1.97735964e+00,  2.10073379e+00,\n",
       "        9.95599101e-02,  1.96918579e+00,  1.91833547e+00, -2.38081572e-01,\n",
       "        2.11206509e+00,  1.93770879e+00,  1.71133321e+00,  1.91848912e+00,\n",
       "        2.90817046e-02,  1.38280785e-01,  1.84999430e-01,  1.49789136e-01,\n",
       "        1.89293355e+00,  4.24191365e-01,  1.84079948e+00,  2.16465352e+00,\n",
       "        1.99537184e+00,  1.98157739e+00,  2.18086360e+00,  1.98409922e+00,\n",
       "       -2.12539880e-02,  1.91185277e+00,  5.05401659e-02,  2.12348275e+00,\n",
       "        1.88360927e+00,  1.89404678e+00,  1.92020106e+00,  2.04350028e+00,\n",
       "        2.21266857e+00,  2.00665041e+00,  2.06963441e+00,  7.31024514e-02,\n",
       "        2.03284820e+00,  2.03172177e+00,  1.88776361e+00,  1.18187628e-01,\n",
       "       -1.30308460e-01,  2.04568295e+00,  1.92162728e+00,  1.89057536e+00,\n",
       "        5.44497339e-01, -8.72606554e-02, -1.37008815e-01,  2.49926699e-01,\n",
       "        2.07135112e+00, -2.19966204e-01, -4.31483700e-01, -1.75287081e-01,\n",
       "       -4.52537060e-01,  3.07535777e-01,  1.78283600e+00, -1.37725992e-01,\n",
       "        1.96106461e+00,  7.61849303e-02, -2.68319696e-01,  1.81311226e+00,\n",
       "        1.85808534e+00,  2.82684010e-01,  8.64181079e-02, -2.10075362e-01,\n",
       "        1.96978514e+00,  9.59409100e-02,  2.08693936e+00,  2.21957581e+00,\n",
       "        2.11515767e+00,  2.78410611e-01, -3.56284275e-01,  2.01382670e+00,\n",
       "        2.10578435e+00, -5.44932371e-01,  2.08912812e+00,  1.98189242e+00,\n",
       "       -3.59208651e-01,  2.13803464e+00,  2.06282557e+00, -3.12523358e-01,\n",
       "        1.97053892e+00,  8.07479163e-02,  1.38210602e-01, -3.47663440e-01,\n",
       "       -2.50148132e-01, -2.31284880e-01,  2.54033361e-02,  1.91625916e+00,\n",
       "       -3.85354773e-01,  2.20949587e+00,  2.03467664e+00,  1.89982471e+00,\n",
       "       -1.97549182e-01,  7.18858988e-01,  1.82477904e+00,  2.01073064e+00,\n",
       "        1.01105534e-01,  1.98738447e+00, -4.09108480e-01, -2.95579046e-01,\n",
       "        3.08504718e-01,  2.37123053e-01,  2.08816852e+00,  2.38936971e-02,\n",
       "        2.21560828e+00, -4.13053988e-01,  1.93021246e+00,  1.85499116e+00,\n",
       "        2.21696932e+00, -4.27104128e-01,  2.52657219e-01,  6.09535992e-02,\n",
       "       -1.14430680e-01,  1.85006133e+00,  6.39250484e-01,  2.15510012e-01,\n",
       "        1.75103880e+00,  1.58337730e-01,  1.91515450e+00, -1.35916002e-01,\n",
       "       -3.49094341e-01,  2.03977626e+00,  2.09769444e+00,  2.29698428e-01,\n",
       "       -2.25537047e-01,  1.86106096e+00,  9.45788775e-02,  1.86878687e+00,\n",
       "        1.97918306e+00,  1.86473694e+00,  4.48622868e-02, -2.73689685e-01,\n",
       "       -1.90663999e-01,  2.83305764e-02,  2.05686495e+00,  1.92070267e+00,\n",
       "        2.16695754e+00,  2.08424841e+00, -2.28580319e-01,  7.93839161e-01,\n",
       "        2.00346958e+00,  1.93332141e+00,  2.02817822e+00,  2.04401102e+00,\n",
       "        2.23963917e-01, -3.63549532e-01,  1.75887496e+00,  1.97305630e+00,\n",
       "        1.85317095e+00,  5.53549599e-01,  2.13520671e+00,  2.18479915e+00,\n",
       "       -8.51262946e-02,  1.92490406e+00,  1.87605045e+00,  4.52324305e-01,\n",
       "        1.71265136e-01,  3.95797607e-01,  2.01850719e+00,  2.40046969e-01,\n",
       "        2.00074228e+00,  1.97137829e+00, -2.17258853e-02,  1.80076420e+00,\n",
       "        1.82151354e+00,  1.98460428e+00,  2.15553913e+00,  1.98378613e+00,\n",
       "        1.80425556e+00,  1.05467700e-01, -9.39886560e-02,  1.87591261e-02,\n",
       "       -3.60984871e-01,  1.88417545e+00,  2.01606913e+00,  1.38107057e-01,\n",
       "        2.02595327e+00, -4.69119038e-01,  2.13579113e+00, -6.91118356e-02,\n",
       "        1.94320459e+00, -1.04552736e-02,  1.87298771e+00,  4.31300444e-01,\n",
       "        2.07744525e+00, -7.73766313e-03,  2.81146374e-01,  5.50454087e-01,\n",
       "        1.84689973e+00,  2.15848323e+00, -4.70687322e-02, -1.93132111e-01,\n",
       "        2.28162873e-01,  1.82591307e-01,  1.89829947e+00,  1.81534372e+00,\n",
       "        1.04217763e-01,  5.40558646e-01,  7.06129343e-02, -1.39839566e-01,\n",
       "        1.96186368e+00,  2.41400596e-01,  2.13453878e+00,  1.92725874e+00,\n",
       "        2.19349009e+00, -5.03390503e-01,  1.40802088e-01,  2.14982698e+00,\n",
       "       -7.16127139e-02,  2.23285605e+00,  2.16693731e+00,  2.10157285e+00,\n",
       "        2.02880162e+00,  6.31700805e-02, -2.10150366e-02,  1.98484915e+00,\n",
       "        1.95450571e+00,  1.96475748e+00, -2.84445721e-01,  1.83250194e+00,\n",
       "       -1.35241263e-01,  3.23710366e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1, 0])\n",
    "b = 1\n",
    "np.sum(X_train * w.T,1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.26894142, 0.73105858])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2],[3,4], [-1,-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 3,  4],\n",
       "       [-1, -2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([2,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = np.tile(b, (D,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [3, 4],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4],\n",
       "       [3, 4],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * np.tile(b,(D,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.60151319598086\n",
      "8.10702163578771\n",
      "7.917556825309926\n",
      "7.882984841067894\n",
      "7.873871215143685\n",
      "7.871115948987946\n",
      "7.870226378119814\n",
      "7.86992898218911\n",
      "7.869827610634751\n",
      "7.869792671524879\n",
      "7.869780551624985\n",
      "7.869776331567634\n",
      "7.869774858927504\n",
      "7.869774344362835\n",
      "7.869774164427351\n",
      "7.86977410147808\n",
      "7.869774079449774\n",
      "7.869774071740022\n",
      "7.869774069041412\n",
      "7.869774068096775\n",
      "7.869774067766097\n",
      "7.869774067650339\n",
      "7.869774067609819\n",
      "7.869774067595633\n",
      "7.869774067590669\n",
      "7.869774067588928\n",
      "7.869774067588321\n",
      "7.869774067588107\n",
      "7.869774067588033\n",
      "7.869774067588007\n",
      "7.869774067588001\n",
      "7.8697740675879935\n",
      "7.869774067587992\n",
      "7.869774067587995\n",
      "7.869774067587993\n",
      "7.869774067587992\n",
      "7.869774067587995\n",
      "7.869774067587994\n",
      "7.869774067587993\n",
      "7.86977406758799\n",
      "7.869774067587991\n",
      "7.86977406758799\n",
      "7.869774067587992\n",
      "7.869774067587992\n",
      "7.869774067587989\n",
      "7.869774067587994\n",
      "7.869774067587993\n",
      "7.869774067587992\n",
      "7.869774067587992\n",
      "7.8697740675879935\n",
      "7.86977406758799\n",
      "7.869774067587992\n",
      "7.8697740675879935\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n",
      "7.869774067587993\n"
     ]
    }
   ],
   "source": [
    "X=X_train\n",
    "y=y_train\n",
    "\n",
    "step_size=0.5\n",
    "max_iterations=1000\n",
    "\n",
    "y = tobinary(y)\n",
    "N, D = X.shape\n",
    "\n",
    "\n",
    "w = np.zeros(D)\n",
    "#w = np.ones(D)\n",
    "\n",
    "    \n",
    "b = 0\n",
    "\n",
    "\n",
    "    ## w = w + step * sum(σ(-yn(wtx + b)) ynxn)\n",
    "    ## b = b + step * sum(σ(-yn(wtx + b)) yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "F = 0.0\n",
    "for i in range(max_iterations):\n",
    "    z = y * (np.sum(X * w.T,1) +b)\n",
    "    sgm = sigmoid(-z) * y\n",
    "    b1 = b + step_size * np.sum(sgm,0) /N\n",
    "    w = w + step_size * np.sum(X * np.tile(sgm, (D,1)).T, 0)/N\n",
    "    b = b1\n",
    "    fz = np.multiply((np.sum(X * w, 1) + b),y)\n",
    "    F = np.sum(np.log(1 + np.exp(-z)))\n",
    "    if i%100 ==0:\n",
    "        print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5\n",
      "2.5000021392477048\n",
      "2.500004504155641\n",
      "2.5000071185186927\n",
      "2.5000100086380384\n",
      "2.500013203583367\n",
      "2.5000167354811595\n",
      "2.5000206398303564\n",
      "2.500024955845572\n",
      "2.5000297268254825\n",
      "2.5000350005388587\n",
      "2.5000408296103975\n",
      "2.500047271868196\n",
      "2.500054390574977\n",
      "2.500062254387789\n",
      "2.500070936740553\n",
      "2.5000805140521734\n",
      "2.500091061597886\n",
      "2.5001026447883934\n",
      "2.500115301490823\n",
      "2.5001290069647046\n",
      "2.5001436052194936\n",
      "2.5001586759085663\n",
      "2.5001732786526274\n",
      "2.5001854682626146\n",
      "2.5001913954921253\n",
      "2.50018370707216\n",
      "2.500148942814878\n",
      "2.5000642169894483\n",
      "2.499896752914183\n",
      "2.4996235364166863\n",
      "2.499339598202104\n",
      "2.4997007230836172\n",
      "2.5034786947861125\n",
      "2.5199147624987575\n",
      "2.570223858070088\n",
      "2.672352691198323\n",
      "2.8080527701342\n",
      "2.948487989256663\n",
      "3.08026034125395\n",
      "3.1987197288274936\n",
      "3.3021382233694467\n",
      "3.3900124263208924\n",
      "3.4626358153634564\n",
      "3.5209040820203965\n",
      "3.5661275858164014\n",
      "3.599841803554935\n",
      "3.6236438363967585\n",
      "3.639072800746972\n",
      "3.647535969632872\n",
      "3.6502727086186373\n",
      "3.648345136582887\n",
      "3.642645353318086\n",
      "3.6339115143250185\n",
      "3.6227475548690764\n",
      "3.60964339166509\n",
      "3.594993859621748\n",
      "3.579115564803595\n",
      "3.5622613902786644\n",
      "3.544632701045679\n",
      "3.526389449217704\n",
      "3.5076584435225784\n",
      "3.488540058207561\n",
      "3.4691136404424143\n",
      "3.4494418470421313\n",
      "3.4295741091569836\n",
      "3.4095493919971354\n",
      "3.3893983878602034\n",
      "3.369145255578856\n",
      "3.348808998142724\n",
      "3.3284045524449746\n",
      "3.307943650464272\n",
      "3.2874354992726733\n",
      "3.2668873176247724\n",
      "3.246304759137946\n",
      "3.2256922458736166\n",
      "3.2050532311827933\n",
      "3.184390406742658\n",
      "3.1637058655845167\n",
      "3.1430012304345514\n",
      "3.122277754725703\n",
      "3.101536402086133\n",
      "3.080777908882359\n",
      "3.060002833425783\n",
      "3.039211594686125\n",
      "3.0184045027516744\n",
      "2.9975817828002036\n",
      "2.976743593969162\n",
      "2.9558900442181013\n",
      "2.935021202043359\n",
      "2.9141371057216023\n",
      "2.8932377706144523\n",
      "2.872323194952727\n",
      "2.8513933644294425\n",
      "2.8304482558603388\n",
      "2.8094878401153753\n",
      "2.7885120844811553\n",
      "2.767520954580008\n",
      "2.746514415944629\n",
      "2.725492435326022\n",
      "2.704454981795946\n",
      "2.683402027692028\n",
      "2.6623335494434768\n",
      "2.641249528307349\n",
      "2.6201499510389916\n",
      "2.599034810515378\n",
      "2.577904106326196\n",
      "2.5567578453444963\n",
      "2.5355960422863744\n",
      "2.5144187202672814\n",
      "2.493225911361128\n",
      "2.47201765716721\n",
      "2.4507940093891025\n",
      "2.4295550304289857\n",
      "2.4083007940003194\n",
      "2.3870313857613867\n",
      "2.3657469039718757\n",
      "2.344447460174435\n",
      "2.3231331799029213\n",
      "2.3018042034189152\n",
      "2.2804606864779444\n",
      "2.2591028011267413\n",
      "2.237730736532794\n",
      "2.216344699847346\n",
      "2.1949449171029487\n",
      "2.1735316341465825\n",
      "2.1521051176093016\n",
      "2.130665655913283\n",
      "2.109213560317051\n",
      "2.087749165999599\n",
      "2.066272833183983\n",
      "2.0447849483008644\n",
      "2.023285925192349\n",
      "2.0017762063562805\n",
      "1.9802562642310253\n",
      "1.9587266025205357\n",
      "1.937187757559272\n",
      "1.9156402997163278\n",
      "1.8940848348377788\n",
      "1.8725220057259868\n",
      "1.8509524936542163\n",
      "1.829377019914517\n",
      "1.8077963473964043\n",
      "1.786211282193337\n",
      "1.7646226752334937\n",
      "1.7430314239307212\n",
      "1.7214384738508668\n",
      "1.6998448203880137\n",
      "1.6782515104443263\n",
      "1.6566596441063564\n",
      "1.6350703763097634\n",
      "1.613484918483346\n",
      "1.5919045401622376\n",
      "1.5703305705589343\n",
      "1.5487644000795695\n",
      "1.5272074817715313\n",
      "1.5056613326870663\n",
      "1.484127535146047\n",
      "1.4626077378794546\n",
      "1.441103657033481\n",
      "1.4196170770124124\n",
      "1.3981498511366242\n",
      "1.3767039020901741\n",
      "1.3552812221305237\n",
      "1.3338838730310028\n",
      "1.312513985724622\n",
      "1.2911737596158874\n",
      "1.2698654615253062\n",
      "1.2485914242293852\n",
      "1.2273540445571036\n",
      "1.2061557810021282\n",
      "1.1849991508085176\n",
      "1.1638867264862833\n",
      "1.1428211317120973\n",
      "1.1218050365696386\n",
      "1.1008411520835975\n",
      "1.0799322240013567\n",
      "1.0590810257768153\n",
      "1.0382903507118337\n",
      "1.0175630032124132\n",
      "0.9969017891190666\n",
      "0.9763095050739885\n",
      "0.9557889268916204\n",
      "0.9353427969041794\n",
      "0.9149738102597648\n",
      "0.894684600157828\n",
      "0.874477722015258\n",
      "0.8543556365661806\n",
      "0.834320691909901\n",
      "0.8143751045344854\n",
      "0.794520939358268\n",
      "0.7747600888485314\n",
      "0.7550942512957151\n",
      "0.7355249083432768\n",
      "0.7160533018979907\n",
      "0.6966804105734576\n",
      "0.6774069258515469\n",
      "0.6582332281829216\n",
      "0.6391593632895697\n",
      "0.6201850189803203\n",
      "0.6013095028457878\n",
      "0.5825317212633383\n",
      "0.563850160217205\n",
      "0.5452628685253631\n",
      "0.5267674441653998\n",
      "0.5083610245084883\n",
      "0.4900402814059904\n",
      "0.47180142222968013\n",
      "0.4536401981463125\n",
      "0.4355519211122273\n",
      "0.4175314913053251\n",
      "0.3995734369702239\n",
      "0.3816719689363295\n",
      "0.36382105237393947\n",
      "0.3460144986728172\n",
      "0.3282460806482307\n",
      "0.31050967458212786\n",
      "0.29279943286364957\n",
      "0.2751099911641444\n",
      "0.25743671411387553\n",
      "0.2397759832700465\n",
      "0.2221255306888209\n",
      "0.20448482052586442\n",
      "0.1868554796575495\n",
      "0.16924177618560804\n",
      "0.15165114169737237\n",
      "0.1340947291392054\n",
      "0.11658799298467348\n",
      "0.09915127196387791\n",
      "0.08181034699702922\n",
      "0.06459693834627034\n",
      "0.04754909681392894\n",
      "0.030711434847438435\n",
      "0.01413513583255066\n",
      "-0.002122324726226976\n",
      "-0.01799781200604672\n",
      "-0.03342357675791374\n",
      "-0.048328752989037216\n",
      "-0.06264132015881824\n",
      "-0.07629050023962441\n",
      "-0.08920950728662264\n",
      "-0.1013385069849434\n",
      "-0.11262758340605372\n",
      "-0.12303946100653729\n",
      "-0.13255170547068956\n",
      "-0.14115814086758666\n",
      "-0.14886928147885964\n",
      "-0.15571168315804212\n",
      "-0.16172625620445785\n",
      "-0.16696572208867166\n",
      "-0.17149150744663982\n",
      "-0.17537042415596843\n",
      "-0.17867147367368452\n",
      "-0.1814630462700808\n",
      "-0.18381068502269826\n",
      "-0.18577547853771753\n",
      "-0.18741305798340735\n",
      "-0.18877311500227179\n",
      "-0.18989932856918956\n",
      "-0.1908295849273675\n",
      "-0.19159638666596668\n",
      "-0.19222736664088763\n",
      "-0.19274584376802217\n",
      "-0.19317137714051436\n",
      "-0.19352029080720584\n",
      "-0.1938061535642317\n",
      "-0.1940402066250944\n",
      "-0.19423173770200186\n",
      "-0.19438840354847714\n",
      "-0.19451650499538692\n",
      "-0.19462121945215283\n",
      "-0.19470679611131875\n",
      "-0.19477671895156617\n",
      "-0.19483384226226497\n",
      "-0.19488050293066383\n",
      "-0.19491861321619175\n",
      "-0.19494973722998504\n",
      "-0.19497515386672187\n",
      "-0.1949959085121606\n",
      "-0.19501285547740776\n",
      "-0.19502669278913043\n",
      "-0.195037990690127\n",
      "-0.19504721497233418\n",
      "-0.19505474606921125\n",
      "-0.19506089467154308\n",
      "-0.19506591449528737\n",
      "-0.19507001271789048\n",
      "-0.19507335850689073\n",
      "-0.19507608998817622\n",
      "-0.19507831993854174\n",
      "-0.19508014043545946\n",
      "-0.19508162665473938\n",
      "-0.19508283997194853\n",
      "-0.19508383049507205\n",
      "-0.19508463913260332\n",
      "-0.19508529928221063\n",
      "-0.19508583820951242\n",
      "-0.1950862781738435\n",
      "-0.1950866373473656\n",
      "-0.19508693056548415\n",
      "-0.19508716993949704\n",
      "-0.1950873653567738\n",
      "-0.19508752488910824\n",
      "-0.19508765512609222\n",
      "-0.19508776144727819\n",
      "-0.19508784824437142\n",
      "-0.19508791910262757\n",
      "-0.19508797694893953\n",
      "-0.1950880241727294\n",
      "-0.19508806272464402\n",
      "-0.19508809419712791\n",
      "-0.19508811989020192\n",
      "-0.19508814086515808\n",
      "-0.19508815798840212\n",
      "-0.19508817196723965\n",
      "-0.19508818337908734\n",
      "-0.19508819269533478\n",
      "-0.1950882003008016\n",
      "-0.19508820650964714\n",
      "-0.19508821157833833\n",
      "-0.19508821571624513\n",
      "-0.19508821909429175\n",
      "-0.19508822185201374\n",
      "-0.19508822410332366\n",
      "-0.19508822594121833\n",
      "-0.19508822744161214\n",
      "-0.1950882286664829\n",
      "-0.19508822966642453\n",
      "-0.19508823048274465\n",
      "-0.19508823114915924\n",
      "-0.19508823169319728\n",
      "-0.19508823213733228\n",
      "-0.1950882324999088\n",
      "-0.1950882327959046\n",
      "-0.1950882330375439\n",
      "-0.19508823323481184\n",
      "-0.19508823339585407\n",
      "-0.1950882335273233\n",
      "-0.19508823363465022\n",
      "-0.19508823372226736\n",
      "-0.19508823379379614\n",
      "-0.19508823385219026\n",
      "-0.19508823389985924\n",
      "-0.1950882339387765\n",
      "-0.19508823397054575\n",
      "-0.1950882339964814\n",
      "-0.19508823401765635\n",
      "-0.1950882340349418\n",
      "-0.195088234049052\n",
      "-0.1950882340605713\n",
      "-0.19508823406997633\n",
      "-0.19508823407765363\n",
      "-0.19508823408392084\n",
      "-0.1950882340890377\n",
      "-0.19508823409321446\n",
      "-0.19508823409662485\n",
      "-0.1950882340994094\n",
      "-0.1950882341016813\n",
      "-0.19508823410353682\n",
      "-0.19508823410505094\n",
      "-0.19508823410628706\n",
      "-0.19508823410729736\n",
      "-0.19508823410812087\n",
      "-0.19508823410879272\n",
      "-0.19508823410934206\n",
      "-0.19508823410979126\n",
      "-0.1950882341101563\n",
      "-0.19508823411045628\n",
      "-0.1950882341106998\n",
      "-0.19508823411089926\n",
      "-0.1950882341110618\n",
      "-0.19508823411119414\n",
      "-0.19508823411130272\n",
      "-0.19508823411139115\n",
      "-0.1950882341114638\n",
      "-0.1950882341115226\n",
      "-0.19508823411157056\n",
      "-0.19508823411160942\n",
      "-0.1950882341116419\n",
      "-0.1950882341116677\n",
      "-0.1950882341116898\n",
      "-0.19508823411170645\n",
      "-0.19508823411172116\n",
      "-0.1950882341117326\n",
      "-0.1950882341117422\n",
      "-0.1950882341117503\n",
      "-0.1950882341117558\n",
      "-0.19508823411176096\n",
      "-0.1950882341117654\n",
      "-0.19508823411176934\n",
      "-0.19508823411177145\n",
      "-0.19508823411177362\n",
      "-0.19508823411177528\n",
      "-0.19508823411177684\n",
      "-0.19508823411177784\n",
      "-0.1950882341117789\n",
      "-0.19508823411177983\n",
      "-0.19508823411178056\n",
      "-0.195088234111781\n",
      "-0.19508823411178244\n",
      "-0.19508823411178228\n",
      "-0.19508823411178278\n",
      "-0.19508823411178344\n",
      "-0.1950882341117831\n",
      "-0.19508823411178328\n",
      "-0.1950882341117834\n",
      "-0.19508823411178372\n",
      "-0.19508823411178355\n",
      "-0.19508823411178378\n",
      "-0.19508823411178355\n",
      "-0.1950882341117835\n",
      "-0.1950882341117835\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n",
      "-0.1950882341117836\n",
      "-0.19508823411178372\n"
     ]
    }
   ],
   "source": [
    "X=X_train\n",
    "y=y_train\n",
    "\n",
    "step_size=0.5\n",
    "max_iterations=1000\n",
    "\n",
    "#y = tobinary(y)\n",
    "N, D = X.shape\n",
    "assert len(np.unique(y)) == 2\n",
    "\n",
    "\n",
    "w = np.zeros(D)\n",
    "#w = np.ones(D)\n",
    "\n",
    "    \n",
    "b = 0\n",
    "\n",
    "\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "for i in range(max_iterations):\n",
    "    z = (np.sum(X * w.T,1) +b)\n",
    "    sgm = sigmoid(z) - y \n",
    "    b = b - step_size * np.sum(sgm,0)\n",
    "    w = w - step_size * np.sum(X * np.tile(sgm, (D,1)).T, 0)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = binary_train(X_train, y_train, w0=None, b0=None, step_size=0.5, max_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p =binary_predict(X_test, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.16336951,  -2.43560776,  -9.00648278,  -7.19670806,\n",
       "        -7.34618021,  -6.34837233,  -8.35634149,  -9.07129849,\n",
       "         6.37162733,  -3.94962175,  -8.38959261,   7.27266234,\n",
       "         8.57298293,   6.33071848,  -6.2881268 ,  -6.78585889,\n",
       "        -9.90584665,   6.65743033,  -6.56358337,  -5.76214388,\n",
       "         7.14766421,  -6.33360276,   8.11429372,  -4.04207364,\n",
       "        -6.73138658,  10.47913348,   7.87770012,   8.62491101,\n",
       "        -5.25908636,  -3.37177141,   7.13369534,  -5.66236834,\n",
       "         7.72645908,   6.81120575,  -9.00898401,  -9.21085196,\n",
       "         7.40017326,   5.34387566,  -8.20556839,   6.25309791,\n",
       "         7.23351916,  -8.9771956 ,   6.05891788,   8.30907799,\n",
       "         5.38920126,   9.38889139,  -7.12630952,   7.72269236,\n",
       "         5.07023082,  -5.92488581,   5.7861725 ,   8.84634521,\n",
       "        -6.61700086,  -8.7377394 , -11.36714803,   6.76365358,\n",
       "         9.47589489,  -6.36526793,  -4.12426397,   8.01524905,\n",
       "        -5.62659769,   9.05046899,   9.03548859,   9.60037236,\n",
       "        -7.4355814 ,   9.90979201,   7.53565169,  -5.8739984 ,\n",
       "        -6.17201235,  -5.15326899,  -6.59562104,  -7.74765187,\n",
       "         5.15909755,  -2.79849736,   5.22290814,  -3.64437874,\n",
       "         8.80054768,   7.13151812,  -5.43300306,   5.56213456,\n",
       "        -7.91747689,  -5.08029718,   9.0188693 ,   7.65294991,\n",
       "         7.59989064,  -4.52790773,  -4.64830345,  -7.36310685,\n",
       "         7.80713773,   7.23624922,   9.05618296,   6.63886131,\n",
       "        -6.93064485,  -7.22586057,   6.29374726,  -7.97349333,\n",
       "        -9.70927881,   6.17985504,   7.77206813,  -8.38074796,\n",
       "         6.52345488,   8.98967986,  -7.63477322,   7.57419713,\n",
       "       -11.96902424,   6.94952691,  -5.23262824,  -8.00900158,\n",
       "        -5.3646505 ,   7.67512643,   9.07863832,   6.97725755,\n",
       "       -10.00289406,   6.93566022,  -6.13770216,  -8.53717884,\n",
       "         6.63785185,  -7.26362307,   7.69151574,  -7.36881512,\n",
       "         7.83240517,   6.75752548, -11.6715164 ,  -9.03117003,\n",
       "        -6.05440749,   6.58446091,  -4.26328294,   5.54785315,\n",
       "        -8.14236417,  -6.48597446,  -5.4890915 ,  -5.95192102,\n",
       "         6.72025408,   5.87058858,  -9.71872839,   6.86012512,\n",
       "         7.29523748,   8.32182248, -10.1171463 ,  -7.46713903,\n",
       "         8.69120865,  -7.7110469 , -11.47840431,  -7.74859345,\n",
       "       -11.03247212,  -6.52686181,   7.12648458,  -5.3206285 ,\n",
       "         8.65382399,   6.50104498])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_test * w,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.13593744e-03,  3.74052234e-04, -9.84326236e-04, -8.63961910e-03,\n",
       "       -7.29656427e-04,  8.87155902e-04, -8.41227927e-04, -2.43841983e-03,\n",
       "       -1.83413579e-03, -4.19010682e-04,  1.08993606e-02,  1.37746561e-03,\n",
       "        2.95496794e-03, -8.25005760e-03, -4.56870008e-03, -1.28191414e-02,\n",
       "        1.48589290e-03,  6.37695103e-01, -3.17427763e-03, -6.54805578e-04,\n",
       "       -1.88712378e-04, -6.34583271e-04,  6.77970110e-04, -1.02017099e-05,\n",
       "        9.40935760e-03,  1.16238958e-04, -1.18538090e-04,  4.02529236e-04,\n",
       "       -1.37441180e-03,  9.65670497e-05,  7.44362626e-02,  6.00687953e-03,\n",
       "       -1.74380222e-03,  3.15742203e-03,  1.26618028e-04, -4.18222036e-03,\n",
       "       -8.79482720e-04,  2.64962322e-04, -3.50989919e-03, -4.63758644e-04,\n",
       "       -5.34415194e-04,  9.43026174e-05, -2.20054059e-02, -6.01207747e-03,\n",
       "        3.01756548e-04,  7.63558985e-05,  1.64849052e-04, -7.41761749e-04,\n",
       "        2.73025965e-04,  4.21225742e-04, -2.60462897e-03, -2.48615560e-04,\n",
       "       -3.24301953e-03,  2.38699647e-03,  2.80705007e-04, -3.13035485e-03,\n",
       "        1.94089884e-04,  1.00396309e-03,  2.24956068e-05,  2.32155647e-03,\n",
       "        2.46590564e-04, -1.49366748e-03,  2.24195032e-04, -6.72909484e-03,\n",
       "       -4.51363826e-03, -1.94030152e-03,  3.37496449e-03, -1.75254503e-03,\n",
       "       -1.56804223e-03,  1.20220034e-02, -4.43550253e-04, -9.30165969e-04,\n",
       "       -5.01590674e-03, -9.07512869e-04, -3.32494335e-04,  5.41904601e-03,\n",
       "       -1.84863641e-04, -3.75744262e-03,  9.40593822e-03, -1.46157610e-04,\n",
       "        1.79448678e-03,  6.23110340e-04,  1.03891147e-02,  4.82979114e-04,\n",
       "       -2.59785798e-05,  4.18728488e-04,  1.81778360e-03,  1.20517953e-02,\n",
       "       -4.64548211e-04,  2.03041352e-02, -8.33296948e-03, -4.01012287e-04,\n",
       "        1.72652445e-04, -1.63481858e-04,  1.96103025e-05, -4.66494910e-04,\n",
       "        2.06346206e-04, -2.88715543e-03,  5.84163682e-04,  4.82529489e-04,\n",
       "       -2.61569388e-03, -1.00434534e-04,  3.85985615e-03, -3.42557526e-03,\n",
       "       -2.84209467e-05, -2.01166595e-03,  4.01450779e-04,  1.21964396e-04,\n",
       "        8.04486102e-04, -9.50636486e-03,  2.68398734e-03,  1.43193794e-03,\n",
       "        2.29463616e-05, -5.95549951e-04,  8.36427654e-05, -1.48980302e-03,\n",
       "        1.73040719e-03,  1.38556141e-03,  3.32614159e-03,  8.71719065e-05,\n",
       "       -1.85504260e-02,  5.04496020e-04,  1.00263374e-03,  1.05037641e-02,\n",
       "        3.91177594e-03, -1.62765195e-03, -6.24312378e-05,  2.57991156e-05,\n",
       "        1.24431722e-03, -1.49934366e-03, -2.52786286e-03, -4.60649782e-04,\n",
       "        7.20885615e-04, -3.30690393e-04, -1.24496951e-03,  1.06224489e-04,\n",
       "       -7.38662444e-04, -8.84659470e-04, -7.95889923e-02, -4.25258777e-03,\n",
       "        1.71496361e-04,  1.81600923e-03,  8.02621205e-03,  1.39241013e-03,\n",
       "       -5.28612145e-04,  5.40750585e-02, -7.02134723e-04, -2.01180182e-04,\n",
       "       -1.36993356e-03, -3.15594146e-04, -2.56589020e-04, -2.69159579e-03,\n",
       "        1.10917362e-03, -1.77566028e-03,  1.61207369e-03, -5.18431316e-04,\n",
       "       -8.52277536e-03, -2.05238629e-03, -1.03317546e-03, -8.59351571e-04,\n",
       "       -7.92650918e-04, -1.08975409e-03, -2.20088806e-04,  1.01729320e-03,\n",
       "       -7.48129085e-05, -4.85858637e-04, -1.62050864e-03,  4.52362646e-04,\n",
       "        1.07288067e-04, -7.07416126e-04, -1.05763006e-03, -5.81689847e-03,\n",
       "        4.13897627e-02,  1.45047672e-03,  4.15959519e-04,  8.59475947e-03,\n",
       "       -2.29538763e-04,  2.06108269e-05,  7.44927905e-05,  4.67504317e-04,\n",
       "        3.00427266e-05,  1.26231678e-02, -5.91367158e-03,  3.18725602e-04,\n",
       "       -6.55855663e-04,  2.02926926e-03,  2.97401329e-04, -1.04104421e-03,\n",
       "       -1.95935353e-03,  8.04997960e-03,  9.30041615e-04,  1.53547521e-04,\n",
       "       -2.62182592e-03,  6.48283101e-04, -6.84164260e-04, -4.16457945e-04,\n",
       "       -2.51537260e-03,  5.33201410e-03,  4.62406767e-05, -1.30702051e-03,\n",
       "       -9.03735364e-04,  4.03607886e-05, -1.10345801e-03, -3.69060932e-03,\n",
       "        3.81149820e-04, -9.23277733e-05, -1.40777199e-03,  2.62606252e-05,\n",
       "       -1.10779626e-03,  4.48279140e-04,  3.79444078e-04,  4.03422858e-05,\n",
       "        2.38582331e-04,  8.56118321e-05,  2.08698977e-04, -5.38339858e-03,\n",
       "        1.15990656e-04, -5.70040446e-04, -7.92776569e-04, -1.15848235e-03,\n",
       "        2.93335102e-04,  1.18468728e-01, -1.31229885e-03, -1.15232825e-03,\n",
       "        5.18248065e-04, -3.95292430e-03,  2.81312615e-05,  1.16261463e-04,\n",
       "        3.65578394e-03,  5.05576886e-04, -3.00194844e-04,  1.62438577e-04,\n",
       "       -4.17661975e-05,  1.14621201e-04, -7.25148462e-03, -6.73612960e-03,\n",
       "       -4.14522904e-05,  1.52932047e-05,  2.19227430e-03,  3.50134237e-03,\n",
       "        2.77712717e-04, -2.34331290e-03,  5.61194569e-02,  6.20902850e-03,\n",
       "       -9.53528476e-04,  2.29929881e-03, -5.72329171e-04,  1.23548854e-04,\n",
       "        4.74288408e-05, -5.82603139e-04, -4.52978722e-04,  2.25930326e-03,\n",
       "        1.16803381e-04, -6.62227333e-03,  5.21561733e-03, -2.08876922e-04,\n",
       "       -3.88411323e-03, -3.20192463e-03,  9.12918095e-04,  7.52732747e-04,\n",
       "        5.04063260e-05,  1.61982109e-03, -1.20749026e-03, -3.34080996e-03,\n",
       "       -4.48656918e-04, -9.73802535e-04,  4.37599683e-05,  5.76997333e-02,\n",
       "       -4.86919823e-03, -1.40696438e-03, -1.06431701e-03, -1.70911888e-03,\n",
       "        1.61252254e-03,  4.75303535e-05, -2.34386246e-02, -7.96974291e-04,\n",
       "       -5.95343378e-03,  1.04035929e-01, -1.05173282e-04, -2.52814321e-04,\n",
       "        8.03381916e-05, -1.76189800e-03, -6.61751516e-04,  1.96498343e-02,\n",
       "        9.50421710e-04,  4.39581494e-03, -1.88631043e-03,  1.74791960e-03,\n",
       "       -9.77183201e-04, -2.30047957e-03,  6.58607496e-04, -8.18890417e-03,\n",
       "       -1.12179484e-02, -2.51142742e-03, -3.33024916e-04, -9.14087598e-04,\n",
       "       -1.77770077e-03,  2.35793136e-03,  1.10491123e-04,  5.27584096e-04,\n",
       "        5.24913858e-05, -4.22389491e-03, -7.22498242e-04,  4.39280034e-03,\n",
       "       -2.91924553e-04,  2.44555196e-05, -7.28035144e-05,  4.73861052e-04,\n",
       "       -1.25383746e-03,  1.78094667e-03, -6.36185691e-03,  1.90253044e-02,\n",
       "       -8.50575380e-04,  2.39294556e-04,  4.34158279e-03,  5.75734959e-03,\n",
       "       -3.35245586e-03, -2.57367214e-04,  2.62489337e-04,  3.15855350e-05,\n",
       "        3.70723637e-03,  1.66396481e-03, -4.60368077e-03, -2.05089603e-02,\n",
       "        1.72889812e-02,  9.64913831e-03,  7.26786851e-04,  7.60248197e-04,\n",
       "       -9.87916474e-04,  2.58712196e-02, -2.01660570e-04, -2.54055369e-03,\n",
       "       -2.22447894e-03,  4.18477792e-05, -9.95515429e-01, -7.55235243e-04,\n",
       "        1.79676737e-03, -3.82217626e-04, -1.23947060e-04, -9.97908720e-04,\n",
       "       -3.25451942e-03,  2.14856404e-04,  1.00778704e-04, -7.36697583e-04,\n",
       "       -2.04628232e-03, -4.42755221e-04,  1.09498810e-04, -1.68679840e-02,\n",
       "        2.48303150e-04,  1.06601286e-03])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2,1,0],[1,1,1],[3,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1,-1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "C = 3\n",
    "for c in range(C):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confrim OVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data: 3 class classification\n",
      "One-versus-rest:\n",
      "train acc: 0.871429, test acc: 0.846667\n",
      "Synthetic data: 5 class classification\n",
      "One-versus-rest:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.697143, test acc: 0.666667\n"
     ]
    }
   ],
   "source": [
    "from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                        toy_data_multiclass_5_classes, \\\n",
    "                        data_loader_mnist \n",
    "    \n",
    "datasets = [(toy_data_multiclass_3_classes_non_separable(), \n",
    "                    'Synthetic data', 3), \n",
    "            (toy_data_multiclass_5_classes(), 'Synthetic data', 5)]#, \n",
    "            #(data_loader_mnist(), 'MNIST', 10)]\n",
    "\n",
    "for data, name, num_classes in datasets:\n",
    "    print('%s: %d class classification' % (name, num_classes))\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "    print('One-versus-rest:')\n",
    "    w, b = OVR_train(X_train, y_train, C=num_classes)\n",
    "    train_preds = OVR_predict(X_train, w=w, b=b)\n",
    "    preds = OVR_predict(X_test, w=w, b=b)\n",
    "    print('train acc: %f, test acc: %f' % \n",
    "          (accuracy_score(y_train, train_preds),\n",
    "         accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -36.0448073 ,  179.81691404],\n",
       "       [  97.46514605,  -35.74131472],\n",
       "       [ -19.25540391,  -70.9456114 ],\n",
       "       [-107.57775957,   17.83314932],\n",
       "       [ 158.69822292,   15.87422578]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = np.dot(X_test, w.T) +  np.tile(b,(X_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1667.90678351,  -207.55495101,    89.02624715,  -905.48424371,\n",
       "         -111.56142864],\n",
       "       [-1232.81846316,   328.71558557,  -313.50386617, -1579.58280689,\n",
       "         1043.24118639],\n",
       "       [-1358.27079172,   -67.88168684,  -107.73505264, -1106.52761372,\n",
       "          276.49282957],\n",
       "       [ -758.63883269,    -2.36568691,  -412.78944643, -1259.78854615,\n",
       "          660.5396405 ],\n",
       "       [-1322.11613411,   269.67705799,  -249.80495537, -1499.99913883,\n",
       "          897.70207295],\n",
       "       [  395.66096259, -1252.60414813,  -489.77430929,    30.39016806,\n",
       "        -1067.53380166],\n",
       "       [-1719.00060712,  -394.90505141,   182.40514927,  -683.07584862,\n",
       "         -470.13662363],\n",
       "       [-1531.63908901,  -378.56923612,    88.62067255,  -726.2010141 ,\n",
       "         -357.55042235],\n",
       "       [ -857.45787604,  -192.81779731,  -295.91414619, -1027.61499527,\n",
       "          275.18480448],\n",
       "       [ -486.22290055, -1112.05478262,  -128.95435026,   -17.06016775,\n",
       "        -1207.66112486],\n",
       "       [-2133.02607273,  -607.7832756 ,   455.18364263,  -384.17455212,\n",
       "        -1035.83572158],\n",
       "       [  356.67230431, -1611.54639929,  -338.45005809,   448.85768023,\n",
       "        -1728.33683152],\n",
       "       [   23.66320689, -1346.78631922,  -280.67797145,   187.13112535,\n",
       "        -1401.76325442],\n",
       "       [-1468.58015675,  -497.87922589,   103.32564824,  -596.9687799 ,\n",
       "         -543.39892471],\n",
       "       [  -54.90262902,  -476.75404171,  -566.43303826,  -804.72700669,\n",
       "          122.99567425],\n",
       "       [ -822.98587201,   177.10577137,  -449.19460153, -1458.14410072,\n",
       "          953.66570891],\n",
       "       [ -580.94228879,  -307.44783436,  -382.8910787 ,  -931.46928293,\n",
       "          192.62922267],\n",
       "       [-1275.24681391,   327.05035605,  -293.02027342, -1572.15973001,\n",
       "         1021.39223172],\n",
       "       [ -743.08924322,  -660.51762677,  -176.07740759,  -503.78529027,\n",
       "         -512.40279682],\n",
       "       [  421.36731458,  -692.45364779,  -709.47213972,  -618.09381189,\n",
       "          -51.93410902],\n",
       "       [  445.24524468, -1165.32650261,  -545.34712   ,   -76.56477029,\n",
       "         -889.02763984],\n",
       "       [  128.57052239, -1384.41020056,  -315.850704  ,   216.85217435,\n",
       "        -1422.56882801],\n",
       "       [  488.18384136, -1075.73020857,  -598.66779652,  -185.32790122,\n",
       "         -709.31956847],\n",
       "       [-1244.51638237,   307.21224914,  -300.05473593, -1553.29869544,\n",
       "          999.49166667],\n",
       "       [ -704.34366612,    19.91242821,  -446.47114035, -1292.49208006,\n",
       "          724.61691328],\n",
       "       [  191.89312479,  -974.33867124,  -497.5237116 ,  -263.66042272,\n",
       "         -659.28844298],\n",
       "       [  566.24536271, -1779.20123576,  -374.42487566,   614.76017662,\n",
       "        -1935.71169498],\n",
       "       [-1871.49383389,  -570.66085961,   318.9640699 ,  -460.86430535,\n",
       "         -853.00934378],\n",
       "       [-1075.04708543,   -39.00148186,  -251.05564724, -1176.53898233,\n",
       "          454.18763112],\n",
       "       [ -363.41561692,  -424.97949163,  -441.1718955 ,  -824.32772506,\n",
       "           78.64537259],\n",
       "       [ -196.4905932 ,  -508.48696487,  -488.3730599 ,  -749.80756429,\n",
       "            3.15840088],\n",
       "       [-1430.85347651,   247.88786254,  -190.8130234 , -1460.7947331 ,\n",
       "          810.29617458],\n",
       "       [ -787.26894729,  -200.1636574 ,  -326.05546218, -1028.26161741,\n",
       "          293.22216853],\n",
       "       [  191.98901324, -1058.8610737 ,  -466.23411192,  -166.32491309,\n",
       "         -810.76743081],\n",
       "       [-1881.30253082,  -636.74340967,   348.0552005 ,  -383.48159769,\n",
       "         -975.83517592],\n",
       "       [ -853.4042587 ,    30.23115874,  -380.5018132 , -1285.03568033,\n",
       "          676.84245246],\n",
       "       [-1462.79462064,  -570.31088161,   127.46884875,  -514.29692713,\n",
       "         -670.67344383],\n",
       "       [ -109.69039197, -1321.09383133,  -227.76256277,   174.84291829,\n",
       "        -1414.99412661],\n",
       "       [  378.21835864, -1128.71107384,  -527.53730574,  -110.03940393,\n",
       "         -853.18818303],\n",
       "       [ -204.22707291,  -814.64977127,  -371.24865077,  -396.18335695,\n",
       "         -549.13319204],\n",
       "       [ -759.07661939,  -494.36796753,  -230.18732364,  -693.07233988,\n",
       "         -221.65764643],\n",
       "       [ -246.48634483,  -180.34470972,  -586.61378491, -1121.25554092,\n",
       "          569.18424442],\n",
       "       [ -837.68761401,  -609.01244192,  -150.87777311,  -550.83161754,\n",
       "         -462.12911032],\n",
       "       [  137.05185426, -1067.1647922 ,  -437.43246412,  -149.6329707 ,\n",
       "         -850.07851347],\n",
       "       [  610.22667485, -1667.27536967,  -436.51189554,   480.14385838,\n",
       "        -1715.51025308],\n",
       "       [ -699.48511263,  -504.34311925,  -254.39186046,  -689.31566132,\n",
       "         -213.04539628],\n",
       "       [  314.93206743, -1502.41408578,  -359.3639886 ,   328.58135405,\n",
       "        -1551.25531047],\n",
       "       [  208.51230577, -1086.9174141 ,  -463.56965947,  -136.15519405,\n",
       "         -853.71718153],\n",
       "       [ -633.11699092,  -587.7582608 ,  -254.54343638,  -601.85438431,\n",
       "         -333.07464993],\n",
       "       [ -763.37135305,   -16.43191621,  -405.35884441, -1242.97383337,\n",
       "          633.21930445],\n",
       "       [  -69.93090821,  -239.32438507,  -647.41725081, -1076.23462607,\n",
       "          541.94946649],\n",
       "       [ -242.08992027,  -698.86724421,  -396.44348859,  -524.6221085 ,\n",
       "         -358.40607093],\n",
       "       [ -336.79697803,  -565.49459768,  -401.54321041,  -665.94447952,\n",
       "         -161.41854608],\n",
       "       [ -792.41795449,  -335.08315646,  -273.62662002,  -872.20114879,\n",
       "           49.06542252],\n",
       "       [-1305.17142794,   499.26889297,  -342.85422794, -1766.62817298,\n",
       "         1316.8204134 ],\n",
       "       [-1969.71119664,  -514.33299585,   344.07034351,  -512.99554022,\n",
       "         -795.69914847],\n",
       "       [-1235.66749298,  -900.88437066,   143.67256132,  -163.03194686,\n",
       "        -1162.30474173],\n",
       "       [-1903.39296384,  -449.37248197,   288.93568523,  -596.41817726,\n",
       "         -649.76051358],\n",
       "       [ -350.21853039,   140.43119057,  -656.96258168, -1477.24697217,\n",
       "         1098.11324317],\n",
       "       [  -91.22443713, -1075.58201564,  -327.42605216,  -110.31920201,\n",
       "         -966.66000496],\n",
       "       [ -242.14018206, -1325.81954142,  -163.99358378,   197.47133861,\n",
       "        -1482.35325859],\n",
       "       [-1212.873256  ,  -105.27644863,  -161.95148525, -1082.32413798,\n",
       "          274.10002827],\n",
       "       [ -858.39442961,  -338.95117349,  -241.3004484 ,  -859.18561422,\n",
       "           12.79803076],\n",
       "       [  144.7283415 , -1073.39201646,  -438.71824775,  -143.45685814,\n",
       "         -857.82895199],\n",
       "       [-1230.52440481,   289.63406847,  -300.08955145, -1534.86866376,\n",
       "          974.20047552],\n",
       "       [  125.13209712, -1002.47436967,  -455.83357241,  -222.59299719,\n",
       "         -739.40883539],\n",
       "       [ -399.60352184,  -752.42345117,  -302.83622076,  -442.50150315,\n",
       "         -524.44607937],\n",
       "       [ -937.37667776,  -704.26451078,   -68.88805919,  -428.19101028,\n",
       "         -677.20754916],\n",
       "       [-1190.93247101,  -668.51280294,    36.58049853,  -436.46836321,\n",
       "         -725.84759907],\n",
       "       [-1906.68776454,  -364.96593749,   259.18686549,  -693.20518322,\n",
       "         -499.91146493],\n",
       "       [-1364.45586838,    98.6163559 ,  -166.5638594 , -1297.48777893,\n",
       "          572.22064237],\n",
       "       [  581.21616598, -1224.58512784,  -587.04426067,   -25.95665553,\n",
       "         -934.80644371],\n",
       "       [ -891.5037007 ,    49.19772849,  -369.69385465, -1301.93678565,\n",
       "          693.90435323],\n",
       "       [ -899.53754645,  -202.08779713,  -272.77454408, -1011.47843145,\n",
       "          239.8579542 ],\n",
       "       [ -704.63733035,    92.21758176,  -473.13890589, -1375.73081636,\n",
       "          854.10635189],\n",
       "       [ -793.26977722,   184.29028593,  -465.77205948, -1470.27452623,\n",
       "          979.75708063],\n",
       "       [  126.53093169, -1513.3228113 ,  -267.10468447,   365.59080793,\n",
       "        -1654.57469258],\n",
       "       [-1155.34262249,    87.08516164,  -260.20218654, -1311.33968545,\n",
       "          644.5210755 ],\n",
       "       [ -258.03905815,  -830.42377154,  -340.20442973,  -371.03357521,\n",
       "         -601.33584485],\n",
       "       [  -73.90703875, -1509.44597381,  -174.69073165,   387.13291987,\n",
       "        -1736.73989288],\n",
       "       [-1222.66419884,  -825.15223796,   109.50828905,  -251.94298383,\n",
       "        -1020.75997142],\n",
       "       [  597.00558944, -1577.30705729,  -463.67480689,   378.23908922,\n",
       "        -1560.10398073],\n",
       "       [ -622.19026274, -1404.52989219,    43.13732122,   337.4375649 ,\n",
       "        -1792.42676863],\n",
       "       [ -248.08298321,  -376.7224963 ,  -513.06419781,  -894.87194424,\n",
       "          216.43184625],\n",
       "       [  311.45570256,  -790.4922024 ,  -621.66289832,  -490.91749758,\n",
       "         -276.55279086],\n",
       "       [-1882.8986168 ,  -472.74795031,   288.00545605,  -572.15486467,\n",
       "         -682.55341969],\n",
       "       [ -465.98439352, -1217.42076543,   -99.36892181,   101.66821169,\n",
       "        -1387.55053304],\n",
       "       [ -233.1198521 , -1132.19544927,  -239.99830931,   -26.70395294,\n",
       "        -1131.23676495],\n",
       "       [  915.12540171, -1698.76979134,  -567.59906451,   476.85607053,\n",
       "        -1636.4110787 ],\n",
       "       [ -478.57010362, -1298.85076643,   -63.28783766,   197.08753718,\n",
       "        -1539.12409913],\n",
       "       [ -462.19534799,  -785.46820366,  -261.27832428,  -396.32107897,\n",
       "         -611.51319931],\n",
       "       [ -435.746738  , -1051.24051191,  -175.13415399,   -93.65186463,\n",
       "        -1076.1987938 ],\n",
       "       [  -45.58388252, -1449.19472167,  -210.28909545,   314.06407152,\n",
       "        -1616.13614561],\n",
       "       [ -819.33423107,   284.6702923 ,  -490.7811794 , -1582.50442189,\n",
       "         1148.11800077],\n",
       "       [ -832.47906103,   321.37014832,  -498.23189155, -1623.06759809,\n",
       "         1208.06488036],\n",
       "       [  545.14486252, -1379.10091009,  -512.87183749,   156.68589359,\n",
       "        -1227.84117036],\n",
       "       [ -539.41953432,  -455.66982406,  -347.38382847,  -766.14353904,\n",
       "          -54.62427203],\n",
       "       [-1806.9673303 ,  -641.73385231,   315.09926303,  -387.37903868,\n",
       "         -951.73186127],\n",
       "       [-1194.36064345,   371.13648746,  -347.23744768, -1633.43069521,\n",
       "         1136.38671346],\n",
       "       [ -124.85501141,  -726.2303159 ,  -441.19230767,  -508.31838874,\n",
       "         -355.33642325],\n",
       "       [-1610.32292922,  -594.11931518,   205.37247828,  -467.73367605,\n",
       "         -778.94573244],\n",
       "       [ -777.40935848,  -360.82446348,  -271.11117915,  -844.50121468,\n",
       "            9.59234869],\n",
       "       [   10.75091322,  -411.69215596,  -621.29404908,  -888.18015299,\n",
       "          268.82051368],\n",
       "       [ -646.04418028,    17.98981818,  -473.05598529, -1297.84220497,\n",
       "          747.0903587 ],\n",
       "       [  173.6842166 ,  -838.7236801 ,  -539.27349886,  -417.49118229,\n",
       "         -424.26987893],\n",
       "       [ -303.54043914,  -424.14292994,  -469.51741927,  -833.06015148,\n",
       "          106.76568581],\n",
       "       [ -237.82068521,  -731.02728967,  -386.51996887,  -488.13599402,\n",
       "         -414.16063048],\n",
       "       [  536.75321367,  -950.62562618,  -667.78875522,  -335.71800277,\n",
       "         -463.4530446 ],\n",
       "       [ -398.71269785,  -978.43022167,  -219.46715915,  -182.31573182,\n",
       "         -929.20783347],\n",
       "       [-1711.41500789,  -492.76762957,   215.13336279,  -571.34771106,\n",
       "         -642.20033693],\n",
       "       [  388.00485927, -1317.56316278,  -462.10761062,   106.19955014,\n",
       "        -1187.38844684],\n",
       "       [-1522.43467757,  -590.3548041 ,   162.82491465,  -483.47309522,\n",
       "         -733.12187331],\n",
       "       [-1601.18149658,  -560.01147295,   188.44757829,  -508.20320961,\n",
       "         -713.73698645],\n",
       "       [-1639.56633059,  -855.97601025,   316.14173973,  -162.34808609,\n",
       "        -1261.37266702],\n",
       "       [  159.26663378,  -946.45148042,  -492.58544601,  -291.54590308,\n",
       "         -623.80143812],\n",
       "       [ -767.25433676,   310.67306541,  -524.80643581, -1619.21036268,\n",
       "         1217.88747888],\n",
       "       [  882.90645044, -1235.44883211,  -724.27744434,   -52.58939321,\n",
       "         -820.149475  ],\n",
       "       [   71.23313369,  -427.93044647,  -643.59374838,  -877.32552998,\n",
       "          266.60097335],\n",
       "       [-1609.17390862,  -293.92088299,    93.54365673,  -813.63373066,\n",
       "         -240.27515155],\n",
       "       [ -502.24566265,  -513.50346569,  -343.34944366,  -704.35752278,\n",
       "         -141.77390549],\n",
       "       [  213.72376709, -1542.91352135,  -296.96104622,   388.35820998,\n",
       "        -1668.8552546 ],\n",
       "       [   43.54160788,  -547.34313303,  -586.35855953,  -736.1999628 ,\n",
       "           40.2205303 ],\n",
       "       [ -374.68904337,  -608.55530629,  -367.8373406 ,  -611.43310707,\n",
       "         -255.45950629],\n",
       "       [-1058.70900957,  -587.79089601,   -55.25613355,  -546.59538418,\n",
       "         -522.35223147],\n",
       "       [  328.39583698,  -864.91883434,  -602.0030361 ,  -407.39527945,\n",
       "         -402.44430684],\n",
       "       [ -389.91846223,  -222.79394814,  -503.71752049, -1053.75439187,\n",
       "          429.3161328 ],\n",
       "       [ -593.05543746, -1195.44036295,   -48.01903778,    92.8401931 ,\n",
       "        -1404.64274977],\n",
       "       [-1402.09571784,    26.25993101,  -122.11545465, -1209.26804229,\n",
       "          425.77397333],\n",
       "       [-1321.08086244,   329.96648948,  -272.64045075, -1569.57130249,\n",
       "         1006.2420146 ],\n",
       "       [-1754.56679885,  -913.25021442,   391.22140929,   -81.46147316,\n",
       "        -1415.17642915],\n",
       "       [-1021.0291145 ,  -957.99845887,    64.34587864,  -125.10104361,\n",
       "        -1169.26336551],\n",
       "       [-1929.83751911,  -610.31422295,   360.9828286 ,  -407.62367511,\n",
       "         -950.03485609],\n",
       "       [-1353.52937999,  -226.03726913,   -51.32302952,  -924.9884873 ,\n",
       "           -4.92147764],\n",
       "       [-1620.60141284,   591.51324856,  -229.35747066, -1831.94214824,\n",
       "         1341.94421156],\n",
       "       [ -132.98446619, -1170.31367563,  -272.75341278,     4.20562107,\n",
       "        -1155.05015059],\n",
       "       [ -491.8478077 , -1387.3973606 ,   -24.24444764,   300.79312537,\n",
       "        -1703.76310139],\n",
       "       [ -956.614291  ,   195.46499957,  -393.43189704, -1461.95063044,\n",
       "          927.1664632 ],\n",
       "       [  248.32019475, -1293.67052951,  -405.56055369,    96.80577858,\n",
       "        -1206.66068708],\n",
       "       [-1707.79256695,  -650.26840539,   271.82656968,  -390.41756368,\n",
       "         -922.93828236],\n",
       "       [  416.15404955, -1005.7538994 ,  -590.88315344,  -256.57650544,\n",
       "         -615.89903081],\n",
       "       [ -574.67111675,    38.0913548 ,  -513.92716281, -1330.25477446,\n",
       "          814.85859049],\n",
       "       [ -546.89822281,    97.11657302,  -548.81335656, -1401.84015113,\n",
       "          933.01980353],\n",
       "       [-1359.48857693,   275.88282337,  -234.60666145, -1502.29742686,\n",
       "          892.2111603 ],\n",
       "       [-1171.5949622 ,  -705.99976409,    41.42340787,  -395.80215787,\n",
       "         -784.45222866],\n",
       "       [-1275.88907212,  -622.36691564,    59.25238261,  -478.59320437,\n",
       "         -680.89466767],\n",
       "       [-1559.81164219,   605.57125112,  -263.03274249, -1856.0209259 ,\n",
       "         1394.17296717],\n",
       "       [ -188.83751535,  -101.1391178 ,  -642.97015805, -1219.95995193,\n",
       "          736.8053114 ],\n",
       "       [  244.51104035, -1614.73033028,  -284.75236254,   467.07789299,\n",
       "        -1783.91172698],\n",
       "       [-1036.28444172,  -168.50303929,  -221.19614544, -1032.4162057 ,\n",
       "          239.26673   ],\n",
       "       [ -158.57713   ,  -868.65800715,  -372.60119223,  -339.90300605,\n",
       "         -625.65661863]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 4, 4, 0, 2, 2, 4, 3, 2, 3, 3, 2, 4, 4, 4, 4, 2, 0, 0, 3,\n",
       "       0, 4, 4, 0, 3, 2, 4, 4, 4, 4, 4, 0, 2, 4, 2, 3, 0, 0, 4, 4, 2, 0,\n",
       "       0, 4, 3, 0, 2, 4, 4, 0, 4, 4, 4, 2, 2, 2, 4, 0, 3, 4, 4, 0, 4, 0,\n",
       "       2, 2, 2, 2, 4, 0, 4, 4, 4, 4, 3, 4, 0, 3, 2, 0, 3, 4, 0, 2, 3, 3,\n",
       "       0, 3, 2, 3, 3, 4, 4, 0, 4, 2, 4, 0, 2, 4, 4, 4, 0, 4, 0, 0, 3, 2,\n",
       "       0, 2, 2, 2, 0, 4, 0, 4, 2, 4, 3, 0, 4, 2, 0, 4, 3, 4, 4, 2, 2, 2,\n",
       "       4, 4, 3, 3, 4, 0, 2, 0, 4, 4, 4, 2, 2, 4, 4, 3, 4, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.argmax(yc, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # avoid overflow\n",
    "\n",
    "    \n",
    "    if len(x.shape) ==1:\n",
    "        c = np.max(x,0)\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,0)\n",
    "        y = exp_x / sum_exp_x\n",
    "    else:\n",
    "        c = np.transpose(np.tile(np.max(x,1),(x.shape[1],1)))\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,1)\n",
    "        y = np.transpose(np.divide(exp_x.T,sum_exp_x))\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00242826, 0.01794253, 0.97962921])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2,4,8])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00242826, 0.01794253, 0.97962921],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[2,4,8],[1,1,1]])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [0.5, 0.5, 0.5]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[2,4,8],[1,1,1]])\n",
    "c = np.array([2,2])\n",
    "np.transpose(np.divide(b.T,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "        x = a\n",
    "        c = np.transpose(np.tile(np.max(x,1),(x.shape[1],1)))\n",
    "        exp_x = np.exp(x - c)\n",
    "        sum_exp_x = np.sum(exp_x,1)\n",
    "        y = np.transpose(np.divide(exp_x.T,sum_exp_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00242826, 0.01794253, 0.97962921],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-983042676ec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (2,) "
     ]
    }
   ],
   "source": [
    "np.subtract(a,np.max(x,1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.55144471, -1.55144471, -0.55144471])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underlog(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = a\n",
    "x = x - np.max(x)\n",
    "y = x - np.log(np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.55144471, -1.55144471, -0.55144471])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.55144471, -1.55144471, -0.55144471])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(softmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lapubu2941/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from data_loader import toy_data_multiclass_3_classes_non_separable, \\\n",
    "                        toy_data_multiclass_5_classes, \\\n",
    "                        data_loader_mnist \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = toy_data_multiclass_5_classes()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, 0, 4, 3, 0, 3, 3, 1, 4, 4, 3, 1, 2, 0, 4, 4, 4, 2, 2, 0,\n",
       "       2, 2, 3, 4, 4, 0, 0, 0, 4, 3, 0, 2, 0, 4, 4, 0, 0, 0, 4, 0, 0, 0,\n",
       "       2, 2, 2, 2, 1, 3, 3, 1, 3, 2, 3, 2, 0, 0, 3, 4, 4, 0, 3, 1, 1, 0,\n",
       "       1, 4, 3, 0, 0, 1, 3, 2, 2, 3, 1, 3, 2, 1, 2, 1, 3, 4, 4, 2, 4, 4,\n",
       "       1, 3, 3, 2, 2, 3, 0, 4, 1, 1, 3, 0, 4, 4, 1, 0, 2, 3, 2, 4, 4, 1,\n",
       "       1, 4, 4, 4, 4, 3, 3, 4, 2, 1, 3, 4, 0, 4, 2, 2, 1, 2, 3, 2, 2, 0,\n",
       "       3, 1, 3, 0, 1, 3, 0, 0, 3, 2, 3, 4, 1, 4, 1, 2, 3, 2, 4, 0, 2, 2,\n",
       "       0, 0, 3, 1, 1, 2, 1, 1, 0, 1, 3, 2, 0, 4, 3, 4, 3, 3, 1, 1, 2, 1,\n",
       "       0, 1, 0, 2, 4, 3, 2, 0, 4, 0, 2, 2, 3, 4, 0, 0, 0, 3, 4, 2, 4, 0,\n",
       "       4, 3, 0, 1, 2, 0, 1, 3, 3, 3, 4, 1, 1, 1, 2, 4, 4, 3, 4, 1, 0, 0,\n",
       "       3, 4, 3, 0, 4, 2, 1, 0, 3, 2, 4, 0, 2, 1, 4, 3, 3, 0, 0, 1, 2, 2,\n",
       "       1, 3, 0, 0, 0, 2, 1, 1, 1, 1, 3, 0, 0, 2, 1, 2, 0, 4, 0, 4, 0, 1,\n",
       "       3, 4, 4, 1, 3, 3, 3, 2, 2, 3, 2, 1, 0, 2, 0, 3, 4, 4, 2, 2, 0, 0,\n",
       "       0, 2, 4, 4, 2, 2, 3, 1, 1, 1, 4, 2, 3, 1, 4, 0, 4, 0, 0, 3, 2, 4,\n",
       "       3, 3, 4, 1, 2, 1, 3, 4, 0, 4, 2, 0, 3, 1, 3, 2, 3, 2, 1, 1, 4, 4,\n",
       "       4, 1, 1, 3, 2, 0, 1, 0, 0, 4, 3, 3, 1, 4, 3, 0, 2, 4, 2, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-06a747cd4463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# x * np.tile(dfn, (D,1)).T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0msgm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "    X = X_train\n",
    "    y = y_train\n",
    "    C = 5\n",
    "    w0=None, \n",
    "    b0=None, \n",
    "    step_size=0.5, \n",
    "    max_iterations=1000\n",
    "\"\"\"\n",
    "    Inputs:\n",
    "    - X: training features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - y: multiclass training labels, a N dimensional numpy array where\n",
    "    N is the number of training points, indicating the labels of \n",
    "    training data\n",
    "    - C: number of classes in the data\n",
    "    - step_size: step size (learning rate)\n",
    "    - max_iterations: number of iterations to perform gradient descent\n",
    "\n",
    "    Returns:\n",
    "    - w: C-by-D weight matrix of multinomial logistic regression, where \n",
    "    C is the number of classes and D is the dimensionality of features.\n",
    "    - b: bias vector of length C, where C is the number of classes\n",
    "\n",
    "    Implement multinomial logistic regression for multiclass \n",
    "    classification. Again use the *average* of the gradients for all training \n",
    "    examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    You may find it useful to use a special (one-hot) representation of the labels, \n",
    "    where each label y_i is represented as a row of zeros with a single 1 in\n",
    "    the column, that corresponds to the class y_i.\n",
    "\"\"\"\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    w = np.zeros((C, D))\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = np.zeros(C)\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: add your code here\n",
    "    \"\"\"\n",
    "    ## w = w - step * sum(σ(wxn+b)-yn)xn\n",
    "    ## b = b - step * sum(σ(wxn+b)-yn)\n",
    "    ## wtx + b = np.sum(X_train * w.T,1) +b\n",
    "    # x * np.tile(dfn, (D,1)).T\n",
    "    \n",
    "    for c in range(C):\n",
    "        yc[:,c] = (y==c)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        \n",
    "        b = b - step_size * np.sum(sgm,0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, 0, 4, 3, 0, 3, 3, 1, 4, 4, 3, 1, 2, 0, 4, 4, 4, 2, 2, 0,\n",
       "       2, 2, 3, 4, 4, 0, 0, 0, 4, 3, 0, 2, 0, 4, 4, 0, 0, 0, 4, 0, 0, 0,\n",
       "       2, 2, 2, 2, 1, 3, 3, 1, 3, 2, 3, 2, 0, 0, 3, 4, 4, 0, 3, 1, 1, 0,\n",
       "       1, 4, 3, 0, 0, 1, 3, 2, 2, 3, 1, 3, 2, 1, 2, 1, 3, 4, 4, 2, 4, 4,\n",
       "       1, 3, 3, 2, 2, 3, 0, 4, 1, 1, 3, 0, 4, 4, 1, 0, 2, 3, 2, 4, 4, 1,\n",
       "       1, 4, 4, 4, 4, 3, 3, 4, 2, 1, 3, 4, 0, 4, 2, 2, 1, 2, 3, 2, 2, 0,\n",
       "       3, 1, 3, 0, 1, 3, 0, 0, 3, 2, 3, 4, 1, 4, 1, 2, 3, 2, 4, 0, 2, 2,\n",
       "       0, 0, 3, 1, 1, 2, 1, 1, 0, 1, 3, 2, 0, 4, 3, 4, 3, 3, 1, 1, 2, 1,\n",
       "       0, 1, 0, 2, 4, 3, 2, 0, 4, 0, 2, 2, 3, 4, 0, 0, 0, 3, 4, 2, 4, 0,\n",
       "       4, 3, 0, 1, 2, 0, 1, 3, 3, 3, 4, 1, 1, 1, 2, 4, 4, 3, 4, 1, 0, 0,\n",
       "       3, 4, 3, 0, 4, 2, 1, 0, 3, 2, 4, 0, 2, 1, 4, 3, 3, 0, 0, 1, 2, 2,\n",
       "       1, 3, 0, 0, 0, 2, 1, 1, 1, 1, 3, 0, 0, 2, 1, 2, 0, 4, 0, 4, 0, 1,\n",
       "       3, 4, 4, 1, 3, 3, 3, 2, 2, 3, 2, 1, 0, 2, 0, 3, 4, 4, 2, 2, 0, 0,\n",
       "       0, 2, 4, 4, 2, 2, 3, 1, 1, 1, 4, 2, 3, 1, 4, 0, 4, 0, 0, 3, 2, 4,\n",
       "       3, 3, 4, 1, 2, 1, 3, 4, 0, 4, 2, 0, 3, 1, 3, 2, 3, 2, 1, 1, 4, 4,\n",
       "       4, 1, 1, 3, 2, 0, 1, 0, 0, 4, 3, 3, 1, 4, 3, 0, 2, 4, 2, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc = np.zeros((N,C)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(C):\n",
    "    yc[:,c] = (y==c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # avoid overflow\n",
    "    c = np.max(x,0)\n",
    "    exp_x = np.exp(x - c)\n",
    "    sum_exp_x = np.sum(exp_x,0)\n",
    "    y = exp_x / sum_exp_x\n",
    "\n",
    "    return y "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###### Q1.1 ######\n",
    "def objective_function(X, y, w, lamb):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A 2 dimensional numpy array of data (number of samples x number of features)\n",
    "    - ytrain: A 1 dimensional numpy array of labels (length = number of samples )\n",
    "    - w: a numpy array of D elements as a D-dimension weight vector\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "\n",
    "    Return:\n",
    "    - obj_value: the value of objective function in SVM primal formulation\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    \n",
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx))  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    zmax = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2) + np.sum(zmax) / N\n",
    "    #print(obj_value)\n",
    "\n",
    "    return obj_value\n",
    "\n",
    "\n",
    "###### Q1.2 ######\n",
    "def pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t]\n",
    "        \n",
    "        # 4\n",
    "        #A_tpls = A_t[np.multiply(y_t, np.dot(X_t,w)) < 1]\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls]\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1 - (ita_t * lamb)) * w + ita_t * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),\n",
    "            axis=0).reshape(D,1)/ k\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w = w_thalf * min(1, 1 / np.sqrt(lamb) / np.linalg.norm(w_thalf))\n",
    "        \n",
    "        train_obj.append(objective_function(Xtrain, ytrain, w, lamb))\n",
    "        print(train_obj[iter-1])\n",
    "        # print(w[0])\n",
    "    \n",
    "\n",
    "    return w, train_obj\n",
    "\n",
    "\n",
    "###### Q1.3 ######\n",
    "def pegasos_test(Xtest, ytest, w_l):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest, w_l))) > 0).ravel()\n",
    "    ytru = ytest > 0\n",
    "    test_acc = sum(ytru == ywx)/N\n",
    "\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NO MODIFICATIONS below this line.\n",
    "You should only write your code in the above functions.\n",
    "\"\"\"\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "\n",
    "    with open(dataset, 'r') as f:\n",
    "            data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = train_set[0]\n",
    "    ytrain = train_set[1]\n",
    "    Xvalid = valid_set[0]\n",
    "    yvalid = valid_set[1]\n",
    "    Xtest = test_set[0]\n",
    "    ytest = test_set[1]\n",
    "\n",
    "    ## below we add 'one' to the feature of each sample, such that we include the bias term into parameter w\n",
    "    Xtrain = np.hstack((np.ones((len(Xtrain), 1)), np.array(Xtrain))).tolist()\n",
    "    Xvalid = np.hstack((np.ones((len(Xvalid), 1)), np.array(Xvalid))).tolist()\n",
    "    Xtest = np.hstack((np.ones((len(Xtest), 1)), np.array(Xtest))).tolist()\n",
    "\n",
    "    for i, v in enumerate(ytrain):\n",
    "        if v < 5:\n",
    "            ytrain[i] = -1.\n",
    "        else:\n",
    "            ytrain[i] = 1.\n",
    "    for i, v in enumerate(ytest):\n",
    "        if v < 5:\n",
    "            ytest[i] = -1.\n",
    "        else:\n",
    "            ytest[i] = 1.\n",
    "\n",
    "    return Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest\n",
    "\n",
    "\n",
    "def pegasos_mnist():\n",
    "\n",
    "    test_acc = {}\n",
    "    train_obj = {}\n",
    "\n",
    "    Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "\n",
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    for lamb in (0.01, 0.1, 1):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    lamb = 0.1\n",
    "    for k in (1, 10, 1000):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    return test_acc, train_obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_acc, train_obj = pegasos_mnist() # results on mnist\n",
    "    print('mnist test acc \\n')\n",
    "    for key, value in test_acc.items():\n",
    "        print('%s: test acc = %.4f \\n' % (key, value))\n",
    "\n",
    "    #with open('pegasos.json', 'w') as f_json:\n",
    "    #    json.dump([test_acc, train_obj], f_json)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "X = np.array(Xtrain)\n",
    "y = np.array(ytrain)\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "w = np.ones((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx)) \n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    D = X.shape[1]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    zmax = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2 ) + np.sum(zmax) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "max_iterations = 500\n",
    "w = np.zeros((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92710240317173\n",
      "2.799573665940772\n",
      "1.1711735104241636\n",
      "0.9889135417648544\n",
      "0.9507995012292567\n",
      "0.8135716991462694\n",
      "0.806213791613021\n",
      "0.7972111617010312\n",
      "0.777981848338262\n",
      "0.7784166338498772\n",
      "0.7660148351239452\n",
      "0.7599369389699608\n",
      "0.7423080670210762\n",
      "0.722839954521328\n",
      "0.7053444499392916\n",
      "0.683891753595169\n",
      "0.6746281030101371\n",
      "0.6597227635703051\n",
      "0.6587316366586422\n",
      "0.6514266087965418\n",
      "0.6499520610937997\n",
      "0.6437100845025393\n",
      "0.6384841266150869\n",
      "0.6387445229591764\n",
      "0.6353801553762549\n",
      "0.6345829322932705\n",
      "0.6360677096197667\n",
      "0.6357822021946509\n",
      "0.6346700106076135\n",
      "0.6358877974737873\n",
      "0.6370034297516821\n",
      "0.6377325956981745\n",
      "0.6393059567515549\n",
      "0.6413408164709637\n",
      "0.6416703450346182\n",
      "0.6431953134069864\n",
      "0.6426174109006499\n",
      "0.6436415562621193\n",
      "0.6446427383826484\n",
      "0.6441188046849974\n",
      "0.646722226391053\n",
      "0.6474462356627982\n",
      "0.6478054836228516\n",
      "0.6492287742852973\n",
      "0.6487519910274467\n",
      "0.6493102121167028\n",
      "0.6497024909245699\n",
      "0.6501100148282885\n",
      "0.6483127910063562\n",
      "0.6478933977899984\n",
      "0.6491467282660388\n",
      "0.6491302939483538\n",
      "0.6495006637416867\n",
      "0.6504708142130751\n",
      "0.6510152472831916\n",
      "0.649589652460997\n",
      "0.6505815705254828\n",
      "0.6493554723858775\n",
      "0.6494102281486295\n",
      "0.6495563093799133\n",
      "0.6495005188154803\n",
      "0.6507697806150861\n",
      "0.6532660457047944\n",
      "0.6526967370903144\n",
      "0.6527140967204207\n",
      "0.6526689086350392\n",
      "0.6532556506128206\n",
      "0.6530308921002305\n",
      "0.6543800623474485\n",
      "0.6548172415900626\n",
      "0.6554093353149831\n",
      "0.6554708836642631\n",
      "0.6554564513917405\n",
      "0.6559709975417773\n",
      "0.6570858721223557\n",
      "0.6561784110713239\n",
      "0.6572289720297984\n",
      "0.6557741551632358\n",
      "0.6555653432149992\n",
      "0.6557094916570013\n",
      "0.6572190286115198\n",
      "0.6561805690324363\n",
      "0.6565312205049654\n",
      "0.6561507594145375\n",
      "0.657414218335903\n",
      "0.6586689690440786\n",
      "0.6581098253168844\n",
      "0.6589664669200423\n",
      "0.6585894972165132\n",
      "0.6593346859225887\n",
      "0.6589495808418817\n",
      "0.6586134086694864\n",
      "0.6583975194322712\n",
      "0.6585160937040592\n",
      "0.6583944790137042\n",
      "0.6589479688395337\n",
      "0.6580858079336964\n",
      "0.6574467343573889\n",
      "0.6577796600956503\n",
      "0.6573235289585423\n",
      "0.6565541425006095\n",
      "0.6566763705013863\n",
      "0.6572060231626199\n",
      "0.6571948483396214\n",
      "0.6578321903553956\n",
      "0.6580277368652703\n",
      "0.6587438004919761\n",
      "0.6592037384683437\n",
      "0.6599803903318512\n",
      "0.6596118446199148\n",
      "0.6594641748137394\n",
      "0.660063374114062\n",
      "0.6597495624840303\n",
      "0.6598925956885529\n",
      "0.6595254718926359\n",
      "0.6595240628950838\n",
      "0.6594266119436529\n",
      "0.6589855455957496\n",
      "0.6589547017068395\n",
      "0.6590026701126552\n",
      "0.6591185154824144\n",
      "0.6584516213315098\n",
      "0.6582968892111892\n",
      "0.6577473759173205\n",
      "0.657269051540348\n",
      "0.6581425482823797\n",
      "0.6576199879968588\n",
      "0.6575045174443527\n",
      "0.6576855611419922\n",
      "0.6573985711326462\n",
      "0.6574127944237534\n",
      "0.6578211415053027\n",
      "0.6580859573577267\n",
      "0.6585109859240892\n",
      "0.6584029914047975\n",
      "0.6587647026948474\n",
      "0.6582955705099172\n",
      "0.658691414232594\n",
      "0.6588874373836767\n",
      "0.6592249183608446\n",
      "0.6593632759263912\n",
      "0.659484216590506\n",
      "0.6598909213480313\n",
      "0.6596815924152319\n",
      "0.6596256354113554\n",
      "0.6597947253628496\n",
      "0.6596148711556065\n",
      "0.6596567314983315\n",
      "0.6598193776109403\n",
      "0.6594859066068828\n",
      "0.658589409036096\n",
      "0.6586923164166059\n",
      "0.6586473429448906\n",
      "0.6584144012992357\n",
      "0.6587265043389917\n",
      "0.6587450041792119\n",
      "0.6583203280885079\n",
      "0.6588061455936448\n",
      "0.6586433627891088\n",
      "0.6586949065593606\n",
      "0.6585964091002041\n",
      "0.658190953896696\n",
      "0.658563503439795\n",
      "0.6586805966727669\n",
      "0.6586174239479755\n",
      "0.6584175576466285\n",
      "0.6579104649532382\n",
      "0.657750896007799\n",
      "0.6578376619492405\n",
      "0.6575972328135968\n",
      "0.6574758621517106\n",
      "0.6574885407985779\n",
      "0.6573895594545519\n",
      "0.6573975278584167\n",
      "0.6574081675050827\n",
      "0.6573922046784287\n",
      "0.6573172110514292\n",
      "0.6573840189980296\n",
      "0.6573918956799032\n",
      "0.6577173497752918\n",
      "0.658006334528044\n",
      "0.6582106960310362\n",
      "0.6581160370777122\n",
      "0.6578894513129461\n",
      "0.6585313719195467\n",
      "0.6579252881478003\n",
      "0.6579112619147613\n",
      "0.6578305268971553\n",
      "0.6577297407551072\n",
      "0.6579299482860745\n",
      "0.6578238824830736\n",
      "0.6580659601852258\n",
      "0.6579102583890035\n",
      "0.6582996916547973\n",
      "0.6579905453302294\n",
      "0.6583144756418577\n",
      "0.6586807241719064\n",
      "0.658804881051591\n",
      "0.6589608457415491\n",
      "0.6592346865053325\n",
      "0.6594452343728294\n",
      "0.6593868354189221\n",
      "0.6590216556457742\n",
      "0.6593587400923426\n",
      "0.659061392193687\n",
      "0.6586936665407899\n",
      "0.6582430857579467\n",
      "0.6580966139404746\n",
      "0.6585371586586081\n",
      "0.6585700600043487\n",
      "0.6588801472428579\n",
      "0.6587582377733235\n",
      "0.6588502031614426\n",
      "0.6590602092134776\n",
      "0.6591533888067237\n",
      "0.6592962097170726\n",
      "0.6592362753638582\n",
      "0.659676399740911\n",
      "0.6591022957094783\n",
      "0.6587140168497859\n",
      "0.6586313897697568\n",
      "0.6588695186859743\n",
      "0.6586256437439247\n",
      "0.65857173207492\n",
      "0.6589246065029802\n",
      "0.6588150044325563\n",
      "0.6587706496813267\n",
      "0.6584198468030332\n",
      "0.6584689519752653\n",
      "0.6585414854773949\n",
      "0.6588724021918175\n",
      "0.658882807254176\n",
      "0.6590809299835083\n",
      "0.6591177974194805\n",
      "0.6593141050581159\n",
      "0.659652414202752\n",
      "0.6596661679500512\n",
      "0.6599733475039464\n",
      "0.6595044062278176\n",
      "0.6592989130260118\n",
      "0.6593681220808346\n",
      "0.6594262725285844\n",
      "0.659641305625416\n",
      "0.65963831720806\n",
      "0.6594800959496785\n",
      "0.6591146659444412\n",
      "0.6592422734598229\n",
      "0.6590797239401905\n",
      "0.6591157505208879\n",
      "0.6591813830539894\n",
      "0.6593572739751357\n",
      "0.6589624557662118\n",
      "0.6591063999862353\n",
      "0.6590730934268395\n",
      "0.658933196976109\n",
      "0.6588846794157532\n",
      "0.6591399813913629\n",
      "0.6593619323203697\n",
      "0.6593693553071407\n",
      "0.6592103059807742\n",
      "0.6590858398577237\n",
      "0.6590148925885473\n",
      "0.658841710045502\n",
      "0.658965655125839\n",
      "0.659118112644086\n",
      "0.6593083563468416\n",
      "0.6590110012287158\n",
      "0.658853213778897\n",
      "0.658950200565096\n",
      "0.6589450129895763\n",
      "0.6588866884233828\n",
      "0.6590117300538894\n",
      "0.6590459249141645\n",
      "0.6587589510038943\n",
      "0.6588601945382374\n",
      "0.6589799510833683\n",
      "0.6590169807976116\n",
      "0.6590318557213887\n",
      "0.6588096187172475\n",
      "0.6587321753757327\n",
      "0.6586483286639775\n",
      "0.6586328513004219\n",
      "0.6587515644015207\n",
      "0.6587725569343512\n",
      "0.6589484806051689\n",
      "0.65893960047484\n",
      "0.659051093829102\n",
      "0.6591704805063033\n",
      "0.6593539674721097\n",
      "0.6593775457071193\n",
      "0.6595453765538721\n",
      "0.6597916285633175\n",
      "0.6601819530269568\n",
      "0.6599832424211303\n",
      "0.6597552641562019\n",
      "0.659684439177369\n",
      "0.6597007222075367\n",
      "0.6598638357641846\n",
      "0.6597259517067888\n",
      "0.6595019210136509\n",
      "0.6595395052202987\n",
      "0.6600382921284991\n",
      "0.6601156212190781\n",
      "0.65996523957379\n",
      "0.6598758257101639\n",
      "0.6599206767333703\n",
      "0.6599405666884293\n",
      "0.6597884844405775\n",
      "0.6596690930802721\n",
      "0.6596637269737284\n",
      "0.6596193977540926\n",
      "0.6597054776654083\n",
      "0.6597815503042043\n",
      "0.6596873168832031\n",
      "0.6598355483877506\n",
      "0.6598876102903358\n",
      "0.6598960463518652\n",
      "0.6599596276604984\n",
      "0.6599907902018861\n",
      "0.66011866840293\n",
      "0.6603734395299685\n",
      "0.6604314952064665\n",
      "0.6603805971300875\n",
      "0.6604895766653027\n",
      "0.6603942835192581\n",
      "0.6603273010787959\n",
      "0.6601592291804294\n",
      "0.6599987578642957\n",
      "0.6601375472262644\n",
      "0.6602626189293738\n",
      "0.660060116738255\n",
      "0.6599187316945441\n",
      "0.6601376010682309\n",
      "0.6600978547652804\n",
      "0.6602482169125764\n",
      "0.6602044655299322\n",
      "0.6602542826204866\n",
      "0.6601410251558281\n",
      "0.6601066153864128\n",
      "0.6600631601060892\n",
      "0.6596898180131721\n",
      "0.659576019762427\n",
      "0.6594122667302307\n",
      "0.6593987980386602\n",
      "0.6596630564589555\n",
      "0.6598611829647183\n",
      "0.659763780595012\n",
      "0.6598021340345881\n",
      "0.6597332276694956\n",
      "0.6598641291558769\n",
      "0.6598900913332452\n",
      "0.6600333460050111\n",
      "0.6601995716221772\n",
      "0.6602904588822731\n",
      "0.6605017323912763\n",
      "0.6605361264897049\n",
      "0.6605374891897856\n",
      "0.6606181872912762\n",
      "0.6604773335645853\n",
      "0.6599540857615986\n",
      "0.6600380306254806\n",
      "0.6601980734233923\n",
      "0.6602375870643927\n",
      "0.6600618075016874\n",
      "0.6601257788755422\n",
      "0.6600040605807093\n",
      "0.6600688973042536\n",
      "0.6600518665186182\n",
      "0.6599314103267464\n",
      "0.6597241898398615\n",
      "0.6596368598613341\n",
      "0.6598228254003944\n",
      "0.6599076320288182\n",
      "0.6601102147617074\n",
      "0.6599033728748298\n",
      "0.6598257264722335\n",
      "0.659764699446544\n",
      "0.6596896600333059\n",
      "0.6594674945905808\n",
      "0.6594251193893147\n",
      "0.659375496895376\n",
      "0.6594945997716003\n",
      "0.6594853333264031\n",
      "0.6595053026448668\n",
      "0.6595528853940238\n",
      "0.6593813980461052\n",
      "0.6594609633115328\n",
      "0.6596397530022142\n",
      "0.6595294373796278\n",
      "0.6594211587967514\n",
      "0.6594691340943768\n",
      "0.6595197715204159\n",
      "0.6597161519384295\n",
      "0.65978009795981\n",
      "0.6599511955864851\n",
      "0.6599579686469499\n",
      "0.6600238324926527\n",
      "0.6597742750865976\n",
      "0.6598143126373964\n",
      "0.6599423248308449\n",
      "0.6601891108823588\n",
      "0.660004736506927\n",
      "0.659831796641405\n",
      "0.659780658368622\n",
      "0.6599947568733164\n",
      "0.6600233889737581\n",
      "0.6600446440216146\n",
      "0.6601406027133057\n",
      "0.660214415794356\n",
      "0.6603268104057627\n",
      "0.6602650076577757\n",
      "0.6601435903502878\n",
      "0.6601253132073415\n",
      "0.660310944607467\n",
      "0.6604638002704413\n",
      "0.6604605503452309\n",
      "0.6604859109734381\n",
      "0.6602912263393494\n",
      "0.6602321752211731\n",
      "0.6602724377088908\n",
      "0.6603815735634562\n",
      "0.660359589703314\n",
      "0.6603322564077021\n",
      "0.6603275915159896\n",
      "0.6602452419032864\n",
      "0.6601849807299851\n",
      "0.6602695957021352\n",
      "0.6602684884155033\n",
      "0.6603228150756587\n",
      "0.6604459270915157\n",
      "0.6602683656895172\n",
      "0.6603206233710914\n",
      "0.6604060093239068\n",
      "0.6605947070325868\n",
      "0.660622297976968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6605793023934641\n",
      "0.6606120419465682\n",
      "0.6606437446820828\n",
      "0.6604954081444636\n",
      "0.6605713358359695\n",
      "0.6607344139339268\n",
      "0.660660102156858\n",
      "0.6608213879102097\n",
      "0.6609459319219592\n",
      "0.6610891096477404\n",
      "0.6610447450375068\n",
      "0.6610806377160064\n",
      "0.6611882072494224\n",
      "0.6611009097744428\n",
      "0.6610873917302088\n",
      "0.6609851025749343\n",
      "0.6610787197464252\n",
      "0.661079010289029\n",
      "0.6611261771464209\n",
      "0.6611069042275738\n",
      "0.6611047878247794\n",
      "0.6613063097487396\n",
      "0.6614783844361928\n",
      "0.6615902075688275\n",
      "0.6615965596020259\n",
      "0.6615172379255501\n",
      "0.661632753702657\n",
      "0.6616415918650884\n",
      "0.6616199588518415\n",
      "0.6616331088620907\n",
      "0.6616399329509869\n",
      "0.6615504795435987\n",
      "0.6617186127102825\n",
      "0.6615503630516615\n",
      "0.6613830970316122\n",
      "0.6614454740116438\n",
      "0.6613945668865788\n",
      "0.6614979494003004\n",
      "0.6614697492716395\n",
      "0.6616462414698259\n",
      "0.6616167141128829\n",
      "0.6615479677567739\n",
      "0.6616369136237218\n",
      "0.6614480983526081\n",
      "0.6615662691869798\n",
      "0.6615812040538062\n",
      "0.661528348958158\n",
      "0.6616196085992502\n",
      "0.6615584205997087\n",
      "0.6617011905660377\n",
      "0.6617179432289645\n",
      "0.6616093963972549\n",
      "0.6615111178052799\n",
      "0.6613977964625748\n",
      "0.661401874503815\n",
      "0.6614425226467299\n",
      "0.6613167504382758\n",
      "0.6612067832154638\n",
      "0.6611548849444028\n",
      "0.6610995885729892\n",
      "0.6610699000103644\n",
      "0.660964589086499\n",
      "0.6609858230189816\n",
      "0.6610219081041049\n",
      "0.6610206108376625\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "#    for iter in range(1,30):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t].astype(int)\n",
    "        \n",
    "        # 4\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t, w)))<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls].astype(int)\n",
    "        #print('min', min(y_tpls))\n",
    "        #print('max', max(y_tpls))\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        #print(ita_t)\n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1-ita_t * lamb) * w + (ita_t / k) * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0], 1), X_tpls),\n",
    "            axis=0).reshape(D,1)\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w_ = w\n",
    "        print(np.linalg.norm(w_thalf))\n",
    "        w = w_thalf * min(1, 1/(np.sqrt(lamb) * np.linalg.norm(w_thalf)))\n",
    "\n",
    "        train_obj.append(objective_function(X_t, y_t, w, lamb))\n",
    "        #print(train_obj[iter-1])\n",
    "        \n",
    "    #print(train_obj)\n",
    "\n",
    "\n",
    "    #return w, train_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2716, 1980,  929, 3041,  557, 4739, 2351, 4039,   99, 2166])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2716, 1980,  929,  557, 4739, 2351, 4039,   99, 2166, 4460])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tpls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1171875  0.5625     0.8125     0.99609375\n",
      " 0.95703125 0.5703125  0.5703125  0.0859375  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.265625\n",
      " 0.92578125 0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.5859375  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23828125 0.94140625 0.98828125 0.98828125\n",
      " 0.9765625  0.81640625 0.765625   0.98828125 0.98828125 0.8828125\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0546875\n",
      " 0.90625    0.98828125 0.98828125 0.80859375 0.14453125 0.\n",
      " 0.0703125  0.93359375 0.98828125 0.8828125  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3203125  0.98828125 0.98828125\n",
      " 0.8828125  0.05859375 0.         0.         0.         0.65625\n",
      " 0.98828125 0.75       0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.3203125  0.98828125 0.98828125 0.80859375 0.0390625\n",
      " 0.         0.         0.078125   0.95703125 0.98828125 0.46484375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3203125\n",
      " 0.98828125 0.98828125 0.98828125 0.08203125 0.         0.\n",
      " 0.4296875  0.98828125 0.98828125 0.203125   0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07421875 0.765625   0.98828125\n",
      " 0.98828125 0.75390625 0.125      0.27734375 0.9765625  0.98828125\n",
      " 0.6953125  0.01171875 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.23828125 0.77734375 0.98828125 0.98828125\n",
      " 0.9375     0.9375     0.98828125 0.9765625  0.2109375  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0703125  0.75390625 0.98828125 0.98828125 0.98828125\n",
      " 0.98828125 0.66796875 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07421875 0.7734375  0.98828125 0.98828125 0.98828125 0.3359375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.5234375\n",
      " 0.98828125 0.98828125 0.98828125 0.8515625  0.09765625 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.91015625 0.98828125 0.98828125\n",
      " 0.98828125 0.98828125 0.1875     0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.27734375 0.9765625  0.98828125 0.828125   0.98828125 0.98828125\n",
      " 0.52734375 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.74609375 0.98828125\n",
      " 0.875      0.11328125 0.93359375 0.98828125 0.8125     0.0234375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.76171875 0.98828125 0.4921875  0.\n",
      " 0.9296875  0.98828125 0.98828125 0.0390625  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.76171875 0.98828125 0.79296875 0.38671875 0.98046875 0.98828125\n",
      " 0.98828125 0.0390625  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3046875  0.98046875\n",
      " 0.98828125 0.984375   0.98828125 0.98828125 0.5625     0.00390625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.49609375 0.98828125 0.98828125\n",
      " 0.98828125 0.98828125 0.1875     0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.01171875 0.41796875 0.74609375 0.56640625 0.46875\n",
      " 0.02734375 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[2176,:])\n",
    "print(ytrain[2176])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63172475])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(ytrain[2176], np.transpose(np.dot(Xtrain[2176,:], w_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.3147609])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(ytrain[3041], np.transpose(np.dot(Xtrain[3041,:], w_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6611226985854638"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_tpls, w_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tpls.reshape(y_tpls.shape[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0    -1]\n",
      " [   -2    -3]\n",
      " [    4     5]\n",
      " ...\n",
      " [-1464 -1465]\n",
      " [-1466 -1467]\n",
      " [ 1468  1469]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array(range(X_tpls.shape[0]*2)).reshape((X_tpls.shape[0],2))\n",
    "print(np.multiply(y_tpls.reshape(y_tpls.shape[0], 1) ,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22438, 22467])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( np.multiply(y_tpls.reshape(y_tpls.shape[0], 1) ,a), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/lamb/iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(lamb * iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-ita_t * lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (ita_t * lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 1)\n",
      "[[0.01638692]\n",
      " [0.02061471]\n",
      " [0.02567001]\n",
      " [0.0257776 ]\n",
      " [0.02368338]]\n"
     ]
    }
   ],
   "source": [
    "print(((1-ita_t * lamb) * w_).shape)\n",
    "print(((1-ita_t * lamb) * w_)[100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4e-06"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ita_t / k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 1)\n",
      "[[14.24609375]\n",
      " [16.66796875]\n",
      " [15.91796875]\n",
      " [15.1953125 ]\n",
      " [11.6875    ]]\n"
     ]
    }
   ],
   "source": [
    "print((np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0], 1), X_tpls),\n",
    "            axis=0).reshape(D,1)).shape)\n",
    "print((np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0], 1), X_tpls),\n",
    "            axis=0).reshape(D,1))[100:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00098249],\n",
       "       [0.00114952],\n",
       "       [0.00109779],\n",
       "       [0.00104795],\n",
       "       [0.00080603]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((ita_t / k) * np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0], 1), X_tpls),\n",
    "            axis=0).reshape(D,1))[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 1)\n",
      "[[ 8.90234375]\n",
      " [13.3671875 ]\n",
      " [12.6484375 ]\n",
      " [16.43359375]\n",
      " [12.70703125]]\n"
     ]
    }
   ],
   "source": [
    "print((np.sum(np.multiply(y_t.reshape(y_t.shape[0], 1), X_t),\n",
    "            axis=0).reshape(D,1)).shape)\n",
    "print((np.sum(np.multiply(y_t.reshape(y_t.shape[0], 1), X_t),\n",
    "            axis=0).reshape(D,1))[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### line 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.414213562373095"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.sqrt(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6421561248852019"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w_thalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2022892993910377"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/np.sqrt(lamb)) / np.linalg.norm(w_thalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2022892993910377"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/ (np.sqrt(lamb) * np.linalg.norm(w_thalf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(1, (1/np.sqrt(lamb)) / np.linalg.norm(w_thalf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01679521]\n",
      " [0.02267094]\n",
      " [0.02501692]\n",
      " [0.02257767]\n",
      " [0.01720586]]\n",
      "[[0.01679521]\n",
      " [0.02267094]\n",
      " [0.02501692]\n",
      " [0.02257767]\n",
      " [0.01720586]]\n"
     ]
    }
   ],
   "source": [
    "print(w[100:105])\n",
    "print((min(1, (1/np.sqrt(lamb)) / np.linalg.norm(w_thalf)) * w_thalf)[100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    lamb = 0.1\n",
    "    w = np.zeros((len(Xtrain[0]), 1))\n",
    "    w_l, train_obj= pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    #ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "    ywx = np.sign((np.multiply(ytest, np.transpose(np.dot(Xtest,w_l)))).ravel())\n",
    "    #ytru = ytest > 0\n",
    "    #test_acc = sum(ytru == ywx)/N\n",
    "    test_acc = sum(ytest == ywx)/N\n",
    "\n",
    "\n",
    "    #return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "ytru = ytest > 0\n",
    "sum(ytru * 1 == ywx * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yytr=[True, True, False, False]\n",
    "yyte=[True, False, False, False]\n",
    "type(yyte)\n",
    "#sum(yytr == yyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2]) > 1\n",
    "b = np.array([[-1],[2]])>0\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==np.transpose(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array([[1,1],[1,0],[0,1],[-1,-1]])\n",
    "wg = np.array([[2],[4]])\n",
    "np.dot(g,wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###### Q1.1 ######\n",
    "def objective_function(X, y, w, lamb):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A 2 dimensional numpy array of data (number of samples x number of features)\n",
    "    - ytrain: A 1 dimensional numpy array of labels (length = number of samples )\n",
    "    - w: a numpy array of D elements as a D-dimension weight vector\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "\n",
    "    Return:\n",
    "    - obj_value: the value of objective function in SVM primal formulation\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    \n",
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx))  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    zmax = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2) + np.sum(zmax) / N\n",
    "    #print(obj_value)\n",
    "\n",
    "    return obj_value\n",
    "\n",
    "\n",
    "###### Q1.2 ######\n",
    "def pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t]\n",
    "        \n",
    "        # 4\n",
    "        #A_tpls = A_t[np.multiply(y_t, np.dot(X_t,w)) < 1]\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls]\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1 - (ita_t * lamb)) * w + ita_t * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),\n",
    "            axis=0).reshape(D,1)/ k\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w = w_thalf * min(1, 1 / np.sqrt(lamb) / np.linalg.norm(w_thalf))\n",
    "        \n",
    "        train_obj.append(objective_function(Xtrain, ytrain, w, lamb))\n",
    "        print(train_obj[iter-1])\n",
    "        # print(w[0])\n",
    "    \n",
    "\n",
    "    return w, train_obj\n",
    "\n",
    "\n",
    "###### Q1.3 ######\n",
    "def pegasos_test(Xtest, ytest, w_l):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest, w_l))) > 0).ravel()\n",
    "    ytru = ytest > 0\n",
    "    test_acc = sum(ytru == ywx)/N\n",
    "\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NO MODIFICATIONS below this line.\n",
    "You should only write your code in the above functions.\n",
    "\"\"\"\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "\n",
    "    with open(dataset, 'r') as f:\n",
    "            data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = train_set[0]\n",
    "    ytrain = train_set[1]\n",
    "    Xvalid = valid_set[0]\n",
    "    yvalid = valid_set[1]\n",
    "    Xtest = test_set[0]\n",
    "    ytest = test_set[1]\n",
    "\n",
    "    ## below we add 'one' to the feature of each sample, such that we include the bias term into parameter w\n",
    "    Xtrain = np.hstack((np.ones((len(Xtrain), 1)), np.array(Xtrain))).tolist()\n",
    "    Xvalid = np.hstack((np.ones((len(Xvalid), 1)), np.array(Xvalid))).tolist()\n",
    "    Xtest = np.hstack((np.ones((len(Xtest), 1)), np.array(Xtest))).tolist()\n",
    "\n",
    "    for i, v in enumerate(ytrain):\n",
    "        if v < 5:\n",
    "            ytrain[i] = -1.\n",
    "        else:\n",
    "            ytrain[i] = 1.\n",
    "    for i, v in enumerate(ytest):\n",
    "        if v < 5:\n",
    "            ytest[i] = -1.\n",
    "        else:\n",
    "            ytest[i] = 1.\n",
    "\n",
    "    return Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest\n",
    "\n",
    "\n",
    "def pegasos_mnist():\n",
    "\n",
    "    test_acc = {}\n",
    "    train_obj = {}\n",
    "\n",
    "    Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "\n",
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    for lamb in (0.01, 0.1, 1):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    lamb = 0.1\n",
    "    for k in (1, 10, 1000):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    return test_acc, train_obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_acc, train_obj = pegasos_mnist() # results on mnist\n",
    "    print('mnist test acc \\n')\n",
    "    for key, value in test_acc.items():\n",
    "        print('%s: test acc = %.4f \\n' % (key, value))\n",
    "\n",
    "    #with open('pegasos.json', 'w') as f_json:\n",
    "    #    json.dump([test_acc, train_obj], f_json)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "X = np.array(Xtrain)\n",
    "y = np.array(ytrain)\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "w = np.ones((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.842"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "clf = svm.LinearSVC(C = 0.25,penalty='l2', loss='hinge')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "accuracy_score(ytest, clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.846"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC(C = 0.5,penalty='l2', loss='hinge')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "accuracy_score(ytest, clf.predict(Xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Takato\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC(C = 0.125,penalty='l2', loss='hinge')\n",
    "clf.fit(Xtrain, ytrain)\n",
    "accuracy_score(ytest, clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([3,4]))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.63671875],\n",
       "       [101.91015625],\n",
       "       [130.578125  ],\n",
       "       ...,\n",
       "       [190.23828125],\n",
       "       [ 97.51953125],\n",
       "       [132.16796875]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.dot(X,w)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  41.63671875,   41.63671875,  -41.63671875, ...,  -41.63671875,\n",
       "          41.63671875,   41.63671875],\n",
       "       [ 101.91015625,  101.91015625, -101.91015625, ..., -101.91015625,\n",
       "         101.91015625,  101.91015625],\n",
       "       [ 130.578125  ,  130.578125  , -130.578125  , ..., -130.578125  ,\n",
       "         130.578125  ,  130.578125  ],\n",
       "       ...,\n",
       "       [ 190.23828125,  190.23828125, -190.23828125, ..., -190.23828125,\n",
       "         190.23828125,  190.23828125],\n",
       "       [  97.51953125,   97.51953125,  -97.51953125, ...,  -97.51953125,\n",
       "          97.51953125,   97.51953125],\n",
       "       [ 132.16796875,  132.16796875, -132.16796875, ..., -132.16796875,\n",
       "         132.16796875,  132.16796875]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y, np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(y, np.dot(X,w))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -40.63671875,  -40.63671875,   42.63671875, ...,   42.63671875,\n",
       "         -40.63671875,  -40.63671875],\n",
       "       [-100.91015625, -100.91015625,  102.91015625, ...,  102.91015625,\n",
       "        -100.91015625, -100.91015625],\n",
       "       [-129.578125  , -129.578125  ,  131.578125  , ...,  131.578125  ,\n",
       "        -129.578125  , -129.578125  ],\n",
       "       ...,\n",
       "       [-189.23828125, -189.23828125,  191.23828125, ...,  191.23828125,\n",
       "        -189.23828125, -189.23828125],\n",
       "       [ -96.51953125,  -96.51953125,   98.51953125, ...,   98.51953125,\n",
       "         -96.51953125,  -96.51953125],\n",
       "       [-131.16796875, -131.16796875,  133.16796875, ...,  133.16796875,\n",
       "        -131.16796875, -131.16796875]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- np.multiply(y, np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- np.multiply(y, np.transpose(np.dot(X,w)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 1 - np.multiply(y, np.dot(X,w))\n",
    "z = z[z>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[0,1]])\n",
    "b = np.array([0,1])\n",
    "np.dot(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "max_iterations = 500\n",
    "w = np.zeros((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "#    for iter in range(1,2):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t].astype(int)\n",
    "        \n",
    "        # 4\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t, w)))<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls].astype(int)\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1-ita_t * lamb) * w + (ita_t / k) * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0], 1) ,X_tpls),\n",
    "            axis=0).reshape(D,1)\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w = w_thalf * min(1, 1/(np.sqrt(lamb) * np.linalg.norm(w_thalf)))\n",
    "\n",
    "        train_obj.append(objective_function(X_t, y_t, w, lamb))\n",
    "        #print(train_obj[iter-1])\n",
    "        \n",
    "    #print(train_obj)\n",
    "\n",
    "\n",
    "    #return w, train_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 785)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[[10, 4, 6],:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 785)\n",
      "(735, 785)\n"
     ]
    }
   ],
   "source": [
    "print(X_t.shape)\n",
    "print(X_tpls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(type(train_obj))\n",
    "print(len(train_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_tpls.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1,  1,  1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1, -1, -1, -1,\n",
       "       -1, -1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "       -1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1,  1, -1,  1, -1, -1, -1,\n",
       "        1, -1, -1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1,\n",
       "       -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1,  1,\n",
       "       -1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1, -1, -1,  1,  1, -1,  1,\n",
       "        1, -1,  1, -1,  1,  1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1,\n",
       "        1, -1, -1, -1, -1,  1, -1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1, -1, -1, -1,  1,  1, -1, -1,  1,  1, -1,  1,  1, -1,  1,\n",
       "       -1,  1, -1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,\n",
       "        1, -1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1, -1, -1,  1,\n",
       "        1, -1,  1, -1,  1,  1, -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,\n",
       "       -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1, -1, -1,\n",
       "       -1,  1, -1,  1,  1,  1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1,\n",
       "        1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1, -1,  1, -1,  1,  1,\n",
       "        1, -1,  1,  1, -1,  1,  1, -1,  1, -1, -1,  1,  1,  1, -1,  1, -1,\n",
       "       -1,  1,  1, -1, -1, -1,  1, -1, -1,  1, -1, -1,  1,  1,  1,  1, -1,\n",
       "        1,  1, -1,  1,  1, -1,  1,  1, -1, -1,  1,  1, -1,  1,  1,  1,  1,\n",
       "       -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1,  1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
       "       -1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1, -1, -1,  1,  1,\n",
       "        1,  1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1,  1,  1, -1, -1,  1,  1, -1,  1, -1, -1,\n",
       "        1,  1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1,\n",
       "        1, -1,  1, -1, -1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "       -1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1, -1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1,  1,  1, -1,\n",
       "        1, -1, -1,  1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1,  1,  1,  1, -1, -1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1,\n",
       "        1,  1,  1,  1, -1,  1,  1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,\n",
       "       -1, -1,  1,  1, -1, -1, -1,  1,  1,  1, -1,  1, -1, -1,  1,  1,  1,\n",
       "       -1,  1, -1,  1,  1,  1, -1,  1, -1,  1,  1, -1, -1, -1,  1,  1, -1,\n",
       "        1, -1, -1, -1,  1, -1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,\n",
       "       -1, -1, -1, -1,  1, -1, -1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,\n",
       "        1, -1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1,  1, -1,  1,  1,\n",
       "       -1, -1, -1,  1, -1,  1,  1, -1,  1, -1,  1,  1,  1, -1,  1,  1, -1,\n",
       "        1,  1,  1, -1,  1, -1,  1,  1, -1,  1, -1, -1, -1,  1,  1, -1, -1,\n",
       "        1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,\n",
       "       -1, -1,  1,  1, -1, -1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1, -1,\n",
       "       -1,  1, -1, -1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1,  1,  1, -1,\n",
       "       -1, -1, -1,  1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).reshape(D,1)\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(735,)\n",
      "(1000, 1000)\n",
      "(1, 1000)\n",
      "[ True  True  True False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True False  True  True False  True  True  True  True  True  True\n",
      " False False  True False  True False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True False  True\n",
      "  True False  True False  True  True  True  True  True False False False\n",
      "  True False  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True False  True  True  True  True False  True  True  True\n",
      "  True False  True False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True  True False False\n",
      "  True False  True  True False  True  True  True  True False False  True\n",
      "  True False  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True False False False False  True  True  True\n",
      " False False False  True  True False  True  True  True False False False\n",
      "  True  True  True  True False  True  True  True  True False  True False\n",
      " False False  True  True  True  True  True  True False  True False  True\n",
      " False False  True  True False  True  True  True  True  True  True  True\n",
      " False  True  True False  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True  True  True False  True  True\n",
      "  True  True  True False  True  True False False  True False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False False\n",
      "  True False  True  True False False  True  True  True  True  True  True\n",
      "  True  True  True  True False False  True  True  True  True  True  True\n",
      "  True False  True False  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True False False  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True False False False  True\n",
      "  True  True  True False  True  True  True  True False  True  True  True\n",
      "  True False  True  True False  True False  True False  True False False\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      " False  True  True False  True  True False False False  True  True  True\n",
      " False  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True False False  True False  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True False  True  True  True  True\n",
      "  True False False  True False  True  True  True False  True False  True\n",
      " False False  True  True  True  True False False  True  True  True False\n",
      "  True False  True False  True False  True  True  True  True  True  True\n",
      "  True False  True False  True  True  True  True False  True False  True\n",
      " False  True  True  True False False  True False  True  True  True  True\n",
      "  True False False  True  True  True False  True  True  True  True  True\n",
      " False  True False  True  True False  True False False  True False False\n",
      " False False  True False  True False  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False  True  True False False\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      " False False  True False  True  True False  True  True  True False False\n",
      "  True False  True False  True False  True False False  True  True False\n",
      " False  True  True False  True  True  True False False  True  True False\n",
      "  True  True False False False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True False False False  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True False  True\n",
      "  True False  True  True  True  True  True False  True  True False  True\n",
      "  True  True  True False  True  True False  True  True False  True  True\n",
      " False False  True  True  True False  True  True False  True  True False\n",
      "  True  True  True  True False  True  True  True  True False  True False\n",
      "  True  True  True False  True  True False  True  True False False  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True False False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True False False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      "  True  True  True False  True  True  True  True  True  True False  True\n",
      " False  True  True False False  True  True  True False  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True False False  True False False False  True\n",
      "  True False False False  True  True False  True False False  True  True\n",
      "  True  True  True  True  True False  True  True  True False  True False\n",
      "  True  True  True  True False  True False  True False  True  True False\n",
      "  True False  True  True False  True  True False False  True  True  True\n",
      "  True  True False  True  True False False False False False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True False False  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False  True False  True  True\n",
      " False  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(A_t.shape)\n",
    "print(A_tpls.shape)\n",
    "print((np.multiply(y_t, np.dot(X_t,w)) < 1).shape)\n",
    "print((np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).shape)\n",
    "print((np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(y_t, np.dot(X_t,w))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.multiply(y_t, np.dot(X_t,w)))<1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73040117, -0.73040117,  0.73040117, ..., -0.73040117,\n",
       "        -0.73040117,  0.73040117],\n",
       "       [-0.67572895, -0.67572895,  0.67572895, ..., -0.67572895,\n",
       "        -0.67572895,  0.67572895],\n",
       "       [-0.02825347, -0.02825347,  0.02825347, ..., -0.02825347,\n",
       "        -0.02825347,  0.02825347],\n",
       "       ...,\n",
       "       [ 0.7524917 ,  0.7524917 , -0.7524917 , ...,  0.7524917 ,\n",
       "         0.7524917 , -0.7524917 ],\n",
       "       [ 0.3820317 ,  0.3820317 , -0.3820317 , ...,  0.3820317 ,\n",
       "         0.3820317 , -0.3820317 ],\n",
       "       [-0.65464567, -0.65464567,  0.65464567, ..., -0.65464567,\n",
       "        -0.65464567,  0.65464567]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y_t, np.dot(X_t,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y_tpls.reshape(y_tpls.shape[0],1), X_tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2716 1980  929 3041  557 4739 2351 4039   99 2166 4460 1262 1741 4575\n",
      " 3074  517 1185 1260 1724  128 2738  989 1059 2283 3626 1569 4644  996\n",
      " 4424 2149 2707  464  521 4475 2221  555 4033 1752 3710 4295  813 3265\n",
      " 3948 3239 2290 2161 2890 2070 4099 2188 2098 3445 3040 4042 3542 3950\n",
      " 4498 1302 3308 4533 4827 1716 1306 2787 1532 2811 3928 3494  828 4260\n",
      "  625 3560 1750 3570 1266 4139 1085 3161 2202 4076 3314 1527 4193 3263\n",
      " 3721 3742 2841 4593 4445 4079 2240 3533 1600 1105 3225 1596 3497 2837\n",
      "  395 1936  924  487 1735 4037   51 1965  866 3855 3172  842 4350 4637\n",
      " 2286 2810 3982 2627 2822 2783  953 3194 3014 2182 3328 1410 3251 3433\n",
      " 3867  270 2313 4228 4822  513 4000 1940 1812 3813 4007 3133 3237 4812\n",
      " 1608 3328 4614 1542  894  945 3658 2718 3057 4633 2246 1123 2204 3691\n",
      " 3456 4976  199 4362 2347 2400 1349 4180 4025 1798  923 3461 3845   12\n",
      " 2553 4392  969 2796 2138 4681  278 4098 3810 3190  421  813 2784 1982\n",
      " 1363 1287  349 3710 3310 2595 3855 2869 2480 1689 1982 2981 2542 1710\n",
      "  919 2173 3674 2669  560 3417 3684 1559 1352 4363 2379 1308  261 4371\n",
      " 1516 3425  972 1342  747  544  648 4856 3314 4523  340 3505 1653 4348\n",
      "  963 3460 2578 4303 1549   84 2485 3043 1231  300 3735 1658 1460 2097\n",
      " 4955   94  694 3137  292 3158 1095  442 1685 2391 4866 2686  705 2096\n",
      " 1609 4903 3681 2827  262 3100 1233 4657  251 4423 4624 3092 3670 2689\n",
      "  966 3518 3388 2250 2884 4929 3745  803 1132 4411 1921 4563 2418 1805\n",
      "  648 1250 4356 2156 4312 3933 4293 3291 3511 2412 2077 2037 1399 4248\n",
      " 4635 4504  235 2233 3986 1791  730 4073 3223 1947 4114 1009  100 2430\n",
      "  637 4618 2630 4759 4250 3249  656  362 2189 1669   16 3235  418 2214\n",
      "  978 4321 3858   15 3634  855 3295 1622   13 3118 3673 2532  214 1721\n",
      " 4244 3754 3283 1784 1127 4818 3362 4382 2720  231 1421  862 2549 2462\n",
      "  871 4617 4908 2449 2974 4606  797 2128 1796  286 2483 3391 1983 4248\n",
      " 4667 1661 2057 3334  807 3728 4913 1769 2422 3424 1242 4704 1463 3658\n",
      " 4912 4502 3490 1117 4748 1973 4296 2899 1372  119 1476 1988 4819 1982\n",
      " 3565 2449 4998 3900 1152  660 3578 1699 1514 3145 4344 3272  901 3716\n",
      " 2845   90 4072 1335  580 4052  444 2143 3715  844 4046 1896 1085  775\n",
      " 3438 3253 3694 3715 3089 4654   22 3855 1035 3251   91 1152 4328  822\n",
      " 1178 1168 2797 1303 1942  788 3199 1729 1153 4580 3259 3852 2226 3205\n",
      " 4658  109  852  512 1935 2604 2306  827 2444 1031  551 2420  162 4121\n",
      " 4458 1088 4511 4866 4155 1873 2967 3434 1211  938 3850 3911 3495 1171\n",
      "   59  112 3550  928 4517 2861 2018  685 3538 4609 2711 2671 4095 2911\n",
      " 4712 4298  898 1194  650  213 3979 2466 1349 4012  244  877 1334 4534\n",
      " 2666 2363  539 3870 1673 4159 2196 3480 3323 1899 3733 4022 1194 4955\n",
      "  723  219 4131 4223 4954 2369 3206 2629 3277 3202  741 2711  215 4381\n",
      " 3882 3166 4003 2593  520 4993 4247 3247 2926 4342 2532 1845 1936 2837\n",
      " 4774 3753 4223 2198 2778 2211  652 3374  940 4937 3228 1780 2596 3839\n",
      " 4795 2732  657 3448 1889  840 2631 3300 3148 3205 4120 3764 1031 3639\n",
      "  880 4248  552 3308  435 1149 3306  219  625 2328 4540   81 4682 1336\n",
      " 2873 2203 1838 2291  971  573 1335  551 2498 4745  145 1521  239 3677\n",
      " 4777 1166 3364 1140 1669 4711 4598 4250 1826 4316 4411 2730 4133 2407\n",
      " 1240 3055 3317 4122  962 1362 3925 4058 4065 3907 1444 3003 2832  501\n",
      " 3274 1873 2520 2275 1487  829  139  704 2191 4752  236 4254 1549 1022\n",
      "  904 2113 1947 2767 3790 4729 4874 4763 2141 4441  816 3652 3584 1138\n",
      " 2449  853  251  613 3360 1691 4753 4353  702 3089 2426 1161  454 4494\n",
      " 1625 1351 2975 2875 3815  843 4334   20  660 4281 1542 2098 1184 3056\n",
      "  338 2683 3806 2989 3740 3168 4486  707  119 1333 1686  618 1483 3313\n",
      " 3554 4032 4622 2138 3198 1685 2210  493 1950 2013 1587 4710 2373  629\n",
      " 1105 2405 1945  804 2505 3892 1300 1043 4331  322  928 1433 4963 1186\n",
      " 3012  531 1416 2353  698   94 3525  357 1722  384 2515 1928 4629 2365\n",
      " 4801 1318 1576 4286 4239 2371  100 2863 4014 4353 2111  626 3382 4937\n",
      " 1793 1855  128 4693 2106  871 4491 3509 3913  944   57  580 1339 1041\n",
      " 2120  434 3332 1244 4242 4004  127  366 4020 3292 2316  668 4053   86\n",
      " 3003 1320  611  916 3469 3412 1872 3831  175 1909 1599 1144 2673 3333\n",
      " 3963 1164 2819 4510 4940 3433 4962 2964 1602 2048 2132 2419  947 4979\n",
      " 1396 4243 3536  528 3661 2744 4294  452 2366 1278 2341 4595 1949 2900\n",
      " 4218 3746  118 1941 2625 3051 4731 4387 1441 1356 3174 4347 3411  700\n",
      " 4943 3382 3681 1093 4729 2501 2334  412 1546 2666  159 1441 3419 1688\n",
      "  891 2793 4672 1310 3001  371 1407  621 2082 3807 1135  288   39  233\n",
      "  823 1350 1742 1569  551 4119 2876 3537 3382    9 2910  704 2348 1858\n",
      " 4645 1600  376 4906  781 3888 2543 2565 1014 3723 3690 1286 4555 3861\n",
      " 1196 2311 2178 2157 2888 4236  995 3595  642 4662  409 1376 4763 2784\n",
      " 2506  989 4337 3082 2448 3514  478 2728  133 3470 1653 1445 1914  887\n",
      " 1189 2608 3579 4936  198 4160 4471 4806  433 2502 2182 1550 4778 2955\n",
      " 4146 4364 1467 2869 2568  183 3491 1354 1871  805 4242 3392 4980 1861\n",
      " 2757 1252 4084  287 1551 4772 4185 3697 1451  418 1037  993 4693 1929\n",
      "   58 4748 3065 1900 1697 1061  267 2816 4088 3697  509 1939 4510 4693\n",
      "  969 2993   43 2377 1781 3541 1193 1080 4289 2930 4584  491  514 4612\n",
      "  860 4114 4670 2666 4404 1328]\n",
      "[2716 1980  929  557 4739 2351 4039   99 2166 4460 1262 4575 3074  517\n",
      " 1185 1260 1724  128 2738 1059 2283 3626 1569  996 4424 2707  464  521\n",
      " 4475 2221  555 3710  813 2290 2161 2890 2070 4099 2188 2098 3445 3040\n",
      " 4042 3542 1302 4533 4827 1306 1532 2811 3928 3494  828 1750 1266 4139\n",
      " 1085 3161 2202 3314 1527 4193 3263 3721 3742 2841 4593 4445 4079 2240\n",
      " 3533 1600 1105 3225 1596 3497  395 1936  924  487 1735 4037   51 1965\n",
      "  866 3855 3172 4637 2286 2810 3982 2627 2783  953 3194 3014 2182 1410\n",
      " 3251 3433 3867  270 4228 4822  513 4000 1940 1812 3813 4007 3133 3237\n",
      " 1608 4614 1542  894  945 3658 3057 4633 2246 1123 3691 3456 4976  199\n",
      " 2347 1349 4180 4025 1798  923 3461 3845   12 4392  969 2796 2138  278\n",
      " 4098 3810  421  813 2784 1982 1363 1287 3710 3310 2595 3855 2869 1982\n",
      " 2542 1710 2173 3674 2669  560 1559 1352 2379 1308  261 4371 1516 3425\n",
      "  972 1342  544  648 4856 3314 4523  340 3460 2578 4303 3043 1231 3735\n",
      " 1658 1460  694 3137  292 3158  442 1685 2391 4866  705 3681 2827  262\n",
      " 3100 1233 4657 4423 3092  966 3518 2250 2884 4929 3745  803 1132 4411\n",
      " 4563 2418  648 1250 4356 2156 4312 3933 3291 3511 2412 2077 2037 4248\n",
      " 4635 4504  235 3986 1791  730 4073 3223 4114 1009  637 2630 4759 4250\n",
      "  656  362 2189 1669   16 3235  418 2214 3858 3634  855   13 3118 3673\n",
      " 2532  214 1721 4244 3754 3283 1784 3362 4382 2720  231 1421  862 2549\n",
      "  871 4908 2449 2974 4606 2128 1796  286 2483 3391 1983 4248 4667 1661\n",
      " 2057 3334  807 3728 4913 1769 2422 3424 1242 1463 3658 4912 3490 1117\n",
      " 4748 1973 4296 2899 1372  119 1476 1982 3565 2449 4998  660 3578 1699\n",
      " 1514 3145 4344 3272   90 4072 1335  580  444 2143 3715  844 1896 1085\n",
      "  775 3438 3694 3715 4654 3855 3251  822 1178 1168 2797 1303 1942  788\n",
      " 3199 1729 4580 3852 2226 4658  109 2604 2306  827 1031 2420  162 4121\n",
      " 4458 1088 4511 4866 4155 1873 2967 3434 1211  938 3850 3911 3495 1171\n",
      "   59 3550  928 4517 2861 2018 4609 2671 4095 4712 4298  898 1194  650\n",
      "  213 3979 1349 4012  244  877 4534 2666 2363  539 3870 4159 2196 3323\n",
      " 1899 3733 4022 1194  219 4223 4954 2369 2629 3202  215 4381 3882 3166\n",
      "  520 4993 4247 2926 2532 1936 4774 3753 4223 2198 2778 2211  652  940\n",
      " 3228 1780 2596 3839 2732 3448  840 2631 3300 4120 1031 3639  880 4248\n",
      "  552 1149 3306  219 2328 4540   81 4682 1336 2203 2291  971 1335 4745\n",
      " 4777 1166 3364 1669 4711 4598 4250 1826 4316 4411 4133 2407 1240 3055\n",
      " 3317  962 1362 3907 1444  501 3274 1873 2520 2275 1487  829 2191  236\n",
      " 4254 1022  904 2113 3790 4874 2141  816 1138 2449  613 3360 4753 4353\n",
      "  702 1161  454 1625 1351  843 4334   20  660 1542 2098 1184 3056  338\n",
      " 2683 3806 2989  707  119 1333 1686  618 1483 3313 3554 4622 2138 3198\n",
      " 1685  493 1950 1587 4710 2373  629 1105 1945  804 3892 1300 1043 4331\n",
      "  928 1433 1186 3012 1416 2353 3525  357 1722 2515 1928 2365 4801 1576\n",
      " 4286 4239 2371 2863 4014 4353 2111 3382 1793 1855  128 2106  871 3509\n",
      " 3913  580 1041 2120  434 3332 1244 4242 4004  366 4020 3292 2316 4053\n",
      " 1320  611  916 3469 1872 3831  175 1909 1599 1144 2673 3963 1164 4940\n",
      " 3433 4962 1602 2048 2132 2419  947 4979 1396 4243  528 3661 2744  452\n",
      " 2366 1278 2341 4595 1949 4218 3746  118 1941 2625 4731 4387 1441 1356\n",
      " 3174 4347  700 3382 3681 2501 2334  412 2666  159 1441 3419 1688  891\n",
      " 2793 4672 1310 3001  371 1407  621 2082 3807 1135  288   39  823 1742\n",
      " 1569 3537 3382 2348 1858 1600  781 3888 2543 2565 1014 3723 3690 4555\n",
      " 3861 1196 2178 2888 4236  995 3595 4662 1376 2784 2506 4337 2448 3514\n",
      " 2728  133 1445 1914  887 1189 2608 4936  198 2182 4778 2955 4146 4364\n",
      " 1467 2869 2568  183 3491 1354  805 4242 3392 4980 1861 1252 4084 4772\n",
      " 4185 3697 1451  418  993 1929   58 4748 3065 1900 1697 1061  267 2816\n",
      " 4088 3697  509 1939  969   43 2377 3541 1193 1080 4289 4584  491  514\n",
      " 4612  860 4114 4670 2666 4404 1328]\n"
     ]
    }
   ],
   "source": [
    "print(A_t)\n",
    "print(A_tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[A_t].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2716, 1980,  929,  557, 4739, 2351, 4039,   99, 2166, 4460, 1262,\n",
       "       4575, 3074,  517, 1185, 1260, 1724,  128, 2738, 1059, 2283, 3626,\n",
       "       1569,  996, 4424, 2707,  464,  521, 4475, 2221,  555, 3710,  813,\n",
       "       2290, 2161, 2890, 2070, 4099, 2188, 2098, 3445, 3040, 4042, 3542,\n",
       "       1302, 4533, 4827, 1306, 1532, 2811, 3928, 3494,  828, 1750, 1266,\n",
       "       4139, 1085, 3161, 2202, 3314, 1527, 4193, 3263, 3721, 3742, 2841,\n",
       "       4593, 4445, 4079, 2240, 3533, 1600, 1105, 3225, 1596, 3497,  395,\n",
       "       1936,  924,  487, 1735, 4037,   51, 1965,  866, 3855, 3172, 4637,\n",
       "       2286, 2810, 3982, 2627, 2783,  953, 3194, 3014, 2182, 1410, 3251,\n",
       "       3433, 3867,  270, 4228, 4822,  513, 4000, 1940, 1812, 3813, 4007,\n",
       "       3133, 3237, 1608, 4614, 1542,  894,  945, 3658, 3057, 4633, 2246,\n",
       "       1123, 3691, 3456, 4976,  199, 2347, 1349, 4180, 4025, 1798,  923,\n",
       "       3461, 3845,   12, 4392,  969, 2796, 2138,  278, 4098, 3810,  421,\n",
       "        813, 2784, 1982, 1363, 1287, 3710, 3310, 2595, 3855, 2869, 1982,\n",
       "       2542, 1710, 2173, 3674, 2669,  560, 1559, 1352, 2379, 1308,  261,\n",
       "       4371, 1516, 3425,  972, 1342,  544,  648, 4856, 3314, 4523,  340,\n",
       "       3460, 2578, 4303, 3043, 1231, 3735, 1658, 1460,  694, 3137,  292,\n",
       "       3158,  442, 1685, 2391, 4866,  705, 3681, 2827,  262, 3100, 1233,\n",
       "       4657, 4423, 3092,  966, 3518, 2250, 2884, 4929, 3745,  803, 1132,\n",
       "       4411, 4563, 2418,  648, 1250, 4356, 2156, 4312, 3933, 3291, 3511,\n",
       "       2412, 2077, 2037, 4248, 4635, 4504,  235, 3986, 1791,  730, 4073,\n",
       "       3223, 4114, 1009,  637, 2630, 4759, 4250,  656,  362, 2189, 1669,\n",
       "         16, 3235,  418, 2214, 3858, 3634,  855,   13, 3118, 3673, 2532,\n",
       "        214, 1721, 4244, 3754, 3283, 1784, 3362, 4382, 2720,  231, 1421,\n",
       "        862, 2549,  871, 4908, 2449, 2974, 4606, 2128, 1796,  286, 2483,\n",
       "       3391, 1983, 4248, 4667, 1661, 2057, 3334,  807, 3728, 4913, 1769,\n",
       "       2422, 3424, 1242, 1463, 3658, 4912, 3490, 1117, 4748, 1973, 4296,\n",
       "       2899, 1372,  119, 1476, 1982, 3565, 2449, 4998,  660, 3578, 1699,\n",
       "       1514, 3145, 4344, 3272,   90, 4072, 1335,  580,  444, 2143, 3715,\n",
       "        844, 1896, 1085,  775, 3438, 3694, 3715, 4654, 3855, 3251,  822,\n",
       "       1178, 1168, 2797, 1303, 1942,  788, 3199, 1729, 4580, 3852, 2226,\n",
       "       4658,  109, 2604, 2306,  827, 1031, 2420,  162, 4121, 4458, 1088,\n",
       "       4511, 4866, 4155, 1873, 2967, 3434, 1211,  938, 3850, 3911, 3495,\n",
       "       1171,   59, 3550,  928, 4517, 2861, 2018, 4609, 2671, 4095, 4712,\n",
       "       4298,  898, 1194,  650,  213, 3979, 1349, 4012,  244,  877, 4534,\n",
       "       2666, 2363,  539, 3870, 4159, 2196, 3323, 1899, 3733, 4022, 1194,\n",
       "        219, 4223, 4954, 2369, 2629, 3202,  215, 4381, 3882, 3166,  520,\n",
       "       4993, 4247, 2926, 2532, 1936, 4774, 3753, 4223, 2198, 2778, 2211,\n",
       "        652,  940, 3228, 1780, 2596, 3839, 2732, 3448,  840, 2631, 3300,\n",
       "       4120, 1031, 3639,  880, 4248,  552, 1149, 3306,  219, 2328, 4540,\n",
       "         81, 4682, 1336, 2203, 2291,  971, 1335, 4745, 4777, 1166, 3364,\n",
       "       1669, 4711, 4598, 4250, 1826, 4316, 4411, 4133, 2407, 1240, 3055,\n",
       "       3317,  962, 1362, 3907, 1444,  501, 3274, 1873, 2520, 2275, 1487,\n",
       "        829, 2191,  236, 4254, 1022,  904, 2113, 3790, 4874, 2141,  816,\n",
       "       1138, 2449,  613, 3360, 4753, 4353,  702, 1161,  454, 1625, 1351,\n",
       "        843, 4334,   20,  660, 1542, 2098, 1184, 3056,  338, 2683, 3806,\n",
       "       2989,  707,  119, 1333, 1686,  618, 1483, 3313, 3554, 4622, 2138,\n",
       "       3198, 1685,  493, 1950, 1587, 4710, 2373,  629, 1105, 1945,  804,\n",
       "       3892, 1300, 1043, 4331,  928, 1433, 1186, 3012, 1416, 2353, 3525,\n",
       "        357, 1722, 2515, 1928, 2365, 4801, 1576, 4286, 4239, 2371, 2863,\n",
       "       4014, 4353, 2111, 3382, 1793, 1855,  128, 2106,  871, 3509, 3913,\n",
       "        580, 1041, 2120,  434, 3332, 1244, 4242, 4004,  366, 4020, 3292,\n",
       "       2316, 4053, 1320,  611,  916, 3469, 1872, 3831,  175, 1909, 1599,\n",
       "       1144, 2673, 3963, 1164, 4940, 3433, 4962, 1602, 2048, 2132, 2419,\n",
       "        947, 4979, 1396, 4243,  528, 3661, 2744,  452, 2366, 1278, 2341,\n",
       "       4595, 1949, 4218, 3746,  118, 1941, 2625, 4731, 4387, 1441, 1356,\n",
       "       3174, 4347,  700, 3382, 3681, 2501, 2334,  412, 2666,  159, 1441,\n",
       "       3419, 1688,  891, 2793, 4672, 1310, 3001,  371, 1407,  621, 2082,\n",
       "       3807, 1135,  288,   39,  823, 1742, 1569, 3537, 3382, 2348, 1858,\n",
       "       1600,  781, 3888, 2543, 2565, 1014, 3723, 3690, 4555, 3861, 1196,\n",
       "       2178, 2888, 4236,  995, 3595, 4662, 1376, 2784, 2506, 4337, 2448,\n",
       "       3514, 2728,  133, 1445, 1914,  887, 1189, 2608, 4936,  198, 2182,\n",
       "       4778, 2955, 4146, 4364, 1467, 2869, 2568,  183, 3491, 1354,  805,\n",
       "       4242, 3392, 4980, 1861, 1252, 4084, 4772, 4185, 3697, 1451,  418,\n",
       "        993, 1929,   58, 4748, 3065, 1900, 1697, 1061,  267, 2816, 4088,\n",
       "       3697,  509, 1939,  969,   43, 2377, 3541, 1193, 1080, 4289, 4584,\n",
       "        491,  514, 4612,  860, 4114, 4670, 2666, 4404, 1328])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [-1, -1],\n",
       "       [ 0,  1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[1,1],[0,1]])\n",
    "b = np.array([1,-1,1])\n",
    "np.multiply(b.reshape(b.shape[0],1),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_thalf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257762095295042"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, w, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, np.zeros(X_t.shape[1]), lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (785,1) and (785,1) not aligned: 1 (dim 1) != 785 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-49a80ee848d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (785,1) and (785,1) not aligned: 1 (dim 1) != 785 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(w,w.reshape(w.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(w) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tpls[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tpls[0:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls)[0:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    lamb = 0.1\n",
    "    w = np.zeros((len(Xtrain[0]), 1))\n",
    "    w_l, train_obj= pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    #ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "    ywx = np.sign((np.multiply(ytest, np.transpose(np.dot(Xtest,w_l)))).ravel())\n",
    "    #ytru = ytest > 0\n",
    "    #test_acc = sum(ytru == ywx)/N\n",
    "    test_acc = sum(ytest == ywx)/N\n",
    "\n",
    "\n",
    "    #return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "ytru = ytest > 0\n",
    "sum(ytru * 1 == ywx * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yytr=[True, True, False, False]\n",
    "yyte=[True, False, False, False]\n",
    "type(yyte)\n",
    "#sum(yytr == yyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2]) > 1\n",
    "b = np.array([[-1],[2]])>0\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==np.transpose(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array([[1,1],[1,0],[0,1],[-1,-1]])\n",
    "wg = np.array([[2],[4]])\n",
    "np.dot(g,wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

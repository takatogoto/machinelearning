{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###### Q1.1 ######\n",
    "def objective_function(X, y, w, lamb):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A 2 dimensional numpy array of data (number of samples x number of features)\n",
    "    - ytrain: A 1 dimensional numpy array of labels (length = number of samples )\n",
    "    - w: a numpy array of D elements as a D-dimension weight vector\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "\n",
    "    Return:\n",
    "    - obj_value: the value of objective function in SVM primal formulation\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    \n",
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx))  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    zmax = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2) + np.sum(zmax) / N\n",
    "    #print(obj_value)\n",
    "\n",
    "    return obj_value\n",
    "\n",
    "\n",
    "###### Q1.2 ######\n",
    "def pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t]\n",
    "        \n",
    "        # 4\n",
    "        #A_tpls = A_t[np.multiply(y_t, np.dot(X_t,w)) < 1]\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls]\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1 - (ita_t * lamb)) * w + ita_t * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),\n",
    "            axis=0).reshape(D,1)/ k\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w = w_thalf * min(1, 1 / np.sqrt(lamb) / np.linalg.norm(w_thalf))\n",
    "        \n",
    "        train_obj.append(objective_function(Xtrain, ytrain, w, lamb))\n",
    "        print(train_obj[iter-1])\n",
    "        # print(w[0])\n",
    "    \n",
    "\n",
    "    return w, train_obj\n",
    "\n",
    "\n",
    "###### Q1.3 ######\n",
    "def pegasos_test(Xtest, ytest, w_l):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest, w_l))) > 0).ravel()\n",
    "    ytru = ytest > 0\n",
    "    test_acc = sum(ytru == ywx)/N\n",
    "\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NO MODIFICATIONS below this line.\n",
    "You should only write your code in the above functions.\n",
    "\"\"\"\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "\n",
    "    with open(dataset, 'r') as f:\n",
    "            data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = train_set[0]\n",
    "    ytrain = train_set[1]\n",
    "    Xvalid = valid_set[0]\n",
    "    yvalid = valid_set[1]\n",
    "    Xtest = test_set[0]\n",
    "    ytest = test_set[1]\n",
    "\n",
    "    ## below we add 'one' to the feature of each sample, such that we include the bias term into parameter w\n",
    "    Xtrain = np.hstack((np.ones((len(Xtrain), 1)), np.array(Xtrain))).tolist()\n",
    "    Xvalid = np.hstack((np.ones((len(Xvalid), 1)), np.array(Xvalid))).tolist()\n",
    "    Xtest = np.hstack((np.ones((len(Xtest), 1)), np.array(Xtest))).tolist()\n",
    "\n",
    "    for i, v in enumerate(ytrain):\n",
    "        if v < 5:\n",
    "            ytrain[i] = -1.\n",
    "        else:\n",
    "            ytrain[i] = 1.\n",
    "    for i, v in enumerate(ytest):\n",
    "        if v < 5:\n",
    "            ytest[i] = -1.\n",
    "        else:\n",
    "            ytest[i] = 1.\n",
    "\n",
    "    return Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest\n",
    "\n",
    "\n",
    "def pegasos_mnist():\n",
    "\n",
    "    test_acc = {}\n",
    "    train_obj = {}\n",
    "\n",
    "    Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "\n",
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    for lamb in (0.01, 0.1, 1):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    lamb = 0.1\n",
    "    for k in (1, 10, 1000):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    return test_acc, train_obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_acc, train_obj = pegasos_mnist() # results on mnist\n",
    "    print('mnist test acc \\n')\n",
    "    for key, value in test_acc.items():\n",
    "        print('%s: test acc = %.4f \\n' % (key, value))\n",
    "\n",
    "    #with open('pegasos.json', 'w') as f_json:\n",
    "    #    json.dump([test_acc, train_obj], f_json)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "X = np.array(Xtrain)\n",
    "y = np.array(ytrain)\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "w = np.ones((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx)) \n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    D = X.shape[1]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    zmax = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2 ) + np.sum(zmax) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "max_iterations = 500\n",
    "w = np.zeros((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "#    for iter in range(1, max_iterations + 1):\n",
    "    for iter in range(1,5):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t, :]\n",
    "        y_t = ytrain[A_t].astype(int)\n",
    "        \n",
    "        # 4\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t, w)))<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls, :]\n",
    "        y_tpls = ytrain[A_tpls].astype(int)\n",
    "        print('min', min(y_tpls)\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1-ita_t * lamb) * w + (ita_t / k) * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0], 1) ,X_tpls),\n",
    "            axis=0).reshape(D,1)\n",
    "        \n",
    "        # 7\n",
    "        #w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        w_ = w\n",
    "        w = w_thalf * min(1, 1/(np.sqrt(lamb) * np.linalg.norm(w_thalf)))\n",
    "\n",
    "        train_obj.append(objective_function(X_t, y_t, w, lamb))\n",
    "        #print(train_obj[iter-1])\n",
    "        \n",
    "    #print(train_obj)\n",
    "\n",
    "\n",
    "    #return w, train_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2069, 3148, 3892, 4257, 4082,  830, 4141,  293, 1000, 3114])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2069, 3892, 4082, 3114,  573, 1539, 1575, 2503, 1500, 4780])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tpls[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.03125    0.2578125  0.26171875 0.2578125  0.2578125  0.2578125\n",
      " 0.328125   0.78515625 0.9921875  0.03515625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.12109375 0.87890625 0.87890625 0.890625   0.98828125\n",
      " 0.9921875  0.98828125 0.98828125 0.98828125 0.98828125 0.98828125\n",
      " 0.9375     0.03125    0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16015625 0.8671875\n",
      " 0.98828125 0.98046875 0.94921875 0.94921875 0.5859375  0.58203125\n",
      " 0.58203125 0.703125   0.98828125 0.984375   0.5        0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.30859375 0.84375    0.98828125 0.88671875 0.26953125\n",
      " 0.         0.         0.         0.         0.         0.5390625\n",
      " 0.98828125 0.9140625  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.08984375 0.84375\n",
      " 0.98828125 0.87890625 0.15625    0.         0.         0.\n",
      " 0.         0.         0.2421875  0.96484375 0.98828125 0.50390625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.40625    0.98828125 0.87890625 0.16796875\n",
      " 0.         0.         0.         0.         0.         0.01171875\n",
      " 0.58203125 0.98828125 0.8359375  0.03515625 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.26953125 0.65625    0.15625    0.         0.         0.\n",
      " 0.         0.         0.         0.41015625 0.98828125 0.98828125\n",
      " 0.54296875 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05859375 0.75390625 0.98828125 0.703125   0.015625   0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.609375   0.98828125\n",
      " 0.9296875  0.22265625 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.41015625 0.9921875  0.98828125 0.44921875 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.16796875 0.89453125\n",
      " 0.99609375 0.6640625  0.08203125 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03515625 0.82421875 0.98828125 0.80078125 0.09375\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.015625   0.65625\n",
      " 0.98828125 0.97265625 0.21875    0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0234375  0.5234375  0.98828125 0.98828125 0.37890625\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.34765625\n",
      " 0.98828125 0.98828125 0.609375   0.01953125 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1796875  0.953125   0.98828125 0.61328125\n",
      " 0.02734375 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.1015625\n",
      " 0.83203125 0.98828125 0.875      0.0234375  0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.51171875 0.98828125 0.9140625\n",
      " 0.0234375  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.34765625 0.9375     0.8984375  0.33984375 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.40625    0.98828125\n",
      " 0.53125    0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain[2069,:])\n",
    "print(ytrain[2069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.67209914])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(ytrain[2069], np.transpose(np.dot(Xtrain[2069,:], w_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.49211635])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(ytrain[3148], np.transpose(np.dot(Xtrain[3148,:], w_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.254998471914033"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_tpls, w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tpls.reshape(y_tpls.shape[0], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   1]\n",
      " [  2   3]\n",
      " [  4   5]\n",
      " [  6   7]\n",
      " [  8   9]\n",
      " [ 10  11]\n",
      " [ 12  13]\n",
      " [ 14  15]\n",
      " [ 16  17]\n",
      " [ 18  19]\n",
      " [ 20  21]\n",
      " [ 22  23]\n",
      " [ 24  25]\n",
      " [ 26  27]\n",
      " [ 28  29]\n",
      " [ 30  31]\n",
      " [ 32  33]\n",
      " [ 34  35]\n",
      " [ 36  37]\n",
      " [ 38  39]\n",
      " [ 40  41]\n",
      " [ 42  43]\n",
      " [ 44  45]\n",
      " [ 46  47]\n",
      " [ 48  49]\n",
      " [ 50  51]\n",
      " [ 52  53]\n",
      " [ 54  55]\n",
      " [ 56  57]\n",
      " [ 58  59]\n",
      " [ 60  61]\n",
      " [ 62  63]\n",
      " [ 64  65]\n",
      " [ 66  67]\n",
      " [ 68  69]\n",
      " [ 70  71]\n",
      " [ 72  73]\n",
      " [ 74  75]\n",
      " [ 76  77]\n",
      " [ 78  79]\n",
      " [ 80  81]\n",
      " [ 82  83]\n",
      " [ 84  85]\n",
      " [ 86  87]\n",
      " [ 88  89]\n",
      " [ 90  91]\n",
      " [ 92  93]\n",
      " [ 94  95]\n",
      " [ 96  97]\n",
      " [ 98  99]\n",
      " [100 101]\n",
      " [102 103]\n",
      " [104 105]\n",
      " [106 107]\n",
      " [108 109]\n",
      " [110 111]\n",
      " [112 113]\n",
      " [114 115]\n",
      " [116 117]\n",
      " [118 119]\n",
      " [120 121]\n",
      " [122 123]\n",
      " [124 125]\n",
      " [126 127]\n",
      " [128 129]\n",
      " [130 131]\n",
      " [132 133]\n",
      " [134 135]\n",
      " [136 137]\n",
      " [138 139]\n",
      " [140 141]\n",
      " [142 143]\n",
      " [144 145]\n",
      " [146 147]\n",
      " [148 149]\n",
      " [150 151]\n",
      " [152 153]\n",
      " [154 155]\n",
      " [156 157]\n",
      " [158 159]\n",
      " [160 161]\n",
      " [162 163]\n",
      " [164 165]\n",
      " [166 167]\n",
      " [168 169]\n",
      " [170 171]\n",
      " [172 173]\n",
      " [174 175]\n",
      " [176 177]\n",
      " [178 179]\n",
      " [180 181]\n",
      " [182 183]\n",
      " [184 185]\n",
      " [186 187]\n",
      " [188 189]\n",
      " [190 191]\n",
      " [192 193]\n",
      " [194 195]\n",
      " [196 197]\n",
      " [198 199]\n",
      " [200 201]\n",
      " [202 203]\n",
      " [204 205]\n",
      " [206 207]\n",
      " [208 209]\n",
      " [210 211]\n",
      " [212 213]\n",
      " [214 215]\n",
      " [216 217]\n",
      " [218 219]\n",
      " [220 221]\n",
      " [222 223]\n",
      " [224 225]\n",
      " [226 227]\n",
      " [228 229]\n",
      " [230 231]\n",
      " [232 233]\n",
      " [234 235]\n",
      " [236 237]\n",
      " [238 239]\n",
      " [240 241]\n",
      " [242 243]\n",
      " [244 245]\n",
      " [246 247]\n",
      " [248 249]\n",
      " [250 251]\n",
      " [252 253]\n",
      " [254 255]\n",
      " [256 257]\n",
      " [258 259]\n",
      " [260 261]\n",
      " [262 263]\n",
      " [264 265]\n",
      " [266 267]\n",
      " [268 269]\n",
      " [270 271]\n",
      " [272 273]\n",
      " [274 275]\n",
      " [276 277]\n",
      " [278 279]\n",
      " [280 281]\n",
      " [282 283]\n",
      " [284 285]\n",
      " [286 287]\n",
      " [288 289]\n",
      " [290 291]\n",
      " [292 293]\n",
      " [294 295]\n",
      " [296 297]\n",
      " [298 299]\n",
      " [300 301]\n",
      " [302 303]\n",
      " [304 305]\n",
      " [306 307]\n",
      " [308 309]\n",
      " [310 311]\n",
      " [312 313]\n",
      " [314 315]\n",
      " [316 317]\n",
      " [318 319]\n",
      " [320 321]\n",
      " [322 323]\n",
      " [324 325]\n",
      " [326 327]\n",
      " [328 329]\n",
      " [330 331]\n",
      " [332 333]\n",
      " [334 335]\n",
      " [336 337]\n",
      " [338 339]\n",
      " [340 341]\n",
      " [342 343]\n",
      " [344 345]\n",
      " [346 347]\n",
      " [348 349]\n",
      " [350 351]\n",
      " [352 353]\n",
      " [354 355]\n",
      " [356 357]\n",
      " [358 359]\n",
      " [360 361]\n",
      " [362 363]\n",
      " [364 365]\n",
      " [366 367]\n",
      " [368 369]\n",
      " [370 371]\n",
      " [372 373]\n",
      " [374 375]\n",
      " [376 377]\n",
      " [378 379]\n",
      " [380 381]\n",
      " [382 383]\n",
      " [384 385]\n",
      " [386 387]\n",
      " [388 389]\n",
      " [390 391]\n",
      " [392 393]\n",
      " [394 395]\n",
      " [396 397]\n",
      " [398 399]\n",
      " [400 401]\n",
      " [402 403]\n",
      " [404 405]\n",
      " [406 407]\n",
      " [408 409]\n",
      " [410 411]\n",
      " [412 413]\n",
      " [414 415]\n",
      " [416 417]\n",
      " [418 419]\n",
      " [420 421]\n",
      " [422 423]\n",
      " [424 425]\n",
      " [426 427]\n",
      " [428 429]\n",
      " [430 431]\n",
      " [432 433]\n",
      " [434 435]\n",
      " [436 437]\n",
      " [438 439]\n",
      " [440 441]\n",
      " [442 443]\n",
      " [444 445]\n",
      " [446 447]\n",
      " [448 449]\n",
      " [450 451]\n",
      " [452 453]\n",
      " [454 455]\n",
      " [456 457]\n",
      " [458 459]\n",
      " [460 461]\n",
      " [462 463]\n",
      " [464 465]\n",
      " [466 467]\n",
      " [468 469]\n",
      " [470 471]\n",
      " [472 473]\n",
      " [474 475]\n",
      " [476 477]\n",
      " [478 479]\n",
      " [480 481]\n",
      " [482 483]\n",
      " [484 485]\n",
      " [486 487]\n",
      " [488 489]\n",
      " [490 491]\n",
      " [492 493]\n",
      " [494 495]\n",
      " [496 497]\n",
      " [498 499]\n",
      " [500 501]\n",
      " [502 503]\n",
      " [504 505]\n",
      " [506 507]\n",
      " [508 509]\n",
      " [510 511]\n",
      " [512 513]\n",
      " [514 515]\n",
      " [516 517]\n",
      " [518 519]\n",
      " [520 521]\n",
      " [522 523]\n",
      " [524 525]\n",
      " [526 527]\n",
      " [528 529]\n",
      " [530 531]\n",
      " [532 533]\n",
      " [534 535]\n",
      " [536 537]\n",
      " [538 539]\n",
      " [540 541]\n",
      " [542 543]\n",
      " [544 545]\n",
      " [546 547]\n",
      " [548 549]\n",
      " [550 551]\n",
      " [552 553]\n",
      " [554 555]\n",
      " [556 557]\n",
      " [558 559]\n",
      " [560 561]\n",
      " [562 563]\n",
      " [564 565]\n",
      " [566 567]\n",
      " [568 569]\n",
      " [570 571]\n",
      " [572 573]\n",
      " [574 575]\n",
      " [576 577]\n",
      " [578 579]\n",
      " [580 581]\n",
      " [582 583]\n",
      " [584 585]\n",
      " [586 587]\n",
      " [588 589]\n",
      " [590 591]\n",
      " [592 593]\n",
      " [594 595]\n",
      " [596 597]\n",
      " [598 599]\n",
      " [600 601]\n",
      " [602 603]\n",
      " [604 605]\n",
      " [606 607]\n",
      " [608 609]\n",
      " [610 611]\n",
      " [612 613]\n",
      " [614 615]\n",
      " [616 617]\n",
      " [618 619]\n",
      " [620 621]\n",
      " [622 623]\n",
      " [624 625]\n",
      " [626 627]\n",
      " [628 629]\n",
      " [630 631]\n",
      " [632 633]\n",
      " [634 635]\n",
      " [636 637]\n",
      " [638 639]\n",
      " [640 641]\n",
      " [642 643]\n",
      " [644 645]\n",
      " [646 647]\n",
      " [648 649]\n",
      " [650 651]\n",
      " [652 653]\n",
      " [654 655]\n",
      " [656 657]\n",
      " [658 659]\n",
      " [660 661]\n",
      " [662 663]\n",
      " [664 665]\n",
      " [666 667]\n",
      " [668 669]\n",
      " [670 671]\n",
      " [672 673]\n",
      " [674 675]\n",
      " [676 677]\n",
      " [678 679]\n",
      " [680 681]\n",
      " [682 683]\n",
      " [684 685]\n",
      " [686 687]\n",
      " [688 689]\n",
      " [690 691]\n",
      " [692 693]\n",
      " [694 695]\n",
      " [696 697]\n",
      " [698 699]\n",
      " [700 701]\n",
      " [702 703]\n",
      " [704 705]\n",
      " [706 707]\n",
      " [708 709]\n",
      " [710 711]\n",
      " [712 713]\n",
      " [714 715]\n",
      " [716 717]\n",
      " [718 719]\n",
      " [720 721]\n",
      " [722 723]\n",
      " [724 725]\n",
      " [726 727]\n",
      " [728 729]\n",
      " [730 731]\n",
      " [732 733]\n",
      " [734 735]\n",
      " [736 737]\n",
      " [738 739]\n",
      " [740 741]\n",
      " [742 743]\n",
      " [744 745]\n",
      " [746 747]\n",
      " [748 749]\n",
      " [750 751]\n",
      " [752 753]\n",
      " [754 755]\n",
      " [756 757]\n",
      " [758 759]\n",
      " [760 761]\n",
      " [762 763]\n",
      " [764 765]\n",
      " [766 767]\n",
      " [768 769]\n",
      " [770 771]\n",
      " [772 773]\n",
      " [774 775]\n",
      " [776 777]\n",
      " [778 779]\n",
      " [780 781]\n",
      " [782 783]\n",
      " [784 785]\n",
      " [786 787]\n",
      " [788 789]\n",
      " [790 791]\n",
      " [792 793]\n",
      " [794 795]\n",
      " [796 797]\n",
      " [798 799]\n",
      " [800 801]\n",
      " [802 803]\n",
      " [804 805]\n",
      " [806 807]\n",
      " [808 809]\n",
      " [810 811]\n",
      " [812 813]\n",
      " [814 815]\n",
      " [816 817]\n",
      " [818 819]\n",
      " [820 821]\n",
      " [822 823]\n",
      " [824 825]\n",
      " [826 827]\n",
      " [828 829]\n",
      " [830 831]\n",
      " [832 833]\n",
      " [834 835]\n",
      " [836 837]\n",
      " [838 839]\n",
      " [840 841]\n",
      " [842 843]\n",
      " [844 845]\n",
      " [846 847]\n",
      " [848 849]\n",
      " [850 851]\n",
      " [852 853]\n",
      " [854 855]\n",
      " [856 857]\n",
      " [858 859]\n",
      " [860 861]\n",
      " [862 863]\n",
      " [864 865]\n",
      " [866 867]\n",
      " [868 869]\n",
      " [870 871]\n",
      " [872 873]\n",
      " [874 875]\n",
      " [876 877]\n",
      " [878 879]\n",
      " [880 881]\n",
      " [882 883]\n",
      " [884 885]\n",
      " [886 887]\n",
      " [888 889]\n",
      " [890 891]\n",
      " [892 893]\n",
      " [894 895]\n",
      " [896 897]\n",
      " [898 899]\n",
      " [900 901]\n",
      " [902 903]\n",
      " [904 905]\n",
      " [906 907]\n",
      " [908 909]\n",
      " [910 911]\n",
      " [912 913]\n",
      " [914 915]\n",
      " [916 917]\n",
      " [918 919]\n",
      " [920 921]\n",
      " [922 923]\n",
      " [924 925]\n",
      " [926 927]\n",
      " [928 929]\n",
      " [930 931]\n",
      " [932 933]\n",
      " [934 935]\n",
      " [936 937]\n",
      " [938 939]\n",
      " [940 941]\n",
      " [942 943]\n",
      " [944 945]\n",
      " [946 947]\n",
      " [948 949]\n",
      " [950 951]\n",
      " [952 953]\n",
      " [954 955]\n",
      " [956 957]\n",
      " [958 959]\n",
      " [960 961]\n",
      " [962 963]\n",
      " [964 965]\n",
      " [966 967]\n",
      " [968 969]\n",
      " [970 971]\n",
      " [972 973]\n",
      " [974 975]\n",
      " [976 977]\n",
      " [978 979]\n",
      " [980 981]\n",
      " [982 983]\n",
      " [984 985]\n",
      " [986 987]\n",
      " [988 989]\n",
      " [990 991]\n",
      " [992 993]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array(range(497*2)).reshape((497,2))\n",
    "print(np.multiply(y_tpls.reshape(y_tpls.shape[0], 1) ,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(735,)\n",
      "(1000, 1000)\n",
      "(1, 1000)\n",
      "[ True  True  True False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True False  True  True False  True  True  True  True  True  True\n",
      " False False  True False  True False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False  True False  True\n",
      "  True False  True False  True  True  True  True  True False False False\n",
      "  True False  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True False False  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True False  True  True  True  True False  True  True  True\n",
      "  True False  True False  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True  True False False\n",
      "  True False  True  True False  True  True  True  True False False  True\n",
      "  True False  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True False False False False  True  True  True\n",
      " False False False  True  True False  True  True  True False False False\n",
      "  True  True  True  True False  True  True  True  True False  True False\n",
      " False False  True  True  True  True  True  True False  True False  True\n",
      " False False  True  True False  True  True  True  True  True  True  True\n",
      " False  True  True False  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True  True  True False  True  True\n",
      "  True  True  True False  True  True False False  True False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False False\n",
      "  True False  True  True False False  True  True  True  True  True  True\n",
      "  True  True  True  True False False  True  True  True  True  True  True\n",
      "  True False  True False  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True False False  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True False False False  True\n",
      "  True  True  True False  True  True  True  True False  True  True  True\n",
      "  True False  True  True False  True False  True False  True False False\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      " False  True  True False  True  True False False False  True  True  True\n",
      " False  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True False False  True False  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True False  True  True  True  True\n",
      "  True False False  True False  True  True  True False  True False  True\n",
      " False False  True  True  True  True False False  True  True  True False\n",
      "  True False  True False  True False  True  True  True  True  True  True\n",
      "  True False  True False  True  True  True  True False  True False  True\n",
      " False  True  True  True False False  True False  True  True  True  True\n",
      "  True False False  True  True  True False  True  True  True  True  True\n",
      " False  True False  True  True False  True False False  True False False\n",
      " False False  True False  True False  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True False  True  True False False\n",
      " False  True  True False False  True  True  True  True  True  True  True\n",
      " False False  True False  True  True False  True  True  True False False\n",
      "  True False  True False  True False  True False False  True  True False\n",
      " False  True  True False  True  True  True False False  True  True False\n",
      "  True  True False False False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True False False False  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True False  True\n",
      "  True False  True  True  True  True  True False  True  True False  True\n",
      "  True  True  True False  True  True False  True  True False  True  True\n",
      " False False  True  True  True False  True  True False  True  True False\n",
      "  True  True  True  True False  True  True  True  True False  True False\n",
      "  True  True  True False  True  True False  True  True False False  True\n",
      " False  True  True  True  True  True  True  True False  True  True  True\n",
      "  True False  True False False  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False  True  True False False  True  True\n",
      "  True False  True  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      "  True  True  True False  True  True  True  True  True  True False  True\n",
      " False  True  True False False  True  True  True False  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True False False  True False False False  True\n",
      "  True False False False  True  True False  True False False  True  True\n",
      "  True  True  True  True  True False  True  True  True False  True False\n",
      "  True  True  True  True False  True False  True False  True  True False\n",
      "  True False  True  True False  True  True False False  True  True  True\n",
      "  True  True False  True  True False False False False False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False  True  True False False  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False  True False  True  True\n",
      " False  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "print(A_t.shape)\n",
    "print(A_tpls.shape)\n",
    "print((np.multiply(y_t, np.dot(X_t,w)) < 1).shape)\n",
    "print((np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).shape)\n",
    "print((np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(y_t, np.dot(X_t,w))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.multiply(y_t, np.dot(X_t,w)))<1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.73040117, -0.73040117,  0.73040117, ..., -0.73040117,\n",
       "        -0.73040117,  0.73040117],\n",
       "       [-0.67572895, -0.67572895,  0.67572895, ..., -0.67572895,\n",
       "        -0.67572895,  0.67572895],\n",
       "       [-0.02825347, -0.02825347,  0.02825347, ..., -0.02825347,\n",
       "        -0.02825347,  0.02825347],\n",
       "       ...,\n",
       "       [ 0.7524917 ,  0.7524917 , -0.7524917 , ...,  0.7524917 ,\n",
       "         0.7524917 , -0.7524917 ],\n",
       "       [ 0.3820317 ,  0.3820317 , -0.3820317 , ...,  0.3820317 ,\n",
       "         0.3820317 , -0.3820317 ],\n",
       "       [-0.65464567, -0.65464567,  0.65464567, ..., -0.65464567,\n",
       "        -0.65464567,  0.65464567]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y_t, np.dot(X_t,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [-1., -0., -0., ..., -0., -0., -0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y_tpls.reshape(y_tpls.shape[0],1), X_tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2716 1980  929 3041  557 4739 2351 4039   99 2166 4460 1262 1741 4575\n",
      " 3074  517 1185 1260 1724  128 2738  989 1059 2283 3626 1569 4644  996\n",
      " 4424 2149 2707  464  521 4475 2221  555 4033 1752 3710 4295  813 3265\n",
      " 3948 3239 2290 2161 2890 2070 4099 2188 2098 3445 3040 4042 3542 3950\n",
      " 4498 1302 3308 4533 4827 1716 1306 2787 1532 2811 3928 3494  828 4260\n",
      "  625 3560 1750 3570 1266 4139 1085 3161 2202 4076 3314 1527 4193 3263\n",
      " 3721 3742 2841 4593 4445 4079 2240 3533 1600 1105 3225 1596 3497 2837\n",
      "  395 1936  924  487 1735 4037   51 1965  866 3855 3172  842 4350 4637\n",
      " 2286 2810 3982 2627 2822 2783  953 3194 3014 2182 3328 1410 3251 3433\n",
      " 3867  270 2313 4228 4822  513 4000 1940 1812 3813 4007 3133 3237 4812\n",
      " 1608 3328 4614 1542  894  945 3658 2718 3057 4633 2246 1123 2204 3691\n",
      " 3456 4976  199 4362 2347 2400 1349 4180 4025 1798  923 3461 3845   12\n",
      " 2553 4392  969 2796 2138 4681  278 4098 3810 3190  421  813 2784 1982\n",
      " 1363 1287  349 3710 3310 2595 3855 2869 2480 1689 1982 2981 2542 1710\n",
      "  919 2173 3674 2669  560 3417 3684 1559 1352 4363 2379 1308  261 4371\n",
      " 1516 3425  972 1342  747  544  648 4856 3314 4523  340 3505 1653 4348\n",
      "  963 3460 2578 4303 1549   84 2485 3043 1231  300 3735 1658 1460 2097\n",
      " 4955   94  694 3137  292 3158 1095  442 1685 2391 4866 2686  705 2096\n",
      " 1609 4903 3681 2827  262 3100 1233 4657  251 4423 4624 3092 3670 2689\n",
      "  966 3518 3388 2250 2884 4929 3745  803 1132 4411 1921 4563 2418 1805\n",
      "  648 1250 4356 2156 4312 3933 4293 3291 3511 2412 2077 2037 1399 4248\n",
      " 4635 4504  235 2233 3986 1791  730 4073 3223 1947 4114 1009  100 2430\n",
      "  637 4618 2630 4759 4250 3249  656  362 2189 1669   16 3235  418 2214\n",
      "  978 4321 3858   15 3634  855 3295 1622   13 3118 3673 2532  214 1721\n",
      " 4244 3754 3283 1784 1127 4818 3362 4382 2720  231 1421  862 2549 2462\n",
      "  871 4617 4908 2449 2974 4606  797 2128 1796  286 2483 3391 1983 4248\n",
      " 4667 1661 2057 3334  807 3728 4913 1769 2422 3424 1242 4704 1463 3658\n",
      " 4912 4502 3490 1117 4748 1973 4296 2899 1372  119 1476 1988 4819 1982\n",
      " 3565 2449 4998 3900 1152  660 3578 1699 1514 3145 4344 3272  901 3716\n",
      " 2845   90 4072 1335  580 4052  444 2143 3715  844 4046 1896 1085  775\n",
      " 3438 3253 3694 3715 3089 4654   22 3855 1035 3251   91 1152 4328  822\n",
      " 1178 1168 2797 1303 1942  788 3199 1729 1153 4580 3259 3852 2226 3205\n",
      " 4658  109  852  512 1935 2604 2306  827 2444 1031  551 2420  162 4121\n",
      " 4458 1088 4511 4866 4155 1873 2967 3434 1211  938 3850 3911 3495 1171\n",
      "   59  112 3550  928 4517 2861 2018  685 3538 4609 2711 2671 4095 2911\n",
      " 4712 4298  898 1194  650  213 3979 2466 1349 4012  244  877 1334 4534\n",
      " 2666 2363  539 3870 1673 4159 2196 3480 3323 1899 3733 4022 1194 4955\n",
      "  723  219 4131 4223 4954 2369 3206 2629 3277 3202  741 2711  215 4381\n",
      " 3882 3166 4003 2593  520 4993 4247 3247 2926 4342 2532 1845 1936 2837\n",
      " 4774 3753 4223 2198 2778 2211  652 3374  940 4937 3228 1780 2596 3839\n",
      " 4795 2732  657 3448 1889  840 2631 3300 3148 3205 4120 3764 1031 3639\n",
      "  880 4248  552 3308  435 1149 3306  219  625 2328 4540   81 4682 1336\n",
      " 2873 2203 1838 2291  971  573 1335  551 2498 4745  145 1521  239 3677\n",
      " 4777 1166 3364 1140 1669 4711 4598 4250 1826 4316 4411 2730 4133 2407\n",
      " 1240 3055 3317 4122  962 1362 3925 4058 4065 3907 1444 3003 2832  501\n",
      " 3274 1873 2520 2275 1487  829  139  704 2191 4752  236 4254 1549 1022\n",
      "  904 2113 1947 2767 3790 4729 4874 4763 2141 4441  816 3652 3584 1138\n",
      " 2449  853  251  613 3360 1691 4753 4353  702 3089 2426 1161  454 4494\n",
      " 1625 1351 2975 2875 3815  843 4334   20  660 4281 1542 2098 1184 3056\n",
      "  338 2683 3806 2989 3740 3168 4486  707  119 1333 1686  618 1483 3313\n",
      " 3554 4032 4622 2138 3198 1685 2210  493 1950 2013 1587 4710 2373  629\n",
      " 1105 2405 1945  804 2505 3892 1300 1043 4331  322  928 1433 4963 1186\n",
      " 3012  531 1416 2353  698   94 3525  357 1722  384 2515 1928 4629 2365\n",
      " 4801 1318 1576 4286 4239 2371  100 2863 4014 4353 2111  626 3382 4937\n",
      " 1793 1855  128 4693 2106  871 4491 3509 3913  944   57  580 1339 1041\n",
      " 2120  434 3332 1244 4242 4004  127  366 4020 3292 2316  668 4053   86\n",
      " 3003 1320  611  916 3469 3412 1872 3831  175 1909 1599 1144 2673 3333\n",
      " 3963 1164 2819 4510 4940 3433 4962 2964 1602 2048 2132 2419  947 4979\n",
      " 1396 4243 3536  528 3661 2744 4294  452 2366 1278 2341 4595 1949 2900\n",
      " 4218 3746  118 1941 2625 3051 4731 4387 1441 1356 3174 4347 3411  700\n",
      " 4943 3382 3681 1093 4729 2501 2334  412 1546 2666  159 1441 3419 1688\n",
      "  891 2793 4672 1310 3001  371 1407  621 2082 3807 1135  288   39  233\n",
      "  823 1350 1742 1569  551 4119 2876 3537 3382    9 2910  704 2348 1858\n",
      " 4645 1600  376 4906  781 3888 2543 2565 1014 3723 3690 1286 4555 3861\n",
      " 1196 2311 2178 2157 2888 4236  995 3595  642 4662  409 1376 4763 2784\n",
      " 2506  989 4337 3082 2448 3514  478 2728  133 3470 1653 1445 1914  887\n",
      " 1189 2608 3579 4936  198 4160 4471 4806  433 2502 2182 1550 4778 2955\n",
      " 4146 4364 1467 2869 2568  183 3491 1354 1871  805 4242 3392 4980 1861\n",
      " 2757 1252 4084  287 1551 4772 4185 3697 1451  418 1037  993 4693 1929\n",
      "   58 4748 3065 1900 1697 1061  267 2816 4088 3697  509 1939 4510 4693\n",
      "  969 2993   43 2377 1781 3541 1193 1080 4289 2930 4584  491  514 4612\n",
      "  860 4114 4670 2666 4404 1328]\n",
      "[2716 1980  929  557 4739 2351 4039   99 2166 4460 1262 4575 3074  517\n",
      " 1185 1260 1724  128 2738 1059 2283 3626 1569  996 4424 2707  464  521\n",
      " 4475 2221  555 3710  813 2290 2161 2890 2070 4099 2188 2098 3445 3040\n",
      " 4042 3542 1302 4533 4827 1306 1532 2811 3928 3494  828 1750 1266 4139\n",
      " 1085 3161 2202 3314 1527 4193 3263 3721 3742 2841 4593 4445 4079 2240\n",
      " 3533 1600 1105 3225 1596 3497  395 1936  924  487 1735 4037   51 1965\n",
      "  866 3855 3172 4637 2286 2810 3982 2627 2783  953 3194 3014 2182 1410\n",
      " 3251 3433 3867  270 4228 4822  513 4000 1940 1812 3813 4007 3133 3237\n",
      " 1608 4614 1542  894  945 3658 3057 4633 2246 1123 3691 3456 4976  199\n",
      " 2347 1349 4180 4025 1798  923 3461 3845   12 4392  969 2796 2138  278\n",
      " 4098 3810  421  813 2784 1982 1363 1287 3710 3310 2595 3855 2869 1982\n",
      " 2542 1710 2173 3674 2669  560 1559 1352 2379 1308  261 4371 1516 3425\n",
      "  972 1342  544  648 4856 3314 4523  340 3460 2578 4303 3043 1231 3735\n",
      " 1658 1460  694 3137  292 3158  442 1685 2391 4866  705 3681 2827  262\n",
      " 3100 1233 4657 4423 3092  966 3518 2250 2884 4929 3745  803 1132 4411\n",
      " 4563 2418  648 1250 4356 2156 4312 3933 3291 3511 2412 2077 2037 4248\n",
      " 4635 4504  235 3986 1791  730 4073 3223 4114 1009  637 2630 4759 4250\n",
      "  656  362 2189 1669   16 3235  418 2214 3858 3634  855   13 3118 3673\n",
      " 2532  214 1721 4244 3754 3283 1784 3362 4382 2720  231 1421  862 2549\n",
      "  871 4908 2449 2974 4606 2128 1796  286 2483 3391 1983 4248 4667 1661\n",
      " 2057 3334  807 3728 4913 1769 2422 3424 1242 1463 3658 4912 3490 1117\n",
      " 4748 1973 4296 2899 1372  119 1476 1982 3565 2449 4998  660 3578 1699\n",
      " 1514 3145 4344 3272   90 4072 1335  580  444 2143 3715  844 1896 1085\n",
      "  775 3438 3694 3715 4654 3855 3251  822 1178 1168 2797 1303 1942  788\n",
      " 3199 1729 4580 3852 2226 4658  109 2604 2306  827 1031 2420  162 4121\n",
      " 4458 1088 4511 4866 4155 1873 2967 3434 1211  938 3850 3911 3495 1171\n",
      "   59 3550  928 4517 2861 2018 4609 2671 4095 4712 4298  898 1194  650\n",
      "  213 3979 1349 4012  244  877 4534 2666 2363  539 3870 4159 2196 3323\n",
      " 1899 3733 4022 1194  219 4223 4954 2369 2629 3202  215 4381 3882 3166\n",
      "  520 4993 4247 2926 2532 1936 4774 3753 4223 2198 2778 2211  652  940\n",
      " 3228 1780 2596 3839 2732 3448  840 2631 3300 4120 1031 3639  880 4248\n",
      "  552 1149 3306  219 2328 4540   81 4682 1336 2203 2291  971 1335 4745\n",
      " 4777 1166 3364 1669 4711 4598 4250 1826 4316 4411 4133 2407 1240 3055\n",
      " 3317  962 1362 3907 1444  501 3274 1873 2520 2275 1487  829 2191  236\n",
      " 4254 1022  904 2113 3790 4874 2141  816 1138 2449  613 3360 4753 4353\n",
      "  702 1161  454 1625 1351  843 4334   20  660 1542 2098 1184 3056  338\n",
      " 2683 3806 2989  707  119 1333 1686  618 1483 3313 3554 4622 2138 3198\n",
      " 1685  493 1950 1587 4710 2373  629 1105 1945  804 3892 1300 1043 4331\n",
      "  928 1433 1186 3012 1416 2353 3525  357 1722 2515 1928 2365 4801 1576\n",
      " 4286 4239 2371 2863 4014 4353 2111 3382 1793 1855  128 2106  871 3509\n",
      " 3913  580 1041 2120  434 3332 1244 4242 4004  366 4020 3292 2316 4053\n",
      " 1320  611  916 3469 1872 3831  175 1909 1599 1144 2673 3963 1164 4940\n",
      " 3433 4962 1602 2048 2132 2419  947 4979 1396 4243  528 3661 2744  452\n",
      " 2366 1278 2341 4595 1949 4218 3746  118 1941 2625 4731 4387 1441 1356\n",
      " 3174 4347  700 3382 3681 2501 2334  412 2666  159 1441 3419 1688  891\n",
      " 2793 4672 1310 3001  371 1407  621 2082 3807 1135  288   39  823 1742\n",
      " 1569 3537 3382 2348 1858 1600  781 3888 2543 2565 1014 3723 3690 4555\n",
      " 3861 1196 2178 2888 4236  995 3595 4662 1376 2784 2506 4337 2448 3514\n",
      " 2728  133 1445 1914  887 1189 2608 4936  198 2182 4778 2955 4146 4364\n",
      " 1467 2869 2568  183 3491 1354  805 4242 3392 4980 1861 1252 4084 4772\n",
      " 4185 3697 1451  418  993 1929   58 4748 3065 1900 1697 1061  267 2816\n",
      " 4088 3697  509 1939  969   43 2377 3541 1193 1080 4289 4584  491  514\n",
      " 4612  860 4114 4670 2666 4404 1328]\n"
     ]
    }
   ],
   "source": [
    "print(A_t)\n",
    "print(A_tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[A_t].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2716, 1980,  929,  557, 4739, 2351, 4039,   99, 2166, 4460, 1262,\n",
       "       4575, 3074,  517, 1185, 1260, 1724,  128, 2738, 1059, 2283, 3626,\n",
       "       1569,  996, 4424, 2707,  464,  521, 4475, 2221,  555, 3710,  813,\n",
       "       2290, 2161, 2890, 2070, 4099, 2188, 2098, 3445, 3040, 4042, 3542,\n",
       "       1302, 4533, 4827, 1306, 1532, 2811, 3928, 3494,  828, 1750, 1266,\n",
       "       4139, 1085, 3161, 2202, 3314, 1527, 4193, 3263, 3721, 3742, 2841,\n",
       "       4593, 4445, 4079, 2240, 3533, 1600, 1105, 3225, 1596, 3497,  395,\n",
       "       1936,  924,  487, 1735, 4037,   51, 1965,  866, 3855, 3172, 4637,\n",
       "       2286, 2810, 3982, 2627, 2783,  953, 3194, 3014, 2182, 1410, 3251,\n",
       "       3433, 3867,  270, 4228, 4822,  513, 4000, 1940, 1812, 3813, 4007,\n",
       "       3133, 3237, 1608, 4614, 1542,  894,  945, 3658, 3057, 4633, 2246,\n",
       "       1123, 3691, 3456, 4976,  199, 2347, 1349, 4180, 4025, 1798,  923,\n",
       "       3461, 3845,   12, 4392,  969, 2796, 2138,  278, 4098, 3810,  421,\n",
       "        813, 2784, 1982, 1363, 1287, 3710, 3310, 2595, 3855, 2869, 1982,\n",
       "       2542, 1710, 2173, 3674, 2669,  560, 1559, 1352, 2379, 1308,  261,\n",
       "       4371, 1516, 3425,  972, 1342,  544,  648, 4856, 3314, 4523,  340,\n",
       "       3460, 2578, 4303, 3043, 1231, 3735, 1658, 1460,  694, 3137,  292,\n",
       "       3158,  442, 1685, 2391, 4866,  705, 3681, 2827,  262, 3100, 1233,\n",
       "       4657, 4423, 3092,  966, 3518, 2250, 2884, 4929, 3745,  803, 1132,\n",
       "       4411, 4563, 2418,  648, 1250, 4356, 2156, 4312, 3933, 3291, 3511,\n",
       "       2412, 2077, 2037, 4248, 4635, 4504,  235, 3986, 1791,  730, 4073,\n",
       "       3223, 4114, 1009,  637, 2630, 4759, 4250,  656,  362, 2189, 1669,\n",
       "         16, 3235,  418, 2214, 3858, 3634,  855,   13, 3118, 3673, 2532,\n",
       "        214, 1721, 4244, 3754, 3283, 1784, 3362, 4382, 2720,  231, 1421,\n",
       "        862, 2549,  871, 4908, 2449, 2974, 4606, 2128, 1796,  286, 2483,\n",
       "       3391, 1983, 4248, 4667, 1661, 2057, 3334,  807, 3728, 4913, 1769,\n",
       "       2422, 3424, 1242, 1463, 3658, 4912, 3490, 1117, 4748, 1973, 4296,\n",
       "       2899, 1372,  119, 1476, 1982, 3565, 2449, 4998,  660, 3578, 1699,\n",
       "       1514, 3145, 4344, 3272,   90, 4072, 1335,  580,  444, 2143, 3715,\n",
       "        844, 1896, 1085,  775, 3438, 3694, 3715, 4654, 3855, 3251,  822,\n",
       "       1178, 1168, 2797, 1303, 1942,  788, 3199, 1729, 4580, 3852, 2226,\n",
       "       4658,  109, 2604, 2306,  827, 1031, 2420,  162, 4121, 4458, 1088,\n",
       "       4511, 4866, 4155, 1873, 2967, 3434, 1211,  938, 3850, 3911, 3495,\n",
       "       1171,   59, 3550,  928, 4517, 2861, 2018, 4609, 2671, 4095, 4712,\n",
       "       4298,  898, 1194,  650,  213, 3979, 1349, 4012,  244,  877, 4534,\n",
       "       2666, 2363,  539, 3870, 4159, 2196, 3323, 1899, 3733, 4022, 1194,\n",
       "        219, 4223, 4954, 2369, 2629, 3202,  215, 4381, 3882, 3166,  520,\n",
       "       4993, 4247, 2926, 2532, 1936, 4774, 3753, 4223, 2198, 2778, 2211,\n",
       "        652,  940, 3228, 1780, 2596, 3839, 2732, 3448,  840, 2631, 3300,\n",
       "       4120, 1031, 3639,  880, 4248,  552, 1149, 3306,  219, 2328, 4540,\n",
       "         81, 4682, 1336, 2203, 2291,  971, 1335, 4745, 4777, 1166, 3364,\n",
       "       1669, 4711, 4598, 4250, 1826, 4316, 4411, 4133, 2407, 1240, 3055,\n",
       "       3317,  962, 1362, 3907, 1444,  501, 3274, 1873, 2520, 2275, 1487,\n",
       "        829, 2191,  236, 4254, 1022,  904, 2113, 3790, 4874, 2141,  816,\n",
       "       1138, 2449,  613, 3360, 4753, 4353,  702, 1161,  454, 1625, 1351,\n",
       "        843, 4334,   20,  660, 1542, 2098, 1184, 3056,  338, 2683, 3806,\n",
       "       2989,  707,  119, 1333, 1686,  618, 1483, 3313, 3554, 4622, 2138,\n",
       "       3198, 1685,  493, 1950, 1587, 4710, 2373,  629, 1105, 1945,  804,\n",
       "       3892, 1300, 1043, 4331,  928, 1433, 1186, 3012, 1416, 2353, 3525,\n",
       "        357, 1722, 2515, 1928, 2365, 4801, 1576, 4286, 4239, 2371, 2863,\n",
       "       4014, 4353, 2111, 3382, 1793, 1855,  128, 2106,  871, 3509, 3913,\n",
       "        580, 1041, 2120,  434, 3332, 1244, 4242, 4004,  366, 4020, 3292,\n",
       "       2316, 4053, 1320,  611,  916, 3469, 1872, 3831,  175, 1909, 1599,\n",
       "       1144, 2673, 3963, 1164, 4940, 3433, 4962, 1602, 2048, 2132, 2419,\n",
       "        947, 4979, 1396, 4243,  528, 3661, 2744,  452, 2366, 1278, 2341,\n",
       "       4595, 1949, 4218, 3746,  118, 1941, 2625, 4731, 4387, 1441, 1356,\n",
       "       3174, 4347,  700, 3382, 3681, 2501, 2334,  412, 2666,  159, 1441,\n",
       "       3419, 1688,  891, 2793, 4672, 1310, 3001,  371, 1407,  621, 2082,\n",
       "       3807, 1135,  288,   39,  823, 1742, 1569, 3537, 3382, 2348, 1858,\n",
       "       1600,  781, 3888, 2543, 2565, 1014, 3723, 3690, 4555, 3861, 1196,\n",
       "       2178, 2888, 4236,  995, 3595, 4662, 1376, 2784, 2506, 4337, 2448,\n",
       "       3514, 2728,  133, 1445, 1914,  887, 1189, 2608, 4936,  198, 2182,\n",
       "       4778, 2955, 4146, 4364, 1467, 2869, 2568,  183, 3491, 1354,  805,\n",
       "       4242, 3392, 4980, 1861, 1252, 4084, 4772, 4185, 3697, 1451,  418,\n",
       "        993, 1929,   58, 4748, 3065, 1900, 1697, 1061,  267, 2816, 4088,\n",
       "       3697,  509, 1939,  969,   43, 2377, 3541, 1193, 1080, 4289, 4584,\n",
       "        491,  514, 4612,  860, 4114, 4670, 2666, 4404, 1328])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [-1, -1],\n",
       "       [ 0,  1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[1,1],[0,1]])\n",
    "b = np.array([1,-1,1])\n",
    "np.multiply(b.reshape(b.shape[0],1),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_thalf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6257762095295042"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, w, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, np.zeros(X_t.shape[1]), lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (785,1) and (785,1) not aligned: 1 (dim 1) != 785 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-49a80ee848d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: shapes (785,1) and (785,1) not aligned: 1 (dim 1) != 785 (dim 0)"
     ]
    }
   ],
   "source": [
    "np.dot(w,w.reshape(w.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(w) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tpls[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tpls[0:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls)[0:4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    lamb = 0.1\n",
    "    w = np.zeros((len(Xtrain[0]), 1))\n",
    "    w_l, train_obj= pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    #ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "    ywx = np.sign((np.multiply(ytest, np.transpose(np.dot(Xtest,w_l)))).ravel())\n",
    "    #ytru = ytest > 0\n",
    "    #test_acc = sum(ytru == ywx)/N\n",
    "    test_acc = sum(ytest == ywx)/N\n",
    "\n",
    "\n",
    "    #return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "ytru = ytest > 0\n",
    "sum(ytru * 1 == ywx * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yytr=[True, True, False, False]\n",
    "yyte=[True, False, False, False]\n",
    "type(yyte)\n",
    "#sum(yytr == yyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2]) > 1\n",
    "b = np.array([[-1],[2]])>0\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a==np.transpose(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ywx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.array([[1,1],[1,0],[0,1],[-1,-1]])\n",
    "wg = np.array([[2],[4]])\n",
    "np.dot(g,wg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

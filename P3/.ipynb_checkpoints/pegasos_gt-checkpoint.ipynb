{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "###### Q1.1 ######\n",
    "def objective_function(X, y, w, lamb):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A 2 dimensional numpy array of data (number of samples x number of features)\n",
    "    - ytrain: A 1 dimensional numpy array of labels (length = number of samples )\n",
    "    - w: a numpy array of D elements as a D-dimension weight vector\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "\n",
    "    Return:\n",
    "    - obj_value: the value of objective function in SVM primal formulation\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    \n",
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx))  \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    z = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2) + sum(z) / N\n",
    "    #print(obj_value)\n",
    "\n",
    "    return obj_value\n",
    "\n",
    "\n",
    "###### Q1.2 ######\n",
    "def pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t]\n",
    "        y_t = ytrain[A_t]\n",
    "        \n",
    "        # 4\n",
    "        #A_tpls = A_t[np.multiply(y_t, np.dot(X_t,w)) < 1]\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls]\n",
    "        y_tpls = ytrain[A_tpls]\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1-ita_t * lamb) * w + ita_t/k * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).reshape(D,1)\n",
    "        \n",
    "        # 7\n",
    "        w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        \n",
    "        train_obj.append(objective_function(X_t, y_t, w, lamb))\n",
    "        # print(train_obj[iter-1])\n",
    "        # print(w[0])\n",
    "    \n",
    "\n",
    "    return w, train_obj\n",
    "\n",
    "\n",
    "###### Q1.3 ######\n",
    "def pegasos_test(Xtest, ytest, w_l):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "    ytru = ytest > 0\n",
    "    test_acc = sum(ytru == ywx)/N\n",
    "\n",
    "\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "NO MODIFICATIONS below this line.\n",
    "You should only write your code in the above functions.\n",
    "\"\"\"\n",
    "\n",
    "def data_loader_mnist(dataset):\n",
    "\n",
    "    with open(dataset, 'r') as f:\n",
    "            data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "\n",
    "    Xtrain = train_set[0]\n",
    "    ytrain = train_set[1]\n",
    "    Xvalid = valid_set[0]\n",
    "    yvalid = valid_set[1]\n",
    "    Xtest = test_set[0]\n",
    "    ytest = test_set[1]\n",
    "\n",
    "    ## below we add 'one' to the feature of each sample, such that we include the bias term into parameter w\n",
    "    Xtrain = np.hstack((np.ones((len(Xtrain), 1)), np.array(Xtrain))).tolist()\n",
    "    Xvalid = np.hstack((np.ones((len(Xvalid), 1)), np.array(Xvalid))).tolist()\n",
    "    Xtest = np.hstack((np.ones((len(Xtest), 1)), np.array(Xtest))).tolist()\n",
    "\n",
    "    for i, v in enumerate(ytrain):\n",
    "        if v < 5:\n",
    "            ytrain[i] = -1.\n",
    "        else:\n",
    "            ytrain[i] = 1.\n",
    "    for i, v in enumerate(ytest):\n",
    "        if v < 5:\n",
    "            ytest[i] = -1.\n",
    "        else:\n",
    "            ytest[i] = 1.\n",
    "\n",
    "    return Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest\n",
    "\n",
    "\n",
    "def pegasos_mnist():\n",
    "\n",
    "    test_acc = {}\n",
    "    train_obj = {}\n",
    "\n",
    "    Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')\n",
    "\n",
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    for lamb in (0.01, 0.1, 1):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    lamb = 0.1\n",
    "    for k in (1, 10, 1000):\n",
    "        w = np.zeros((len(Xtrain[0]), 1))\n",
    "        w_l, train_obj['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)\n",
    "        test_acc['k=' + str(k) + '_lambda=' + str(lamb)] = pegasos_test(Xtest, ytest, w_l)\n",
    "\n",
    "    return test_acc, train_obj\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_acc, train_obj = pegasos_mnist() # results on mnist\n",
    "    print('mnist test acc \\n')\n",
    "    for key, value in test_acc.items():\n",
    "        print('%s: test acc = %.4f \\n' % (key, value))\n",
    "\n",
    "    with open('pegasos.json', 'w') as f_json:\n",
    "        json.dump([test_acc, train_obj], f_json)\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xvalid, yvalid, Xtest, ytest = data_loader_mnist(dataset = 'mnist_subset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(Xtrain)\n",
    "y = np.array(ytrain)\n",
    "X.shape[1]\n",
    "D = X.shape[1]\n",
    "w = np.ones((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248.8182765625\n"
     ]
    }
   ],
   "source": [
    "    # 0.5 * lamb * ||w||^2 + 1/N sum(max(0,1-ywx)) \n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    D = X.shape[1]\n",
    "    z = 1- np.multiply(y, np.transpose(np.dot(X,w)))\n",
    "    z = z[z>0]\n",
    "    obj_value = 0.5 * lamb * (np.linalg.norm(w) **2 ) + sum(z) / N\n",
    "    print(obj_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(np.array([3,4]))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.63671875],\n",
       "       [101.91015625],\n",
       "       [130.578125  ],\n",
       "       ...,\n",
       "       [190.23828125],\n",
       "       [ 97.51953125],\n",
       "       [132.16796875]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.dot(X,w)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.63671875,  101.91015625, -130.578125  , ..., -190.23828125,\n",
       "         97.51953125,  132.16796875])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y, np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(y, np.dot(X,w))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785,)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -40.63671875, -100.91015625,  131.578125  , ...,  191.23828125,\n",
       "        -96.51953125, -131.16796875])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- np.multiply(y, np.dot(X,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- np.multiply(y, np.transpose(np.dot(X,w)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 1 - np.multiply(y, np.dot(X,w))\n",
    "z = z[z>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[0,1]])\n",
    "b = np.array([0,1])\n",
    "np.dot(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000\n",
    "max_iterations = 500\n",
    "w = np.zeros((D,1))\n",
    "lamb = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtrain: A list of num_train elements, where each element is a list of D-dimensional features.\n",
    "    - ytrain: A list of num_train labels\n",
    "    - w: a numpy array of D elements as a D-dimension vector, which is the weight vector and initialized to be all 0s\n",
    "    - lamb: lambda used in pegasos algorithm\n",
    "    - k: mini-batch size\n",
    "    - max_iterations: the total number of iterations to update parameters\n",
    "\n",
    "    Returns:\n",
    "    - learnt w\n",
    "    - train_obj: a list of the objective function value at each iteration during the training process, length of 500.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    ytrain = np.array(ytrain)\n",
    "    N = Xtrain.shape[0]\n",
    "    D = Xtrain.shape[1]\n",
    "\n",
    "    train_obj = []\n",
    "\n",
    "    for iter in range(1, max_iterations + 1):\n",
    "#    for iter in range(1,2):\n",
    "        A_t = np.floor(np.random.rand(k) * N).astype(int)  # index of the current mini-batch\n",
    "\n",
    "        # you need to fill in your solution here\n",
    "        X_t = Xtrain[A_t]\n",
    "        y_t = ytrain[A_t]\n",
    "        \n",
    "        # 4\n",
    "        #A_tpls = A_t[np.multiply(y_t, np.dot(X_t,w)) < 1]\n",
    "        A_tpls = A_t[(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()]\n",
    "        X_tpls = Xtrain[A_tpls]\n",
    "        y_tpls = ytrain[A_tpls]\n",
    "        \n",
    "        # 5\n",
    "        ita_t = 1/(lamb * iter)\n",
    "        \n",
    "        # 6\n",
    "        w_thalf = (1-ita_t * lamb) * w + ita_t/k * np.sum(\n",
    "            np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).reshape(D,1)\n",
    "        \n",
    "        # 7\n",
    "        w = w_thalf * min(1,1/(np.sqrt(lamb) * np.linalg.norm(np.array(w_thalf))))\n",
    "        \n",
    "        train_obj.append(objective_function(X_t, y_t, w, lamb))\n",
    "        #print(train_obj[iter-1])\n",
    "        #print(w[0])\n",
    "    #print(train_obj)\n",
    "\n",
    "\n",
    "    #return w, train_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4054, 1740, 1057,  296, 4380, 4592,  600, 1672,  876,  579, 4499,\n",
       "        284, 4902,  482, 4317, 2832, 1839, 1711, 3786, 1572, 3286, 2586,\n",
       "       2424, 4505, 2773, 4134, 3627,  192, 3865, 1084, 4515,  214, 1665,\n",
       "        498, 2377, 4100, 1490,  754, 1651, 4069,  701, 1136,  344, 3528,\n",
       "       1976, 1554, 3593, 1679, 3638, 4075, 1088, 4869,  811, 1454,  898,\n",
       "       1727, 2400, 2610, 4268, 4447, 1100, 3114,  557, 2294, 1611, 1582,\n",
       "       2412, 3649,  345, 4395, 3674,  882, 4695, 2531, 4999,  986, 2674,\n",
       "       1451, 1520, 2955, 4608, 4026, 3619, 2795, 4611, 2461, 4369, 4169,\n",
       "       1069, 3856,   60, 1614, 1147, 2534, 3684,  488, 2574, 4692, 1143,\n",
       "       3385])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(type(train_obj))\n",
    "print(len(train_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tpls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = np.sum(np.multiply(y_tpls.reshape(y_tpls.shape[0],1),X_tpls),axis=0).reshape(D,1)\n",
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000,)\n",
      "(1000, 1000)\n",
      "(1, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(A_t.shape)\n",
    "print(A_tpls.shape)\n",
    "print((np.multiply(y_t, np.dot(X_t,w)) < 1).shape)\n",
    "print((np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).shape)\n",
    "(np.multiply(y_t, np.transpose(np.dot(X_t,w)) )<1).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.multiply(y_t, np.dot(X_t,w))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((np.multiply(y_t, np.dot(X_t,w)))<1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0., ...,  0., -0., -0.],\n",
       "       [ 0.,  0.,  0., ...,  0., -0., -0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(y_t, np.dot(X_t,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 628 3819 4309 2998 1208 4396 1839 4390 3595 1413  874 2902 1686 3614\n",
      " 2484 2570 4346 4159  309 3401  805 2301 2893 1844 1795 2240   74 4091\n",
      "  566 1503 2202 4904  418 3682 3906  184 3969 1407  182 1263 4495  832\n",
      " 1014 2979 1471 4556 1983 1422  986   28 1552 1767 3743 3366 2284 4079\n",
      " 2027 2753 4921 1194  573  825 2931 2416   97 2975  727 4012 1248  364\n",
      " 3643 2508 2233 1670  921 3738 2488  201 2850 2757 1033 4348 4179 1672\n",
      " 3636 3298 2073 1867  866  686 1416 2698 2021 4434 3069 2322 2150  547\n",
      "  687 2053 4231 4919 3705  857  132 2556  631 4576 4747 1571 1567 2385\n",
      " 1241  586 2185  726 4522 2427 1330 2642 4604 3390  313 4742 4584 2860\n",
      " 3863  889 3970 2391  805 1475  806 2512  663 4742 2422 4875 2198 2890\n",
      " 1436 1187 1846  640 2070 1461 4801 4582 2767 4290  566 3310 4047 1015\n",
      " 1053 1781 1787 4071 4064 3252 1590 2555 3818   36 4126 4103 4239 1496\n",
      " 1682  973  366  918 2858  567 1392 4593 2235  878 4065  139 3834 4244\n",
      " 2142 2213 3007 4135 1294 2589 4446 4888   58 4104 2419 2946  202 3668\n",
      " 4054 4074 3933   56  760  710 3042  833 2939  621 1210  510   86 2811\n",
      " 1007 2639  568 4390 4653 4879 4275 3466  172 2435 1998 3898 2377  198\n",
      "  438 4503 2969  948 3000 2445  593 2339 2352 4663 4548  495  561  110\n",
      " 1529 1987 2788  350 3010 4372 4896 1772 2827 3295  868 4729 4383  552\n",
      " 4664  736 3932 4509  561 4879 1638 4173 4587 2497 3311  414 2172 2041\n",
      " 2308 2678  100 2237 4953 4176 3166 3817 2101 3515 4884 4534 2409 4742\n",
      " 2101 3072  629 3835 4240  218 2577  869 4401  891 3011 3960 2469 4666\n",
      "  764 2258 4347  922 3392 4895 2652 4537 2871 4507  239 4848 1701 1447\n",
      " 3665 2190 1514  489  704 2725 1017 2301 1474 1211 2830 4184 2969 2647\n",
      " 2013 2452 1463  734 1313 1163 4664 1039 3250 2917 4799 1061 2727 3211\n",
      " 1092 3196  746 3639 2454 3518 2726 2209  241 3221 3894 2553 1009 2237\n",
      " 3495  746  199 2205 4624 4885 3353 2751 3632 1492 1373 4237 3866 3855\n",
      "  506 1201 3519 4198 2442 1453 1926 4113 3273 1166 2345 2277 1919 3892\n",
      " 2499 4269 4703 3261 2593 4219 1834 3048 2148 2075 2331 4698 4277 1156\n",
      " 4129 2130 2210 2768 2609 3463  483 3321 3092 1367 2086 3917 3222 3768\n",
      " 4115   15 1844 2064 2635  361 3388 3352 2952 4768 1530  622 3491  206\n",
      "  931  767  743 3515 1855   71 1347 2123 1857 1300  413 3361 4117 2587\n",
      "  137 4873 3638 3868  931 3590 2351 4263  925  457 1197 1250 3869 3987\n",
      " 2980 1759 1066 4358 3621 3476   75 2936 2752 4288  413 3979 2412  374\n",
      " 3246 2862 3279  344 4622 2105 2852  339 2378  554 1372 3242  209 4832\n",
      " 3072 4239 2279 4388 1992  934 3278 4924  655  265  994 3780 1778 1344\n",
      " 2389 1005  757 2678 1817 3879 2065 1771 2436 3761 2614 2434 1212 4076\n",
      " 2063 1138 3615 4808 1610 1843  877 2652 2611 1315  207 2351 3218  577\n",
      " 3663 4625 3767 4912  946  223 2924  371 3767 4944 4072 2405  314 2942\n",
      "  110  907  865 2448 3310  913  894 1735 2797  743  703 3060 2965 4783\n",
      " 3625  788 3127  720 4360 2371 2695 3318 2213 3416 2780 4667 1640 4214\n",
      " 4271 3094 3232 1664  688 2316 1886 3999 3654 1547 3388 3072 1251 3109\n",
      " 3348 4542 4652 1700   57 2168 1928  718 2531 1231 3277 4800 1548 1719\n",
      "  640 2235  837 4564   38 3725 3430   67  881 1994 4416 3965  803 3899\n",
      "  527 4675  499 3981 2240 2622 1512 2217 3659 4697 3895 1209 4422 3277\n",
      "  984 2211 2298 4626 1180 2466 1331 1313 2843 2593 1564  786 1743 3645\n",
      " 4490 3032 3224 4436  877 2697  809   66 4109 2184 1763  475 3300 1436\n",
      " 2185 1831 4418 2909 2594 2979  411 1499 3208 1101 3469 2680 3482 2027\n",
      " 3792 4817 4182 3685  490 2893 2543 4216 1748 2282 3820 2615  567 1149\n",
      " 1563 2165 4518 1092 2318 3734 1908 2736 4836 2305  626 4581  629   93\n",
      "   81 3280 1820 2295 4957  968 1363   40  601 4501 3113 3245 2632  142\n",
      "  877  359 3322  506   28 1521  239 2330 2821 2715 3371 2455 2725 2917\n",
      " 2123 3593 4903 3006 2841 4110 2399 2778 4030 4239 4117  334 2563  748\n",
      " 2289 2648 4183  243 4814 2885  798 4547 1509 4548 3365 2009 3862 3792\n",
      " 1549 1530 3984 4049  478  335 3451 2200 1566 2960   97 2161  114 1133\n",
      " 4086 4082 2742 2436 1891 3220 3710 1522 4957  621 1253 4726  493 4493\n",
      " 3857 3348 3574  755  825  362 2756 3718 4129 2069 3488   87 2305 4075\n",
      " 1503 3452 1551 3277 3419 2519 2308 1375  839  126 4190 3529 1735   10\n",
      " 3983  785  808 1029 4680 4712 2847  543 2194 2878 2728  412 1085  946\n",
      " 4506 1678 1286 1930  814 3718 3535 1937  525 3580 1430  172 1031  866\n",
      "  420  764 4146 4638 4947 4617 4619 3712 4012  571  479 3456 2855 4823\n",
      " 3838 3905 1610 1136 3272 2666 4690 4473  398 4686 4457 4221 1335 3609\n",
      "   59 2418 4402 2684  700 2882 2100 3277  557 4732  827 4308 3154 3821\n",
      " 2942 3084 2237  703  347  944 4767 1011 2576 1238 3939 3555 3550 4716\n",
      " 3867 3175 2872 2238 4361 4582  499 1058 1474  261 1659 4698 1364 1864\n",
      " 4072 2208 4285  888  392 1380  425 1840 2770 2687 1585 2736 4252 3249\n",
      " 2491 4323   58 2807 4997 2328  305 4986 1289 1292 3774 3870 2055 3416\n",
      " 1966 1244 1994 3851 3042 4798 1115 3494 1745 4319  358 3896 3241 1098\n",
      " 4642 4798  161 4087 1771 2910 4736 3008 1258 4032 1023 4756 2886 3627\n",
      "  230  901 2173 1224  654 3668 2722 3039  941 2628 4186  829 1584 2989\n",
      " 1138 1052 1526 2064 3763 1411 4189 2003 4571 2083 3898 1399 1626  216\n",
      " 2424 4971  694 2848 4101 3129 1017  869  409 3194 1462 3578 1270  553\n",
      " 4263 3629 2537 4050 2755 1128]\n",
      "[ 628 3819 4309 2998 1208 4396 1839 4390 3595 1413  874 1686 2484 2570\n",
      " 4346 4159  309  805 2301 2893 1795 2240  566 1503 2202  418  184 3969\n",
      " 1407  182 1263 4495 1014 2979 1471 4556 1983 1422  986 3743 3366 2284\n",
      " 4079 4921 1194  825 2931 2416   97  727 4012  364 3643 2508 1670  921\n",
      " 3738  201 2850 1033 1672  866 1416 2698 2021 2322  547 3705  132 2556\n",
      "  631 4576 4747 1571 1567 2385 1241  586 2185  726 4522 2427 1330 2642\n",
      " 4604 3390 4742 2860 3863  889 3970 2391  805  663 4742 2422 4875 2198\n",
      " 2890 1436 1187 1846 2070 4801 4582  566 3310 4047 1053 1787 4071 4064\n",
      " 3252 1590   36 4126 4103 4239 1496 1682  973  366  918 2858 4593 2235\n",
      "  878  139 3834 4244 2142 2213 3007 4135 4446 4888   58 4104 2419 2946\n",
      "  202 3668 4054 4074 3933  760  710 3042  833 2939  621 1210  510 2811\n",
      " 1007 2639  568 4390 4653 4879 4275 3466 3898 2377  198  438 4503 2969\n",
      "  948 3000 2445  593 2339 2352 4663 4548  495  561  110 1529 1987 2788\n",
      "  350 3010 4372 4896 1772 2827  868  552 4664  736 4509  561 4879 1638\n",
      " 4173 2497 3311  414 2172 2041 2308 2678 2237 4953 4176 3166 3817 2101\n",
      " 3515 4534 2409 4742 2101 3072  629 3835 4240  218 2577  869 4401  891\n",
      " 3011 3960 2469 4666 4347  922 3392 2652 4537 2871 4507 4848 1701 1447\n",
      " 3665 2190 1514  489 2725 1017 2301 1211 2830 4184 2969 1463  734 1313\n",
      " 4664 2917 4799 1061 2727 3196  746 3639 3518 2726 2209  241 3221 1009\n",
      " 2237 3495  746  199 2205 4885 3353 2751 3632 1492 1373 3866 3855 1201\n",
      " 4198 1453 1926 3273 2345 1919 3892 4269 4703 3261 1834 3048 2148 2075\n",
      " 2331 4698 1156 4129  483 3321 3092 1367 2086 3917 3222 3768 4115 2635\n",
      " 2952 1530 3491  206  743 3515 1855   71 1347 2123 1300  413 3361 2587\n",
      "  137 4873 3638 3868 2351  925  457 3869 3987 4358 3621   75 2936 2752\n",
      " 4288  413 3979 2412  374 3246 2862 4622 2105 2852 1372 3242  209 3072\n",
      " 4239 2279 4388 1992  934 3278 4924  655  265  994 3780 1778 1344 1005\n",
      "  757 2678 3879 2065 1771 2436 3761 2614 1212 2063 1138 3615 4808 1610\n",
      " 1843  877 2652 2611  207 2351 3218  577 3663 4625 4912  946  371 4944\n",
      " 4072  110 2448 3310  913  894 1735 2797  743  703 2965 4783 3625  788\n",
      " 3127  720 4360 2371 3318 2213 3416 4667 1640 3232 1664  688 2316 1886\n",
      " 3999 3654 3072 1251 3109 3348 4542 4652 1700 2168 1928 2531 1231 4800\n",
      " 1719 2235  837 4564 3725 1994 4416 3965  803  499 3981 2240 2622 1512\n",
      " 2217 3659 4697 3895 1209  984 2211 2298 4626 1180 1331 1313 2843  786\n",
      " 1743 3645 3032 4436  877 2697  809   66 1763  475 3300 1436 2185 1831\n",
      " 4418 2594 2979  411 3208 1101 3469 2680 3792 4182 3685  490 2893 2543\n",
      " 4216 1748 2282 3820 2615 1149 1563 2165 4518 2318 3734 1908 2736 2305\n",
      " 4581  629   93   81 1820 2295 4957  968 1363   40  601 4501 3113 2632\n",
      "  877  359 2330 2821 2715 3371 2455 2725 2917 2123 3593 2841 4110 2399\n",
      " 2778 4030 4239 2563  748 2289 4183 2885  798 4547 1509 4548 3365 3862\n",
      " 3792 1530 3984 4049  335 3451 2200 2960   97 2161  114 2742 2436 3220\n",
      " 3710 4957  621 1253  493 3857 3348 3574  825  362 2756 3718 4129 3488\n",
      "   87 2305 1503 3452 2519 2308  839  126 3529 1735   10  785  808 1029\n",
      " 4712 2847  543 2194 2878 2728  412 1085  946 4506 1678 1930  814 3718\n",
      " 3535 1937  525 1430 1031  866 4146 4947 4619 3712 4012 3456 2855 3905\n",
      " 1610 1136 3272 2666 4690 4473  398 4457 4221 1335 3609   59 2418 2684\n",
      "  700  557  827 4308 3154 3084 2237  703  347 4767 1011 2576 1238 3939\n",
      " 3555 3550 4716 3867 3175 2872 4361 4582  499 1058  261 1659 4698 1364\n",
      " 1864 4072 2208 4285  392 1380  425 1840 2687 2736 4252 2491 4323   58\n",
      " 4997 2328 4986 1289 1292 3774 3870 3416 1966 1994 3851 3042 3494 1745\n",
      " 4319  358 3896 3241 1098 4642 4087 1771 4736 3008 1258 1023 4756 2886\n",
      " 3627 2173 1224  654 3668 2722 3039  941 2628  829 2989 1138 1052 1526\n",
      " 3763 2003 4571 2083 3898 1626  216 2424  694 4101 3129 1017  869 3194\n",
      " 1462 3578  553 3629 4050]\n"
     ]
    }
   ],
   "source": [
    "print(A_t)\n",
    "print(A_tpls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[A_t].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4054, 1740,  296, 4592, 1672, 4499,  284,  482, 1839, 3786, 1572,\n",
       "       2424,  192, 3865, 4515, 1665, 2377, 1490, 4069, 1136,  344, 1976,\n",
       "       1554, 3593, 3638, 1454, 4447, 1100, 1611, 2412, 3649, 3674, 4695,\n",
       "       2531,  986, 2674, 1451, 4026, 3619, 4169, 1069, 3856,  488, 2574,\n",
       "       4692])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tpls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ita_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [-1, -1],\n",
       "       [ 0,  1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[1,1],[0,1]])\n",
    "b = np.array([1,-1,1])\n",
    "np.multiply(b.reshape(b.shape[0],1),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_thalf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6404338979628443"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, w, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_function(X_t, y_t, np.zeros(X_t.shape[1]), lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43231824])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(w,w.reshape(w.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4323182393174562"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(w) **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "    max_iterations = 500\n",
    "    k = 100\n",
    "    lamb = 0.1\n",
    "    w = np.zeros((len(Xtrain[0]), 1))\n",
    "    w_l, train_obj= pegasos_train(Xtrain, ytrain, w, lamb, k, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Xtest: A list of num_test elements, where each element is a list of D-dimensional features.\n",
    "    - ytest: A list of num_test labels\n",
    "    - w_l: a numpy array of D elements as a D-dimension vector, which is the weight vector of SVM classifier and learned by pegasos_train()\n",
    " \n",
    "    Returns:\n",
    "    - test_acc: testing accuracy.\n",
    "    \"\"\"\n",
    "    # you need to fill in your solution here\n",
    "    Xtest = np.array(Xtest)\n",
    "    ytest = np.array(ytest)\n",
    "    N = Xtest.shape[0]\n",
    "    ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "    ytru = ytest > 0\n",
    "    test_acc = sum(ytru == ywx)/N\n",
    "\n",
    "\n",
    "    #return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.568"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ywx = (np.multiply(ytest, np.transpose(np.dot(Xtest,w_l))) > 0).ravel()\n",
    "ytru = ytest > 0\n",
    "sum(ytru == ywx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False],\n",
       "       [False,  True]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([1,2]) > 1\n",
    "b = np.array([[-1],[2]])>0\n",
    "a==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==np.transpose(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-634c2f48861a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lambda='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "train_obj['k=' + str(k) + '_lambda=' + str(lamb)]  = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
